{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52280c20-0123-467d-b7e5-2c8cd0bc8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import distutils.dir_util\n",
    "import glob\n",
    "import IPython.display\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import pyautogui\n",
    "import pytz\n",
    "import re\n",
    "import requests\n",
    "import selenium\n",
    "import shutil\n",
    "import statsapi\n",
    "import statsmodels.formula.api as smf\n",
    "import time\n",
    "import unidecode\n",
    "import warnings\n",
    "import webbrowser\n",
    "import xlrd\n",
    "import random\n",
    "import urllib\n",
    "from urllib.request import urlopen, Request\n",
    "import zipfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from IPython.display import display, Javascript\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from openpyxl import load_workbook\n",
    "from functools import partial\n",
    "\n",
    "from statsapi import get\n",
    "from pydfs_lineup_optimizer import get_optimizer, Site, Sport, Player, TeamStack, PlayerFilter, RandomFantasyPointsStrategy\n",
    "\n",
    "os.chdir(r\"C:\\Users\\james\\Documents\\MLB\\Code\")\n",
    "\n",
    "# from Utilities import *\n",
    "# from Classes import *\n",
    "# from simulation_functions_three import *\n",
    "\n",
    "import smtplib\n",
    "import ssl\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "# Ensure the warning is ignored only once\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "# Display the DataFrame\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Set paths\n",
    "model_path = r\"C:\\Users\\james\\Documents\\MLB\\Code\\Models\"\n",
    "baseball_path = r\"C:\\Users\\james\\Documents\\MLB\\Data2\"\n",
    "download_path = r\"C:\\Users\\james\\Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62fcd0d8-26c6-44ce-a207-3c4d905f7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads in a map of team name, codes, and the shorthand MLB uses in their URLs\n",
    "team_map = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Team Map.csv\"))\n",
    "\n",
    "# We just need teams right now\n",
    "team_map = team_map[['FULLNAME', 'BBREFTEAM', 'MLBURL', 'FANGRAPHSTEAM', 'VENUE_ID', 'SFBBTEAM', 'DKTEAM', 'ROTOWIRETEAM', 'FANPROSTEAM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a3ba4-339d-4968-ba68-b84655c65ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62818352-cc48-4e74-a0dd-865666f24f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in API data (Stats API or Statcast)\n",
    "def read_api(directory):\n",
    "    # Specify the directory path\n",
    "    directory_path = r'C:\\Users\\james\\Documents\\MLB\\Data2\\3. MLB API\\\\' + directory\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate through each file in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.csv'):  # You can adjust the file extension if needed\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)  # Read the CSV file into a DataFrame\n",
    "            dataframes.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Keep only regular season game \n",
    "    df = df[df['game_type'] == \"R\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edebeaef-1c5d-4243-a746-80ef8450a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset of raw data, merging Stats API and Statcast\n",
    "def raw_dataset():\n",
    "    # Read in Stats API data\n",
    "    statsapi = read_api('Stats API')\n",
    "    # Read in Statcast data\n",
    "    statcast = read_api('Statcast')\n",
    "    \n",
    "    # Merge Stats API and Statcast data\n",
    "    df = pd.merge(statsapi, statcast, on=['gamePk', 'atBatIndex'], how='left', indicator=True)    \n",
    "    \n",
    "    # Sort\n",
    "    df.sort_values(['game_date', 'gamePk', 'atBatIndex'], inplace=True)\n",
    "    \n",
    "    # Only keep one observation per at bat\n",
    "    df.drop_duplicates(['gamePk', 'atBatIndex'], keep='last', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dea7c3-4432-4af4-aa1a-5bff61b2810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wind vectors\n",
    "# Note: 2 is to centerfield, 6 is from centerfield, clockwise\n",
    "# Note: y vector is positive to centerfield, negative from centerfield\n",
    "# Note: x vector is positive from left to right, negatives from right to left\n",
    "# Assumption is wind is blowing in 8 cardinal directions, so we can use simple right isosceles triangles\n",
    "def y_vect(df):\n",
    "    wind_speed = df['windSpeed']\n",
    "    angled = df['windSpeed'] / 2 * math.sqrt(2)\n",
    "    \n",
    "    if df['windDirection'] == \"Out To CF\": \n",
    "        y_vect = wind_speed\n",
    "    elif df['windDirection'] == \"Out To RF\": \n",
    "        y_vect = angled\n",
    "    elif df['windDirection'] == \"L To R\": \n",
    "        y_vect = 0\n",
    "    elif df['windDirection'] == \"In From LF\": \n",
    "        y_vect = angled * -1\n",
    "    elif df['windDirection'] == \"In From CF\": \n",
    "        y_vect = wind_speed * - 1\n",
    "    elif df['windDirection'] == \"In From RF\": \n",
    "        y_vect = angled * -1\n",
    "    elif df['windDirection'] == \"R To L\": \n",
    "        y_vect = 0\n",
    "    elif df['windDirection'] == \"Out To LF\": \n",
    "        y_vect = angled\n",
    "    else:\n",
    "        y_vect = 0\n",
    "        \n",
    "    return y_vect\n",
    "\n",
    "def x_vect(df):\n",
    "    wind_speed = df['windSpeed']\n",
    "    angled = df['windSpeed'] / 2 * math.sqrt(2)\n",
    "    \n",
    "    if df['windDirection'] == \"L To R\": \n",
    "        x_vect = wind_speed\n",
    "    elif df['windDirection'] == \"In From LF\": \n",
    "        x_vect = angled\n",
    "    elif df['windDirection'] == \"In From CF\": \n",
    "        x_vect = 0\n",
    "    elif df['windDirection'] == \"In From RF\": \n",
    "        x_vect = angled * -1\n",
    "    elif df['windDirection'] == \"R To L\": \n",
    "        x_vect = wind_speed * - 1\n",
    "    elif df['windDirection'] == \"Out To LF\": \n",
    "        x_vect = angled * -1\n",
    "    elif df['windDirection'] == \"Out To CF\": \n",
    "        x_vect = 0\n",
    "    elif df['windDirection'] == \"Out To RF\": \n",
    "        x_vect = angled\n",
    "    else:\n",
    "        x_vect = 0\n",
    "        \n",
    "    return x_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2187436c-bce8-4cbe-8b65-3d7e6b71b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather(df):\n",
    "    # Separate weather into temperature and weather type\n",
    "    df[['temperature', 'weather']] = df['weather'].str.split(\", \", expand=True)\n",
    "    df['temperature'] = df['temperature'].str.replace(\" degrees\", \"\").astype('int')\n",
    "    # Separate wind into speed and direction\n",
    "    df[['windSpeed', 'windDirection']] = df['wind'].str.split(\", \", expand=True)\n",
    "    df['windSpeed'].fillna(\"0 mph\", inplace=True)\n",
    "    df['windSpeed'] = df['windSpeed'].str.replace(\" mph\", \"\")\n",
    "    df['windSpeed'] = pd.to_numeric(df['windSpeed'], errors='coerce')\n",
    "    df['windSpeed'].fillna(0, inplace=True)\n",
    "    df['windDirection'].fillna('L to R', inplace=True)\n",
    "    df['windSpeed'].unique()\n",
    "    df['windDirection'] = df['windDirection'].str.replace(\".\", \"\")\n",
    "    # Calculate vectors\n",
    "    df['x_vect'] = df.apply(x_vect, axis=1)\n",
    "    df['y_vect'] = df.apply(y_vect, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8de170e-47bb-45c3-9e03-ebcc8ff5ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = clean_weather(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7d1ba1-f465-41b5-95e4-f0a22cb7c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_events(df):\n",
    "    event_mapping = {\n",
    "        'Strikeout': 'so',\n",
    "        'Strikeout Double Play': 'so',\n",
    "        'Groundout': 'go',\n",
    "        'Fielders Choice': 'go',\n",
    "        'Double Play': 'go',\n",
    "        'Grounded Into DP': 'go',\n",
    "        'Triple Play': 'go',\n",
    "        'Field Error': 'go',\n",
    "        'Forceout': 'go',\n",
    "        'Lineout': 'lo',\n",
    "        'Bunt Lineout': 'lo',\n",
    "        'Flyout': 'fo',\n",
    "        'Sac Fly': 'fo',\n",
    "        'Sac Fly Double Play': 'fo',\n",
    "        'Pop Out': 'po',\n",
    "        'Bunt Pop Out': 'po',\n",
    "        'Hit By Pitch': 'hbp',\n",
    "        'Walk': 'bb',\n",
    "        'Intent Walk': 'bb',\n",
    "        'Single': 'b1',\n",
    "        'Double': 'b2',\n",
    "        'Triple': 'b3',\n",
    "        'Home Run': 'hr'\n",
    "    }\n",
    "\n",
    "    df['eventsModel'] = df['event'].map(event_mapping).fillna('Cut')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaea8337-35b8-41f8-9fc7-cdcae198d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = create_events(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf6523a-6f08-4275-aece-97657a2657ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This turns several variables, including events, venues, hands, and bases into dummies\n",
    "def create_dummies(df):\n",
    "    \n",
    "    # Events\n",
    "    event_dummies = pd.get_dummies(df['eventsModel'])\n",
    "    # Venues\n",
    "    venue_dummies = pd.get_dummies(df['venue_id'], prefix='venue')\n",
    "    # Hands\n",
    "    pitcher_dummies = pd.get_dummies(df['pitchHand'], prefix='p')\n",
    "    batter_dummies = pd.get_dummies(df['batSide'], prefix='b')\n",
    "    # Years\n",
    "    df['year'] = df['game_date'].str[:4]\n",
    "    year_dummies = pd.get_dummies(df['year'], prefix='year')\n",
    "    \n",
    "    # Create lists of dummies\n",
    "    venue_list = venue_dummies.columns.tolist()\n",
    "    year_list = year_dummies.columns.tolist()\n",
    "    dummy_list = venue_list + year_list\n",
    "    \n",
    "    # Add dummies to dataframe\n",
    "    df = pd.concat([df, event_dummies, venue_dummies, pitcher_dummies, batter_dummies, year_dummies], axis=1)\n",
    "    \n",
    "    # Create dummy for runners on base\n",
    "    df['preOnFirst'] = df.groupby(['gamePk', 'inning', 'halfInning'])['postOnFirst'].shift(1)\n",
    "    df['preOnSecond'] = df.groupby(['gamePk', 'inning', 'halfInning'])['postOnSecond'].shift(1)\n",
    "    df['preOnThird'] = df.groupby(['gamePk', 'inning', 'halfInning'])['postOnThird'].shift(1)\n",
    "    \n",
    "    df['onFirst'] = df['preOnFirst'].notnull().astype('int')\n",
    "    df['onSecond'] = df['preOnSecond'].notnull().astype('int')\n",
    "    df['onThird'] = df['preOnThird'].notnull().astype('int')\n",
    "    \n",
    "    # Top of the inning dummy\n",
    "    df['top'] = np.where(df['halfInning'] == \"top\", 1, 0)\n",
    "    \n",
    "    # Calculate PAs and ABs\n",
    "    df['pa'] = np.where(df['eventsModel'] != \"Cut\", 1, 0)\n",
    "    df['ab'] = df['pa'] - df['hbp'] - df['bb']\n",
    "    \n",
    "    return df, dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4da4b8e-aa74-4771-9e25-780bfc6af091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4, dummy_list = create_dummies(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fcba04c-28ab-42ba-b2bd-e64e378c6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(lst):\n",
    "    return max(lst) if lst else 0\n",
    "\n",
    "def clean_statcast(df):\n",
    "    # Hard hit dummy\n",
    "    df['hard_hit'] = (df['launch_speed'] >= 95).astype('int')\n",
    "    \n",
    "    # Barrel dummy\n",
    "    df['barrel'] = (df['launch_speed_angle'] == 6).astype('int')\n",
    "\n",
    "    # Spray \n",
    "    df['spray_angle'] = np.arctan((df['hc_x'] - 125.42) / (198.27 - df['hc_y'])) * 180 / np.pi * 0.75\n",
    "    df['to_left'] = (df['spray_angle'] < -15).astype('int')\n",
    "    df['to_middle'] = ((df['spray_angle'] >= -15) & (df['spray_angle'] <= 15)).astype('int')\n",
    "    df['to_right'] = (df['spray_angle'] > 15).astype('int')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe115f48-6a50-4358-b31c-d30427073b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5 = clean_statcast(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d216f84e-79d8-4e94-a6ae-9a437d7636ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_park_factors(team_map):\n",
    "    # Read in park factors\n",
    "    park_factors_l = pd.read_excel(r\"C:\\Users\\james\\Documents\\MLB\\Data2\\Utilities\\Statcast Park Factors.xlsx\", sheet_name='L')\n",
    "    park_factors_l['batSide'] = \"L\"\n",
    "    park_factors_r = pd.read_excel(r\"C:\\Users\\james\\Documents\\MLB\\Data2\\Utilities\\Statcast Park Factors.xlsx\", sheet_name='R')\n",
    "    park_factors_r['batSide'] = \"R\"\n",
    "\n",
    "    # Append \n",
    "    park_factors = pd.concat([park_factors_l, park_factors_r], axis=0)\n",
    "    # Clean\n",
    "    park_factors['Team'] = park_factors['Team'].str.strip()\n",
    "  \n",
    "    # Merge with team map to get venue ID\n",
    "    park_factors = park_factors.merge(team_map[['FANGRAPHSTEAM', 'VENUE_ID']], left_on='Team', right_on='FANGRAPHSTEAM', how='inner')\n",
    "    park_factors.rename(columns={'VENUE_ID':'venue_id'}, inplace=True)\n",
    "    \n",
    "    # Keep relevant variables\n",
    "    park_factors = park_factors[['venue_id', 'batSide', 'Park Factor', '1B', '2B', '3B', 'HR', 'BB', 'SO']]\n",
    "    \n",
    "    # Convert to mean of 1, not 100\n",
    "    factor_list = ['Park Factor', '1B', '2B', '3B', 'HR', 'BB', 'SO']\n",
    "    for factor in factor_list:\n",
    "        park_factors[factor] = park_factors[factor] / 100\n",
    "    \n",
    "    return park_factors\n",
    "\n",
    "park_factors = read_park_factors(team_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4f7ebe-8ffe-4c3c-af8b-120d3c69f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_adjustments(df, park_factors):\n",
    "    # Merge with park factors\n",
    "    df = df.merge(park_factors, on=['venue_id', 'batSide'], how='left')\n",
    "    \n",
    "    # Old/other parks get all 1s\n",
    "    df['Park Factor'].fillna(1, inplace=True)\n",
    "    df['1B'].fillna(1, inplace=True)\n",
    "    df['2B'].fillna(1, inplace=True)\n",
    "    df['3B'].fillna(1, inplace=True)\n",
    "    df['HR'].fillna(1, inplace=True)\n",
    "    df['BB'].fillna(1, inplace=True)\n",
    "    df['SO'].fillna(1, inplace=True)\n",
    "    \n",
    "    # Adjust stats by park factor\n",
    "    df['b1'] = df['b1'] / df['1B']\n",
    "    df['b2'] = df['b2'] / df['2B']\n",
    "    df['b3'] = df['b3'] / df['3B']\n",
    "    df['hr'] = df['hr'] / df['HR']\n",
    "    df['bb'] = df['bb'] / df['BB']\n",
    "    df['so'] = df['so'] / df['SO']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d579b3f8-ff4c-4be9-95bc-696eb6751e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df6 = park_adjustments(df5, park_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4a05515-a0be-468f-b393-45c522ef754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a dataframe that can eventually be used as the model input. Has pitcher vs hitter stats, specific to hand\n",
    "def rolling_pas(df, pa_num):\n",
    "    # Rename for compatibility purposes\n",
    "    df.rename(columns={'hit_distance_sc':'totalDistance', 'launch_speed':'launchSpeed'}, inplace=True)\n",
    "    \n",
    "    # Stats to calculate rolling averages/maximums\n",
    "    stat_list = ['so', 'b1', 'b2', 'b3', 'hr', 'bb', 'hbp', 'lo', 'po', 'go', 'fo', \n",
    "                 'hard_hit', 'barrel', 'to_left', 'to_middle', 'to_right', \n",
    "                 'estimated_woba_using_speedangle',\n",
    "                 'pa', 'ab']\n",
    "    \n",
    "    max_list = ['totalDistance', 'maxSpeed', 'maxSpin', 'launchSpeed']\n",
    "\n",
    "                \n",
    "    # \n",
    "    df['pa_num'] = df.index\n",
    "    \n",
    "    batter_stats = []\n",
    "    pitcher_stats = []\n",
    "    batter_stats2 = []\n",
    "    pitcher_stats2 = []\n",
    "\n",
    "    for stat in stat_list:\n",
    "        batter_stat = stat + \"_b\"\n",
    "        pitcher_stat = stat + \"_p\"\n",
    "        batter_stats.append(batter_stat)\n",
    "        pitcher_stats.append(pitcher_stat)\n",
    "\n",
    "    for stat in max_list:\n",
    "        batter_stat = stat + \"_b\"\n",
    "        pitcher_stat = stat + \"_p\"\n",
    "        batter_stats2.append(batter_stat)\n",
    "        pitcher_stats2.append(pitcher_stat)\n",
    "        \n",
    "    df[batter_stats] = df.groupby(['batter', 'pitchHand'])[stat_list].transform(lambda x: x.shift().rolling(pa_num, min_periods=1).sum())\n",
    "    df[batter_stats2] = df.groupby(['batter', 'pitchHand'])[max_list].transform(lambda x: x.shift().rolling(pa_num, min_periods=1).max())\n",
    "                \n",
    "    df[pitcher_stats] = df.groupby(['pitcher', 'batSide'])[stat_list].transform(lambda x: x.shift().rolling(pa_num, min_periods=1).sum())\n",
    "    df[pitcher_stats2] = df.groupby(['pitcher', 'batSide'])[max_list].transform(lambda x: x.shift().rolling(pa_num, min_periods=1).max())\n",
    "                \n",
    "    df.sort_values(['pa_num'], axis=0, ascending=True, inplace=True)\n",
    "\n",
    "    # wOBA - using 2022 values throughout\n",
    "    df['woba_b'] = (0.690 * df['bb_b']) + (0.721 * df['hbp_b']) + (0.885 * df['b1_b']) + (1.262 * df['b2_b']) + (1.601 * df['b3_b']) + (2.070 * df['hr_b'])\n",
    "    df['woba_p'] = (0.690 * df['bb_p']) + (0.721 * df['hbp_p']) + (0.885 * df['b1_p']) + (1.262 * df['b2_p']) + (1.601 * df['b3_p']) + (2.070 * df['hr_p'])\n",
    "    \n",
    "    # Slugging\n",
    "    df['slg_b'] = (1 * df['b1_b']) + (2 * df['b2_b']) + (3 * df['b3_b']) + (4 * df['hr_b'])\n",
    "    df['slg_b'] = df['slg_b'] / df['ab_b']\n",
    "    df['slg_p'] = (1 * df['b1_p']) + (2 * df['b2_p']) + (3 * df['b3_p']) + (4 * df['hr_p'])\n",
    "    df['slg_p'] = df['slg_p'] / df['ab_p']\n",
    "\n",
    "    # OBP    \n",
    "    df['obp_b'] = df[['b1_b', 'b2_b', 'b3_b', 'hr_b', 'bb_b', 'hbp_b']].sum(axis=1)\n",
    "    df['obp_p'] = df[['b1_p', 'b2_p', 'b3_p', 'hr_p', 'bb_p', 'hbp_p']].sum(axis=1)\n",
    "    \n",
    "    # ISO\n",
    "    df['iso_b'] = df['b2_b'] * 1 + df['b3_b'] * 2 + df['hr_b'] * 3\n",
    "    df['iso_p'] = df['b2_p'] * 1 + df['b3_p'] * 2 + df['hr_p'] * 3\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculate rates\n",
    "    stat_short = ['so', 'b1', 'b2', 'b3', 'hr', 'bb', 'hbp', 'lo', 'po', 'go', 'fo', \n",
    "                  'estimated_woba_using_speedangle', 'woba', 'obp', 'iso', 'hard_hit', 'barrel', \n",
    "                  'to_left', 'to_middle', 'to_right']\n",
    "    \n",
    "    for stat in stat_short:\n",
    "        batter_stat = stat + \"_b\"\n",
    "        pitcher_stat = stat + \"_p\"  \n",
    "        df[batter_stat] = df[batter_stat] / df['pa_b']\n",
    "        df[pitcher_stat] = df[pitcher_stat] / df['pa_p']\n",
    "        \n",
    "    df.sort_values('pa_num', inplace=True)\n",
    "    \n",
    "    batter_stats = batter_stats + batter_stats2\n",
    "    pitcher_stats = pitcher_stats + pitcher_stats2\n",
    "                \n",
    "        \n",
    "    return df, batter_stats, pitcher_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0b30462-8853-44a5-8bb7-2fac11549d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df7, batter_stats, pitcher_stats = rolling_pas(df6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f89bced-acbf-46eb-bf4a-a43792d985a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df7['game_date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c208e2de-ca70-4ab4-951e-3b4c95da4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts raw data to clean model input\n",
    "def create_model_input(date):\n",
    "    keep_list = ['so_b', 'b1_b', 'b2_b', 'b3_b', 'hr_b', 'bb_b', 'hbp_b', 'lo_b', 'po_b', 'go_b', 'fo_b', 'pa_b', 'ab_b', \n",
    "                 'estimated_woba_using_speedangle_b', 'woba_b', 'slg_b', 'obp_b', 'iso_b', 'hard_hit_b', 'barrel_b', \n",
    "                 'to_left_b', 'to_middle_b', 'to_right_b', \n",
    "                 'totalDistance_b', 'maxSpeed_b', 'maxSpin_b', 'launchSpeed_b',\n",
    "                 'so_p', 'b1_p', 'b2_p', 'b3_p', 'hr_p', 'bb_p', 'hbp_p', 'lo_p', 'po_p', 'go_p', 'fo_p', 'pa_p', 'ab_p', \n",
    "                 'estimated_woba_using_speedangle_p', 'woba_p', 'slg_p', 'obp_p', 'iso_p', 'hard_hit_p', 'barrel_p', \n",
    "                 'to_left_p', 'to_middle_p', 'to_right_p', \n",
    "                 'totalDistance_p', 'maxSpeed_p', 'maxSpin_p', 'launchSpeed_p']\n",
    "    \n",
    "    # Read in raw data\n",
    "    df = raw_dataset()    \n",
    "    # Clean weather variables\n",
    "    df2 = clean_weather(df)\n",
    "    # Create events\n",
    "    df3 = create_events(df2)\n",
    "    # Make dummies\n",
    "    df4, dummy_list = create_dummies(df3)\n",
    "    # Create Statcast stats\n",
    "    df5 = clean_statcast(df4)\n",
    "    # Make park adjustments\n",
    "    df6 = park_adjustments(df5, park_factors)\n",
    "    \n",
    "    # Restrict on data\n",
    "    # date = datetime.datetime.strptime(date, \"%Y%m%d\")\n",
    "    df6['date'] = df6['game_date'].str.replace(\"-\", \"\")\n",
    "    df6 = df6[df6['date'] < date]\n",
    "    # Cut if event isn't one we care about (usually these are weird base running things)\n",
    "    df6 = df6[df6['Cut'] != 1]\n",
    "\n",
    "    # Calculate short time frame rolling stats (100 PAs for now)\n",
    "    dfshort, batter_stats, pitcher_stats = rolling_pas(df6, 100)\n",
    "    dfmain = dfshort.copy()\n",
    "    # Calculate long time frame rolling stats (300 PAs for now)\n",
    "    dflong, batter_stats, pitcher_stats = rolling_pas(df6, 300)\n",
    "    dflong = dflong[keep_list]\n",
    "    dflong = dflong.add_suffix(\"_long\")\n",
    "    # Concatenate them together\n",
    "    sample = pd.concat([dfmain, dflong], axis=1)\n",
    "            \n",
    "    # Delete intermediate DFs\n",
    "    del dfmain, dfshort, dflong, df, df2, df3, df4, df5, df6    \n",
    "    \n",
    "    \n",
    "    # Determine score before PA\n",
    "    sample['preAwayScore'] = sample.groupby(['gamePk', 'inning', 'halfInning'])['awayScore'].shift(1)\n",
    "    sample['preHomeScore'] = sample.groupby(['gamePk', 'inning', 'halfInning'])['homeScore'].shift(1)\n",
    "    \n",
    "    sample['preAwayScore'].fillna(sample['awayScore'], inplace=True)\n",
    "    sample['preHomeScore'].fillna(sample['homeScore'], inplace=True)\n",
    "    \n",
    "    # Calculate score differential\n",
    "    sample['score_diff'] = np.where(sample['top'] == 1, sample['preAwayScore'] - sample['preHomeScore'], sample['preHomeScore'] - sample['preAwayScore'])\n",
    "    \n",
    "    \n",
    "    # Start dummy (=1 for first batter for each starter)\n",
    "    sample['start'] = 0\n",
    "\n",
    "    # Group by 'gamePk' and 'halfInning', then find the index of the first occurrence\n",
    "    top_first_idx = sample[sample['halfInning'] == 'top'].groupby('gamePk').head(1).index\n",
    "    bottom_first_idx = sample[sample['halfInning'] == 'bottom'].groupby('gamePk').head(1).index\n",
    "\n",
    "    # Update 'start' column based on the first occurrences\n",
    "    sample.loc[top_first_idx, 'start'] = 1\n",
    "    sample.loc[bottom_first_idx, 'start'] = 1\n",
    "    \n",
    "    # Add them up\n",
    "    sample['starts'] = sample[sample['date'] > \"20190330\"].groupby(['pitcher'])['start'].cumsum()\n",
    "    \n",
    "    \n",
    "    # Group by 'gamePk' and 'pitcher', then identify the index of the last observation\n",
    "    last_observation_idx = sample.groupby(['gamePk', 'pitcher']).tail(1).index   \n",
    "    \n",
    "    # Pulled dummy (=1 for last batter for each pitcher)\n",
    "    sample['pulled'] = 0\n",
    "\n",
    "    # Update 'pulled' column for the last observations\n",
    "    sample.loc[last_observation_idx, 'pulled'] = 1\n",
    "    \n",
    "    \n",
    "    # Batters faced\n",
    "    sample['faced'] = 1\n",
    "    games = sample.groupby(['pitcher', 'gamePk'])['faced'].sum().reset_index()\n",
    "    # Average of last n games, rolling, shifted\n",
    "    games['avgFaced'] = games.groupby('pitcher')['faced'].rolling(30, min_periods=1).mean().shift().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Merge to get avgFaced back\n",
    "    sample = sample.merge(games, on=['pitcher', 'gamePk'], how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0d80be6-e9ec-4854-8249-a1c0744c2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = create_model_input(\"20220818\")\n",
    "# sample.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36148a3a-bd41-45e4-974d-ec92ce91ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batter inputs\n",
    "def create_batter_df(df, date):\n",
    "    # Stats of interest\n",
    "    batter_list = ['batter',  'batterName', 'batSide', 'p_L',\n",
    "     'so_b', 'b1_b', 'b2_b', 'b3_b', 'hr_b', 'bb_b', 'hbp_b', 'lo_b', 'po_b', 'go_b', 'fo_b', \n",
    "     'pa_b', 'ab_b', 'estimated_woba_using_speedangle_b', 'woba_b', 'slg_b', 'obp_b', 'iso_b', \n",
    "     'hard_hit_b', 'barrel_b', 'to_left_b', 'to_middle_b', 'to_right_b', \n",
    "     'totalDistance_b', 'maxSpeed_b', 'maxSpin_b', 'launchSpeed_b', \n",
    "     'so_b_long', 'b1_b_long', 'b2_b_long', 'b3_b_long', 'hr_b_long', 'bb_b_long', 'hbp_b_long', 'lo_b_long', 'po_b_long', 'go_b_long', 'fo_b_long', \n",
    "     'pa_b_long', 'ab_b_long', 'estimated_woba_using_speedangle_b_long', 'woba_b_long', 'slg_b_long', 'obp_b_long', 'iso_b_long', \n",
    "     'hard_hit_b_long', 'barrel_b_long', 'to_left_b_long', 'to_middle_b_long', 'to_right_b_long', \n",
    "     'totalDistance_b_long', 'maxSpeed_b_long', 'maxSpin_b_long', 'launchSpeed_b_long']\n",
    "\n",
    "    # Only keep relevant stats\n",
    "    batters = df[batter_list]\n",
    "    # Only care about most recent stats of each batter before PA\n",
    "    batters.drop_duplicates(subset=['batter', 'p_L'], keep='last', inplace=True)\n",
    "\n",
    "    # Create separate dataframes for vs RHP and LHP\n",
    "    vs_r = batters.query('p_L == 0')\n",
    "    vs_l = batters.query('p_L == 1')\n",
    "\n",
    "    # Merge them together\n",
    "    batters = vs_l.merge(vs_r, on='batter', how='outer', suffixes=('_l', '_r'))\n",
    "\n",
    "    # Drop duplicate columns\n",
    "    batters.drop(columns={'batterName_r', 'p_L_l', 'p_L_r'}, inplace=True)\n",
    "    # Only need this once\n",
    "    batters.rename(columns={'batterName_l': 'batterName'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Merge with Chadwick\n",
    "    batters = batters.merge(chadwick, left_on='batter', right_on='key_mlbam', how='left')\n",
    "    \n",
    "    \n",
    "    # Export\n",
    "    batters.to_csv(os.path.join(baseball_path, \"4. Dataset\", \"Batters\", \"Batters\" + date + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fff4ce1-5c52-4048-864e-15ff8534013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pitcher inputs\n",
    "def create_pitcher_df(df, date):\n",
    "    # Stats of interest\n",
    "    pitcher_list =  ['pitcher',  'pitcherName', 'pitchHand', 'b_L',\n",
    "                     'b1_p', 'b2_p', 'b3_p', 'hr_p', 'bb_p', 'hbp_p', \n",
    "                     'so_p', 'lo_p', 'po_p', 'go_p', 'fo_p', \n",
    "                     'estimated_woba_using_speedangle_p', 'woba_p', 'slg_p', 'obp_p', 'iso_p',\n",
    "                     'to_left_p', 'to_middle_p', 'to_right_p', \n",
    "                     'hard_hit_p', 'barrel_p', 'maxSpeed_p', 'maxSpin_p', 'totalDistance_p', 'launchSpeed_p',\n",
    "                     'pa_p', 'ab_p',\n",
    "                     \n",
    "                     'b1_p_long', 'b2_p_long', 'b3_p_long', 'hr_p_long', 'bb_p_long', 'hbp_p_long', \n",
    "                     'so_p_long', 'lo_p_long', 'po_p_long', 'go_p_long', 'fo_p_long', \n",
    "                     'estimated_woba_using_speedangle_p_long', 'woba_p_long', 'slg_p_long', 'obp_p_long', 'iso_p_long', \n",
    "                     'to_left_p_long', 'to_middle_p_long', 'to_right_p_long',\n",
    "                     'hard_hit_p_long', 'barrel_p_long', 'maxSpeed_p_long', 'maxSpin_p_long', 'totalDistance_p_long', 'launchSpeed_p_long',\n",
    "                     'pa_p_long', 'ab_p_long', \n",
    "                     'avgFaced', 'starts',\n",
    "                     'inning', 'outs', 'gamePk', 'eventsModel', 'game_date']\n",
    "    \n",
    "    # Only keep relevant stats\n",
    "    pitchers = df[pitcher_list]\n",
    "    \n",
    "    # Calculate average outs\n",
    "    # Create a copy of the dataframe\n",
    "    pitchers_cut = pitchers.copy()\n",
    "    # Only look at PAs since 2019\n",
    "    pitchers_cut = pitchers_cut[pitchers_cut['game_date'] > '2019-04-01']\n",
    "    pitchers_cut.drop_duplicates(subset=['pitcher', 'gamePk', 'inning'], keep='last', inplace=True)\n",
    "    # Identify if they're a starter\n",
    "    pitchers_cut['starter'] = (pitchers_cut['inning'] == 1).astype('int')\n",
    "    # Add up starts\n",
    "    pitchers_cut = pitchers_cut.groupby(['pitcher', 'gamePk'])['outs', 'starter'].sum().reset_index()\n",
    "    # Calculate mean outs and sum of starts\n",
    "    pitchers_cut = pitchers_cut.groupby('pitcher').agg({'outs': np.mean, 'starter': np.sum}).reset_index()\n",
    "\n",
    "    # Only care about most recent stats of each pitcher before PA\n",
    "    pitchers.drop_duplicates(subset=['pitcher', 'b_L'], keep='last', inplace=True)\n",
    "    \n",
    "    # Create separate dataframes for vs RHB and LHB\n",
    "    vs_r = pitchers.query('b_L == 0')\n",
    "    vs_l = pitchers.query('b_L == 1')\n",
    "\n",
    "    # Merge them together\n",
    "    pitchers = vs_l.merge(vs_r, on='pitcher', how='outer', suffixes=('_l', '_r'))\n",
    "    # And add outs/starts\n",
    "    pitchers = pitchers.merge(pitchers_cut, on='pitcher', how='left')\n",
    "    \n",
    "    # Drop duplicate columns\n",
    "    pitchers.drop(columns={'pitcherName_r', 'b_L_l', 'b_L_r', 'inning_r', 'outs_r', 'gamePk_r', 'eventsModel_r', 'game_date_r',  'inning_l',\n",
    "                           'outs_l', 'gamePk_l', 'eventsModel_l', 'game_date_l', 'starts_l', 'avgFaced_l'}, inplace=True)\n",
    "    # Only need this once\n",
    "    pitchers.rename(columns={'pitcherName_l': 'pitcherName', 'starts_r': 'start', 'avgFaced_r':'avgFaced'}, inplace=True)\n",
    "    \n",
    "    # Merge with Chadwick\n",
    "    pitchers = pitchers.merge(chadwick, left_on='pitcher', right_on='key_mlbam', how='left')\n",
    "    \n",
    "    \n",
    "    # Export    \n",
    "    pitchers.to_csv(os.path.join(baseball_path, \"4. Dataset\", \"Pitchers\", \"Pitchers\" + date + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d97bd1d-cac6-4ddb-8788-6010def60984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates inputs on a given date\n",
    "def create_datasets(date):\n",
    "    # Create data for model and data for model inputs \n",
    "    sample = create_model_input(date)\n",
    "    # Create batter and pitcher csvfiles\n",
    "    create_batter_df(sample, date)\n",
    "    create_pitcher_df(sample, date)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1452633a-8a6e-47c1-9800-24e5bacb2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"Utilities.ipynb\"\n",
    "# # This reads in Chadwick register with player codes.\n",
    "# keep_list = ['key_mlbam', 'key_fangraphs', 'key_bbref_minors', 'key_bbref', 'name_first', 'name_last']\n",
    "# chadwick = read_chadwick(keep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3cec59e-0cf5-4593-99d4-6a368b378384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230330\n",
      "20230331\n",
      "20230401\n",
      "20230402\n",
      "20230403\n",
      "20230404\n",
      "20230405\n",
      "20230406\n",
      "20230407\n",
      "20230408\n",
      "20230409\n",
      "20230410\n",
      "20230411\n",
      "20230412\n",
      "20230413\n",
      "20230414\n",
      "20230415\n",
      "20230416\n",
      "20230417\n",
      "20230418\n",
      "20230419\n",
      "20230420\n",
      "20230421\n",
      "20230422\n",
      "20230423\n",
      "20230424\n",
      "20230425\n",
      "20230426\n",
      "20230427\n",
      "20230428\n",
      "20230429\n",
      "20230430\n",
      "20230501\n",
      "20230502\n",
      "20230503\n",
      "20230504\n",
      "20230505\n",
      "20230506\n",
      "20230507\n",
      "20230508\n",
      "20230509\n",
      "20230510\n",
      "20230511\n",
      "20230512\n",
      "20230513\n",
      "20230514\n",
      "20230515\n",
      "20230516\n",
      "20230517\n",
      "20230518\n",
      "20230519\n",
      "20230520\n",
      "20230521\n",
      "20230522\n",
      "20230523\n",
      "20230524\n",
      "20230525\n",
      "20230526\n",
      "20230527\n",
      "20230528\n",
      "20230529\n",
      "20230530\n",
      "20230531\n",
      "20230601\n",
      "20230602\n",
      "20230603\n",
      "20230604\n",
      "20230605\n",
      "20230606\n",
      "20230607\n",
      "20230608\n",
      "20230609\n",
      "20230610\n",
      "20230611\n",
      "20230612\n",
      "20230613\n",
      "20230614\n",
      "20230615\n",
      "20230616\n",
      "20230617\n",
      "20230618\n",
      "20230619\n",
      "20230620\n",
      "20230621\n",
      "20230622\n",
      "20230623\n",
      "20230624\n",
      "20230625\n",
      "20230626\n",
      "20230627\n",
      "20230628\n",
      "20230629\n",
      "20230630\n",
      "20230701\n",
      "20230702\n",
      "20230703\n",
      "20230704\n",
      "20230705\n",
      "20230706\n",
      "20230707\n",
      "20230708\n",
      "20230709\n",
      "20230714\n",
      "20230715\n",
      "20230716\n",
      "20230717\n",
      "20230718\n",
      "20230719\n",
      "20230720\n",
      "20230721\n",
      "20230722\n",
      "20230723\n",
      "20230724\n",
      "20230725\n",
      "20230726\n",
      "20230727\n",
      "20230728\n",
      "20230729\n",
      "20230730\n",
      "20230731\n",
      "20230801\n",
      "20230802\n",
      "20230803\n",
      "20230804\n",
      "20230805\n",
      "20230806\n",
      "20230807\n",
      "20230808\n",
      "20230809\n",
      "20230810\n",
      "20230811\n",
      "20230812\n",
      "20230813\n",
      "20230814\n",
      "20230815\n",
      "20230816\n",
      "20230817\n",
      "20230818\n",
      "20230819\n",
      "20230820\n"
     ]
    }
   ],
   "source": [
    "# for filename in os.listdir(r\"C:\\Users\\james\\Documents\\MLB\\Data2\\4. Dataset\\Batters\"): \n",
    "#     # 2023 \n",
    "#     if filename.startswith(\"Batters2023\"):\n",
    "#         # Pull out date\n",
    "#         date = filename[7:15]\n",
    "#         print(date)\n",
    "#         sample = create_datasets(date)\n",
    "\n",
    "# # date_list = \n",
    "# # import os\n",
    "\n",
    "# # directory = r'C:\\Users\\james\\Documents\\MLB\\Data2\\4. Dataset\\Batters'\n",
    "# # file_list = []\n",
    "\n",
    "# # for filename in os.listdir(directory):\n",
    "# #     if os.path.isfile(os.path.join(directory, filename)):\n",
    "# #         extracted_name = filename[7:15]\n",
    "# #         file_list.append(extracted_name)\n",
    "\n",
    "# # list_2023 = [file for file in file_list if file.startswith(\"2023\")]\n",
    "\n",
    "# # # Can't do max/near max because it'll take up too much memory\n",
    "# # # Note: There are better ways to do this! Just create dataset once and then work backwards to create daily \n",
    "# # Parallel(n_jobs=4, verbose=5)(delayed(create_datasets)(date) for date in list_2023)\n",
    "\n",
    "# # print(list_2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc4669-ddae-483d-91e9-fa971f2d95d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
