{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# 006. Weather "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66c3d1-2a3f-4bc4-a77a-728a13dbbe93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b3626-a33a-4c54-bb97-e138778f4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"C:\\Users\\james\\Documents\\MLB\\Code\\A02. MLB API.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2af29b-1396-412b-8f77-a1b5387395f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c503c7-9876-416a-8fda-3c7014283079",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ed6df-103d-49bb-9194-8e83acb0e1b1",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69330848-9511-4906-b1cc-344c5a3d8b06",
   "metadata": {},
   "source": [
    "Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53901474-7f58-43c1-9209-9aedab87c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reverses winds so that they're named for where they're going, not where they're from. This is so vectors make more sense logically.\n",
    "def wind_reverser(direction):\n",
    "    direction = direction.replace(\"N\", \"s\")\n",
    "    direction = direction.replace(\"S\", \"n\")\n",
    "    direction = direction.replace(\"E\", \"w\")\n",
    "    direction = direction.replace(\"W\", \"e\")\n",
    "    \n",
    "    return direction.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0be44-6c91-4ba6-af6a-2c20f16b6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates number of degrees for each direction\n",
    "def find_degree(direction):\n",
    "    if direction == \"N\":\n",
    "        degree = 0\n",
    "    elif direction == \"NNE\":\n",
    "        degree = 1\n",
    "    elif direction == \"NE\":\n",
    "        degree = 2\n",
    "    elif direction == \"ENE\":\n",
    "        degree = 3\n",
    "    elif direction == \"E\":\n",
    "        degree = 4\n",
    "    elif direction == \"ESE\":\n",
    "        degree = 5\n",
    "    elif direction == \"SE\":\n",
    "        degree = 6\n",
    "    elif direction == \"SSE\":\n",
    "        degree = 7\n",
    "    elif direction == \"S\":\n",
    "        degree = 8\n",
    "    elif direction == \"SSW\":\n",
    "        degree = 9\n",
    "    elif direction == \"SW\":\n",
    "        degree = 10\n",
    "    elif direction == \"WSW\":\n",
    "        degree = 11\n",
    "    elif direction == \"W\":\n",
    "        degree = 12\n",
    "    elif direction == \"WNW\":\n",
    "        degree = 13\n",
    "    elif direction == \"NW\":\n",
    "        degree = 14\n",
    "    elif direction == \"NNW\":\n",
    "        degree = 15\n",
    "        \n",
    "    degree = degree * 22.5 \n",
    "\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea2c54-4014-4043-837c-b7c3ecaac1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the x and y vectors given the park's orientation and the wind's direction\n",
    "def calculate_vectors(row):\n",
    "    # Determines degree of centerfield\n",
    "    park_angle = find_degree(row['CF'])\n",
    "    # Determine degree of wind\n",
    "    row['Direction'] = wind_reverser(row['Direction'])\n",
    "    wind_angle = find_degree(row['Direction']) \n",
    "    \n",
    "    # Determine angle between them\n",
    "    angle = wind_angle - park_angle \n",
    "\n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row['Speed']\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row['Speed']\n",
    "\n",
    "    return x_vect, y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911a879-6dac-429f-8e42-28f8b17c1977",
   "metadata": {},
   "source": [
    "Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c9cdb2-c7c2-4356-a7d9-36bcd2299748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Swish Analytics for weather data\n",
    "def swishanalytics(date):\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    # Swish Analytics URL \n",
    "    url = \"https://swishanalytics.com/mlb/weather?date=\" + date_dash\n",
    "\n",
    "     # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class 'weather-card'\n",
    "        weather_cards = soup.find_all('div', class_='weather-card')\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dfs = []\n",
    "        \n",
    "        # Iterate over each weather card\n",
    "        for weather_card in weather_cards:\n",
    "            # Extract relevant information from the weather card\n",
    "            time_info = weather_card.find('small', class_='text-muted')\n",
    "            location_info = weather_card.find('h4', class_='lato inline vert-mid bold')\n",
    "            \n",
    "            # Extract time and location information\n",
    "            time = time_info.text.strip() if time_info else None\n",
    "            location = location_info.text.strip() if location_info else None\n",
    "            \n",
    "            # Find the table within the weather card\n",
    "            table = weather_card.find('table', class_='table-bordered')\n",
    "            \n",
    "            # If table exists, extract data from it\n",
    "            if table:\n",
    "                # Extract table data into a list of lists\n",
    "                rows = table.find_all('tr')\n",
    "                data = []\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    row_data = [cell.text.strip() for cell in cells]\n",
    "                    data.append(row_data)\n",
    "                \n",
    "                # Convert data into a pandas DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Set the first row as the column headers\n",
    "                df.columns = df.iloc[0]\n",
    "                df = df[1:]  # Remove the first row since it's the header row\n",
    "                \n",
    "                # Add time and location as additional columns\n",
    "                df['Time'] = time\n",
    "                df['Location'] = location\n",
    "\n",
    "                # Create dataframem from the second time period scraped\n",
    "                daily_weather_df = pd.DataFrame(df.iloc[:, 2]).T\n",
    "                # Extract home team name \n",
    "                daily_weather_df['Matchup'] = df['Location'][1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['Matchup'].str.split(\"@\", expand=True).iloc[:, 1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['FANGRAPHSTEAM'].str.replace(\"\\xa0\\xa0\", \"\")\n",
    "\n",
    "                dfs.append(daily_weather_df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    # Append together dataframes\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "\n",
    "    # Identify CF\n",
    "    df = df.merge(team_map[['FANGRAPHSTEAM', 'BBREFTEAM', 'CF']], on='FANGRAPHSTEAM', how='left')\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={1:'Weather', 2:'temperature', 3:'Feels Like', 4:'Humidity', 5:'Speed', 6:'Direction', 'BBREFTEAM': 'home_team'}, inplace=True)\n",
    "\n",
    "    # Remove mph\n",
    "    df['Speed'] = df['Speed'].str.replace(\" mph\", \"\").astype(float)\n",
    "    df['temperature'] = df['temperature'].str.replace('°', '')\n",
    "    df['Feels Like'] = df['Feels Like'].str.replace('°', '')\n",
    "    \n",
    "    # Apply the calculate_vectors function row-wise and assign results to new columns\n",
    "    df[['x_vect', 'y_vect']] = df.apply(calculate_vectors, axis=1, result_type='expand')\n",
    "    \n",
    "    \n",
    "    return df[['Matchup', 'home_team', 'Weather', 'Feels Like', 'Humidity', 'Speed', 'Direction', 'FANGRAPHSTEAM', 'CF', 'temperature', 'x_vect', 'y_vect']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb060b-9677-4f9e-9d2e-de88f9874c7b",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbe8bc-5e6f-4c13-8453-4f4caf44bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotogrinders(date, team_map):\n",
    "    # URL of the web page containing the table\n",
    "    url = \"https://rotogrinders.com/weather/mlb\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Get the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = soup.find_all(\"li\", class_=\"weather-blurb\")\n",
    "\n",
    "        # Create an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        for li_element in li_elements:\n",
    "            # Extract the tag colors from the <span> elements\n",
    "            tag_elements = li_element.find_all(\"span\", class_=[\"green\", \"yellow\", \"orange\", \"red\"])\n",
    "        \n",
    "            # Extract the first tag color\n",
    "            tag = tag_elements[0].text.strip() if tag_elements else None\n",
    "        \n",
    "            # Extract the second tag color if it exists\n",
    "            tag2 = tag_elements[1].text.strip() if len(tag_elements) > 1 else None\n",
    "        \n",
    "            # Extract the matchup from the <span> element with class \"bold\"\n",
    "            matchup_span = li_element.find(\"span\", class_=\"bold\")\n",
    "            matchup = matchup_span.text.strip() if matchup_span else None\n",
    "        \n",
    "            # Extract the description if it exists\n",
    "            if matchup_span:\n",
    "                description_span = matchup_span.find_next_sibling(\"span\")\n",
    "                description = description_span.text.strip() if description_span else None\n",
    "            else:\n",
    "                description = None\n",
    "        \n",
    "            # Append the data to the list\n",
    "            data.append({\"Tag\": tag, \"Tag2\": tag2, \"Matchup\": matchup, \"Description\": description})\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        df[['away', 'home']] = df['Matchup'].str.split(\" @ \", expand=True)\n",
    "\n",
    "        # Add in DK team abbreviations \n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['away'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_away\"))\n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['home'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_home\"))\n",
    "        df = df[['Tag', 'Tag2', 'Matchup', 'DKTEAM', 'DKTEAM_home', 'Description']]\n",
    "        df.rename(columns={'DKTEAM':'Away', 'DKTEAM_home': 'Home'}, inplace=True)\n",
    "        \n",
    "        # Add the date column to the DataFrame\n",
    "        df['date'] = date\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        # Return an error message if the response is not successful\n",
    "        return \"Failed to retrieve data. Response status code: {}\".format(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513ddc2-82ab-45b5-938b-c97e9d9a2cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8a3b07-f812-4a86-87e6-c79adcbb62f1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e995618-7cd1-4af1-b01c-19e9ce1060e1",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a707003-941c-4acc-8fd0-596008959fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish Scrape Analytics\n",
    "try:\n",
    "    swishanalytics_df = swishanalytics(todaysdate)\n",
    "    # To csv\n",
    "    swishanalytics_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Swish Analytics\", f\"Swish Analytics {todaysdate}.csv\"), index=False, encoding='iso-8859-1')\n",
    "except:\n",
    "    print(\"Could not scrape Swish Analytics weather data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b471b-a217-478d-ae6f-6704d87675c0",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7527eda-bb27-46c5-8b7b-7ed0e4a30022",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Scrape RotoGrinders\n",
    "    rotogrinders_df = rotogrinders(todaysdate, team_map)\n",
    "    # To csv\n",
    "    rotogrinders_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"2. RotoGrinders\", f\"RotoGrinders {todaysdate}.csv\"), index=False)\n",
    "except:\n",
    "    print(\"Could not scrape RotoGrinders weather data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
