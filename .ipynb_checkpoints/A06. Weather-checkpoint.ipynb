{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# A06. Weather "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66c3d1-2a3f-4bc4-a77a-728a13dbbe93",
   "metadata": {},
   "source": [
    "Note: All historic Park and Weather Factors files are created from M01. Park and Weatehr Factors.ipynb upon the training of new models. A06. Weather is for daily files only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports executed\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\"\n",
    "    print(\"Imports executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b0035-717d-4266-87f0-d96534b092b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24f93710-16ec-421a-a2f5-b31fec6633de",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c011b295-fa27-4240-a02d-536a5a9114d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_df created.\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Set date range \n",
    "    start_date = todaysdate\n",
    "    end_date = todaysdate\n",
    "    game_df = create_games(start_date, end_date, team_map)\n",
    "    print(\"game_df created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13e765-9b57-4d4a-aa39-f8f8dd07bfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c503c7-9876-416a-8fda-3c7014283079",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ed6df-103d-49bb-9194-8e83acb0e1b1",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cee7f9-76d2-4639-a001-66c8f0603dc4",
   "metadata": {},
   "source": [
    "Swish Analytics contains weather projections to be used before MLB Stats API updates theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911a879-6dac-429f-8e42-28f8b17c1977",
   "metadata": {},
   "source": [
    "Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6c9cdb2-c7c2-4356-a7d9-36bcd2299748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Swish Analytics for weather data\n",
    "def swishanalytics(date):\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    # Swish Analytics URL \n",
    "    url = \"https://swishanalytics.com/mlb/weather?date=\" + date_dash\n",
    "\n",
    "     # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class 'weather-card'\n",
    "        weather_cards = soup.find_all('div', class_='weather-card')\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dfs = []\n",
    "        \n",
    "        # Iterate over each weather card\n",
    "        for weather_card in weather_cards:\n",
    "            # Extract relevant information from the weather card\n",
    "            time_info = weather_card.find('small', class_='text-muted')\n",
    "            location_info = weather_card.find('h4', class_='lato inline vert-mid bold')\n",
    "            \n",
    "            # Extract time and location information\n",
    "            time = time_info.text.strip() if time_info else None\n",
    "            location = location_info.text.strip() if location_info else None\n",
    "            \n",
    "            # Find the table within the weather card\n",
    "            table = weather_card.find('table', class_='table-bordered')\n",
    "            \n",
    "            # If table exists, extract data from it\n",
    "            if table:\n",
    "                # Extract table data into a list of lists\n",
    "                rows = table.find_all('tr')\n",
    "                data = []\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    row_data = [cell.text.strip() for cell in cells]\n",
    "                    data.append(row_data)\n",
    "                \n",
    "                # Convert data into a pandas DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Set the first row as the column headers\n",
    "                df.columns = df.iloc[0]\n",
    "                df = df[1:]  # Remove the first row since it's the header row\n",
    "                \n",
    "                # Add time and location as additional columns\n",
    "                df['Time'] = time\n",
    "                df['Location'] = location\n",
    "\n",
    "                # Create dataframem from the second time period scraped\n",
    "                daily_weather_df = pd.DataFrame(df.iloc[:, 2]).T\n",
    "                # Extract home team name \n",
    "                daily_weather_df['Matchup'] = df['Location'][1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['Matchup'].str.split(\"@\", expand=True).iloc[:, 1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['FANGRAPHSTEAM'].str.replace(\"\\xa0\\xa0\", \"\")\n",
    "\n",
    "                dfs.append(daily_weather_df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    # Append together dataframes\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={1:'Weather', 2:'Temperature', 3:'Feels Like', 4:'Humidity', 5:'Speed', 6:'Direction', 'BBREFTEAM': 'home_team'}, inplace=True)\n",
    "\n",
    "    # Clean\n",
    "    df['Speed'] = df['Speed'].str.replace(\" mph\", \"\").astype(float)\n",
    "    df['Temperature'] = df['Temperature'].str.replace('°', '')\n",
    "    df['Feels Like'] = df['Feels Like'].str.replace('°', '')\n",
    "    df.reset_index(drop=False, inplace=True, names='Time')\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1a5d5-af47-463b-8708-911b012cf77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0feb060b-9677-4f9e-9d2e-de88f9874c7b",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5cd41-3e3d-472a-9c6b-721ea3d1db24",
   "metadata": {},
   "source": [
    "RotoGrinders hosts weather warnings used to identify matchups to avoid based on weather risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e3674-87ac-4227-9a07-7e552894006b",
   "metadata": {},
   "source": [
    "Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1cbe8bc-5e6f-4c13-8453-4f4caf44bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotogrinders(date, team_map):\n",
    "    # URL of the web page containing the table\n",
    "    url = \"https://rotogrinders.com/weather/mlb\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Get the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = soup.find_all(\"li\", class_=\"weather-blurb\")\n",
    "\n",
    "        # Create an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        for li_element in li_elements:\n",
    "            # Extract the tag colors from the <span> elements\n",
    "            tag_elements = li_element.find_all(\"span\", class_=[\"green\", \"yellow\", \"orange\", \"red\"])\n",
    "        \n",
    "            # Extract the first tag color\n",
    "            tag = tag_elements[0].text.strip() if tag_elements else None\n",
    "        \n",
    "            # Extract the second tag color if it exists\n",
    "            tag2 = tag_elements[1].text.strip() if len(tag_elements) > 1 else None\n",
    "        \n",
    "            # Extract the matchup from the <span> element with class \"bold\"\n",
    "            matchup_span = li_element.find(\"span\", class_=\"bold\")\n",
    "            matchup = matchup_span.text.strip() if matchup_span else None\n",
    "        \n",
    "            # Extract the description if it exists\n",
    "            if matchup_span:\n",
    "                description_span = matchup_span.find_next_sibling(\"span\")\n",
    "                description = description_span.text.strip() if description_span else None\n",
    "            else:\n",
    "                description = None\n",
    "        \n",
    "            # Append the data to the list\n",
    "            data.append({\"Tag\": tag, \"Tag2\": tag2, \"Matchup\": matchup, \"Description\": description})\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        df[['away', 'home']] = df['Matchup'].str.split(\" @ \", expand=True)\n",
    "\n",
    "        # Add in DK team abbreviations \n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['away'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_away\"))\n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['home'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_home\"))\n",
    "        df = df[['Tag', 'Tag2', 'Matchup', 'DKTEAM', 'DKTEAM_home', 'Description']]\n",
    "        df.rename(columns={'DKTEAM':'Away', 'DKTEAM_home': 'Home'}, inplace=True)\n",
    "        \n",
    "        # Add the date column to the DataFrame\n",
    "        df['date'] = date\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        # Return an error message if the response is not successful\n",
    "        return \"Failed to retrieve data. Response status code: {}\".format(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d82a29-46f9-4318-b170-46ae15462ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3b4d98-8ef6-4622-ab4b-1d492d48c65f",
   "metadata": {},
   "source": [
    "##### 3. Park x Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9cc871-b944-4690-8ebc-26c49bb5d4a5",
   "metadata": {},
   "source": [
    "Reverse wind to direction to, not from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53901474-7f58-43c1-9209-9aedab87c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reverses winds so that they're named for where they're going, not where they're from. This is so vectors make more sense logically.\n",
    "def wind_reverser(direction):\n",
    "    direction = direction.replace(\"N\", \"s\")\n",
    "    direction = direction.replace(\"S\", \"n\")\n",
    "    direction = direction.replace(\"E\", \"w\")\n",
    "    direction = direction.replace(\"W\", \"e\")\n",
    "    \n",
    "    return direction.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad04b4-2cdf-4b53-b89b-bf045476d08b",
   "metadata": {},
   "source": [
    "Determine angle between wind and park directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34f0be44-6c91-4ba6-af6a-2c20f16b6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates number of degrees for each direction\n",
    "def find_degree(direction):\n",
    "    if direction == \"N\":\n",
    "        degree = 0\n",
    "    elif direction == \"NNE\":\n",
    "        degree = 1\n",
    "    elif direction == \"NE\":\n",
    "        degree = 2\n",
    "    elif direction == \"ENE\":\n",
    "        degree = 3\n",
    "    elif direction == \"E\":\n",
    "        degree = 4\n",
    "    elif direction == \"ESE\":\n",
    "        degree = 5\n",
    "    elif direction == \"SE\":\n",
    "        degree = 6\n",
    "    elif direction == \"SSE\":\n",
    "        degree = 7\n",
    "    elif direction == \"S\":\n",
    "        degree = 8\n",
    "    elif direction == \"SSW\":\n",
    "        degree = 9\n",
    "    elif direction == \"SW\":\n",
    "        degree = 10\n",
    "    elif direction == \"WSW\":\n",
    "        degree = 11\n",
    "    elif direction == \"W\":\n",
    "        degree = 12\n",
    "    elif direction == \"WNW\":\n",
    "        degree = 13\n",
    "    elif direction == \"NW\":\n",
    "        degree = 14\n",
    "    elif direction == \"NNW\":\n",
    "        degree = 15\n",
    "        \n",
    "    degree = degree * 22.5 \n",
    "\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e9aea-1d4c-40d6-93a3-96a05f159ad3",
   "metadata": {},
   "source": [
    "Calculate wind x and y vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f8cff7-bb5c-4f4d-9cd5-590369237f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors(row, azimuth_column, wind_column, speed_column):\n",
    "    angle = row[wind_column] - row[azimuth_column]\n",
    "    \n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row[speed_column]\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row[speed_column]\n",
    "\n",
    "    return pd.Series([x_vect, y_vect], index=['x_vect', 'y_vect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075fbd2-9e62-4e1b-a055-edff550f3ec8",
   "metadata": {},
   "source": [
    "Calculate Park x Weather Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18f382b6-7b53-49fc-9d22-981c9c143f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wfx(game_df, swishanalytics_df, venue_map_df, l_park_latest_df, r_park_latest_df, base_rate_df):\n",
    "    # Add box score data\n",
    "    game_df[['weather', 'wind', 'venue', 'date', 'missing_weather']] = game_df['game_id'].apply(lambda game_id: pd.Series(create_box(game_id)))\n",
    "    # Merge in roofType and azimuthAngle\n",
    "    weather_input_df = game_df.merge(venue_map_df[['id', 'fieldInfo.roofType', 'location.azimuthAngle']], left_on=['venue_id'], right_on=['id'], how='left')\n",
    "    # Clean weather data\n",
    "    weather_input_df = clean_weather(weather_input_df)\n",
    "\n",
    "    # Merge in Swish weather data, if available\n",
    "    if swishanalytics_df is not None:\n",
    "        # Merge on home team\n",
    "        swishanalytics_df['home_team'] = swishanalytics_df['FANGRAPHSTEAM'].map(team_dict)\n",
    "        weather_input_df['home_team'] = weather_input_df['home_team'].map(team_dict)\n",
    "        weather_input_df = pd.merge(weather_input_df, swishanalytics_df.drop_duplicates(), on=['home_team'], how='left') # Note: double headers don't seem to get different weather, so who cares?\n",
    "        # Reverse wind direction for calculating angles (set to N for no wind - won't matter)\n",
    "        weather_input_df['Swish_Direction'] = weather_input_df['Direction'].fillna(\"N\").apply(wind_reverser)\n",
    "        # Calculate angle wind is blowing to\n",
    "        weather_input_df['Swish_Angle'] = weather_input_df['Swish_Direction'].apply(find_degree)\n",
    "        # Calculate wind vectors\n",
    "        weather_input_df[['Swish_x_vect', 'Swish_y_vect']] = weather_input_df.apply(lambda row: calculate_vectors(row, 'location.azimuthAngle', 'Swish_Angle', 'Speed'), axis=1)\n",
    "        # Standardize temperature name\n",
    "        weather_input_df['Swish_temperature'] = weather_input_df['Temperature'].copy()\n",
    "\n",
    "        # Take Swish Analytics weather if MLB Stats API is missing weather\n",
    "        for column in ['x_vect', 'y_vect', 'temperature']:\n",
    "            weather_input_df[column] = np.where(weather_input_df['missing_weather'] == True, weather_input_df[f'Swish_{column}'], weather_input_df[column])\n",
    "\n",
    "\n",
    "    ### Inputs\n",
    "    # Venue dummies\n",
    "    venue_dummies = [col for col in l_park_latest_df.columns if (col.startswith(\"venue_\") and not col.endswith(\"id\"))]\n",
    "\n",
    "    # Identify inputs\n",
    "    model_input_list = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        model_input_list += [f'{event}_lg', f'{event}_lg', f'{event}_pfx']\n",
    "    \n",
    "    ### LHB        \n",
    "    weather_input_l_df = pd.merge(weather_input_df, l_park_latest_df.drop(columns={'gamePk', 'game_date'}), on=['venue_id'], suffixes=(\"\", \"_l\"))\n",
    "    # Predicted outputs\n",
    "    wfx_l_columns = [f\"{col}_wfx_l\" for col in list(predict_wfx_r.classes_)]\n",
    "    weather_input_l_df[wfx_l_columns] = predict_wfx_l.predict_proba(weather_input_l_df[model_input_list].values)\n",
    "\n",
    "    # Convert to PFX\n",
    "    for event in events_list:\n",
    "        weather_input_l_df[f\"{event}_wfx_l\"] = weather_input_l_df[f\"{event}_wfx_l\"] / base_rate_df[event][0]\n",
    "\n",
    "    ### RHB    \n",
    "    weather_input_r_df = pd.merge(weather_input_df, r_park_latest_df.drop(columns={'gamePk', 'game_date'}), on=['venue_id'], suffixes=(\"\", \"_r\"))\n",
    "\n",
    "    # Predicted outputs\n",
    "    wfx_r_columns = [f\"{col}_wfx_r\" for col in list(predict_wfx_r.classes_)]\n",
    "    weather_input_r_df[wfx_r_columns] = predict_wfx_r.predict_proba(weather_input_r_df[model_input_list].values)\n",
    "\n",
    "    # Convert to PFX\n",
    "    for event in events_list:\n",
    "        weather_input_r_df[f\"{event}_wfx_r\"] = weather_input_r_df[f\"{event}_wfx_r\"] / base_rate_df[event][0]\n",
    "\n",
    "    # Combine LHB and RHB weather effects\n",
    "    wfx_df = pd.concat([weather_input_l_df, weather_input_r_df[wfx_r_columns]], axis=1)\n",
    "\n",
    "    # Clean date\n",
    "    wfx_df['date'] = wfx_df['game_date'].str.replace(\"-\", \"\")\n",
    "    # Rename\n",
    "    wfx_df.rename(columns={'game_id': 'gamePk'}, inplace=True)\n",
    "\n",
    "    \n",
    "    return wfx_df[['gamePk', 'game_date', 'date', 'game_num', 'away_team', 'home_team', 'venue_id', 'x_vect', 'y_vect', 'temperature', 'weather'] + wfx_l_columns + wfx_r_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0aec-75ff-4191-8979-07eafb32b0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8a3b07-f812-4a86-87e6-c79adcbb62f1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e995618-7cd1-4af1-b01c-19e9ce1060e1",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a707003-941c-4acc-8fd0-596008959fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Scrape Swish Analytics\n",
    "    swishanalytics_df = swishanalytics(todaysdate)\n",
    "    # To CSV\n",
    "    swishanalytics_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Swish Analytics\", f\"Swish Analytics {todaysdate}.csv\"), index=False, encoding='iso-8859-1')\n",
    "except:\n",
    "    print(\"Could not scrape Swish Analytics weather data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b471b-a217-478d-ae6f-6704d87675c0",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7527eda-bb27-46c5-8b7b-7ed0e4a30022",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Scrape RotoGrinders\n",
    "    rotogrinders_df = rotogrinders(todaysdate, team_map)\n",
    "    # To CSV\n",
    "    rotogrinders_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"2. RotoGrinders\", f\"RotoGrinders {todaysdate}.csv\"), index=False)\n",
    "except:\n",
    "    print(\"Could not scrape RotoGrinders weather data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa4ad3-4ddb-467c-80ae-28717461422d",
   "metadata": {},
   "source": [
    "##### 3. Park x Weather Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "efce2990-91c7-46ee-b162-20bd0e6a317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Swish data\n",
    "try:\n",
    "    swishanalytics_df = pd.read_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Swish Analytics\", f\"Swish Analytics {todaysdate}.csv\"), encoding='iso-8859-1')\n",
    "    if 'x_vect' in swishanalytics_df.columns:\n",
    "        swishanalytics_df.drop(columns=['x_vect', 'y_vect', 'CF'], inplace=True)\n",
    "        swishanalytics_df.rename(columns={'temperature': 'Temperature'}, inplace=True)\n",
    "except:\n",
    "    print(\"Swish Analytics data not available.\")\n",
    "    swishanalytics_df = None\n",
    "\n",
    "# Read in latest park data\n",
    "l_park_latest_df = pd.read_csv(os.path.join(baseball_path, \"Park Latest - LHB.csv\"))\n",
    "r_park_latest_df = pd.read_csv(os.path.join(baseball_path, \"Park Latest - RHB.csv\"))\n",
    "\n",
    "# Read in base rates\n",
    "base_rate_df = pd.read_csv(os.path.join(baseball_path, \"Base Rates.csv\"))\n",
    "\n",
    "# Extract day's games\n",
    "daily_game_df = game_df[game_df['date'].astype(str) == todaysdate].reset_index(drop=True)\n",
    "\n",
    "# Calculate PFX\n",
    "wfx_df = calculate_wfx(daily_game_df, swishanalytics_df, venue_map_df, l_park_latest_df, r_park_latest_df, base_rate_df)\n",
    "\n",
    "# Write to CSV\n",
    "wfx_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"3. Park and Weather Factors\", f\"{todaysdate} Park and Weather Factors.csv\"), index=False, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fea165cf-c64d-4e4e-b9fe-692f3674b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_df['away_team'].fillna(\"OAK\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83e676-83fc-4372-8746-c04410099e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
