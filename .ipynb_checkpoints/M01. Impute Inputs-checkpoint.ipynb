{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9e2ecd-906a-4598-bee8-ceb569b94e67",
   "metadata": {},
   "source": [
    "# M01. Impute Inputs\n",
    "- This imputes model inputs using Steamer projections\n",
    "- Type: Model\n",
    "- Run Frequency: Irregular\n",
    "- Sources:\n",
    "    - MLB API\n",
    "    - Steamer\n",
    "- Dates:\n",
    "    - Created: 1/28/2024\n",
    "    - Updated: 1/31/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cc308-feb2-41d5-8cf8-a461ad6ae2ba",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99989d7f-4603-42c9-a27f-5331b84e4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"U1. Imports.ipynb\"\n",
    "%run \"U2. Utilities.ipynb\"\n",
    "%run \"U3. Classes.ipynb\"\n",
    "%run \"U4. Datasets.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05147b04-9747-4bce-9576-738e8efd7030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import log_loss, classification_report, f1_score, make_scorer\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from tensorflow import keras\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03feeeee-7566-4fa7-a7b0-9dc536ddba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in park factors\n",
    "multiplier_df = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff256e5d-9037-44a5-96a9-7205a3082e83",
   "metadata": {},
   "source": [
    "### Batters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e418d-69a3-4206-8b8b-f79df11154fc",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac2fa0-b6ad-4945-8cd3-a43e7ea0bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = create_pa_inputs(multiplier_df, 2015, 2024, short=50, long=300, adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845085cb-7eaf-4d3f-8798-cbfd84170eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df.drop_duplicates(['gamePk', 'batter', 'b_L', 'p_L'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43fd16-a618-40e0-8979-db25e8b50411",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = hitters_df[['batter', 'date', 'b_L', 'p_L', 'imp_b'] + batter_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145780ed-7793-458e-8057-c6baee6a3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "batter_stats_scaler = StandardScaler()\n",
    "hitters_df[batter_inputs] = batter_stats_scaler.fit_transform(hitters_df[batter_inputs])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, f\"batter_stats_scaler_{todaysdate}.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(batter_stats_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70b22d-ea26-45fe-8e4f-39b364405933",
   "metadata": {},
   "source": [
    "##### Steamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d7068-b9c9-4f3b-bcf5-eb07d8d1bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Steamer hitters \n",
    "steamer_hitters_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_hitters_weekly_log.csv\"), encoding='iso-8859-1')\n",
    "# Clean\n",
    "steamer_hitters_df2 = clean_steamer_hitters(steamer_hitters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493f60f-a989-49b9-b043-c4cdad6f7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "batter_stats_fg_scaler = StandardScaler()\n",
    "steamer_hitters_df2[batter_stats_fg] = batter_stats_fg_scaler.fit_transform(steamer_hitters_df2[batter_stats_fg])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, f\"batter_stats_fg_scaler_{todaysdate}.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(batter_stats_fg_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99cd28-7224-4ecb-83f1-75a8042d3e22",
   "metadata": {},
   "source": [
    "##### Create compatible dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7bece-39ec-4ba5-9662-24252c7649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column steamer_date column in hitters_df equal to the highest number <= a number in this list of uniques\n",
    "steamer_dates = list(steamer_hitters_df2['date'].unique())\n",
    "\n",
    "# Define a function to find the largest number in \"steamer_dates\" less than or equal to a given \"date\"\n",
    "def find_steamer_date(date):\n",
    "    max_steamer_date = max(filter(lambda d: d <= date, steamer_dates), default=None)\n",
    "    return max_steamer_date\n",
    "\n",
    "# Apply the function to create the \"steamer_date\" column in your DataFrame\n",
    "hitters_df[\"steamer_date\"] = hitters_df[\"date\"].apply(find_steamer_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628181e-a882-478c-b176-425e6776f562",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056a781-2864-44df-9c62-11d6d4d6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steamer stats we want to keep\n",
    "batter_stats_fg_plus = ['mlbamid', 'steamerid', 'date'] + batter_stats_fg \n",
    "# Merge\n",
    "hitters_merged_df = pd.merge(hitters_df, steamer_hitters_df2[batter_stats_fg_plus], left_on=['batter', 'steamer_date'], right_on=['mlbamid', 'date'], how='inner')\n",
    "# Only keep those without missing data\n",
    "hitters_merged_df = hitters_merged_df.dropna(subset=batter_inputs).dropna(subset=batter_stats_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5928065-2556-4181-8989-5df4e7e55e35",
   "metadata": {},
   "source": [
    "##### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb6268-83dd-40cf-a609-8d26b497a6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add hands to use in imputation\n",
    "batter_stats_fg_imp = batter_stats_fg + ['b_L', 'p_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dfe54-c771-4f2a-8b05-b63006f3904d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the architecture of the neural network\n",
    "layers = (30, 30, 30)  # Example architecture\n",
    "\n",
    "batter_imputations_model_filename = f\"batter_imputations_model_{todaysdate}.sav\"\n",
    "print(batter_imputations_model_filename)\n",
    "\n",
    "# Create the MLPRegressor model\n",
    "batter_imputation_model = MLPRegressor(hidden_layer_sizes=layers, activation='relu', random_state=10, learning_rate_init=0.0001, max_iter=10)\n",
    "\n",
    "# Prepare data for training\n",
    "X_train = hitters_merged_df[batter_stats_fg_imp]\n",
    "y_train = hitters_merged_df[batter_inputs]\n",
    "\n",
    "# Train the model\n",
    "batter_imputation_model.fit(X_train, y_train)\n",
    "\n",
    "# Update y_train with imputed values\n",
    "hitters_merged_df.loc[:, batter_inputs] = batter_imputation_model.predict(X_train)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(batter_imputation_model, open(os.path.join(model_path, batter_imputations_model_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fd4a0-097d-4599-927c-78ce2f9ba4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce47a9ed-14d5-4631-98e6-a20a087cc3b4",
   "metadata": {},
   "source": [
    "### Pitchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5611736-0eeb-4379-904b-3b1ad38bc613",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b5de7-3680-4422-8b92-45e1b449ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_df = create_pa_inputs(multiplier_df, 2015, 2024, short=50, long=300, adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad1a1f-4209-4d8e-aff2-d885b1d6cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_df.drop_duplicates(['gamePk', 'pitcher', 'b_L', 'p_L'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462b11b-dc8a-40f9-afc9-367457c34231",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_df = pitchers_df[['pitcher', 'date', 'b_L', 'p_L', 'imp_p'] + pitcher_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad4577-d277-472e-a9ba-89786c504656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "pitcher_stats_scaler = StandardScaler()\n",
    "pitchers_df[pitcher_inputs] = pitcher_stats_scaler.fit_transform(pitchers_df[pitcher_inputs])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, f\"pitcher_stats_scaler_{todaysdate}.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pitcher_stats_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7825ce-6237-42ed-bf67-920bb567f049",
   "metadata": {},
   "source": [
    "##### Steamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a9c2d-b86f-4c2b-aa68-99eca469b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Steamer hitters \n",
    "steamer_pitchers_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_pitchers_weekly_log.csv\"), encoding='iso-8859-1')\n",
    "# Clean\n",
    "steamer_pitchers_df2 = clean_steamer_pitchers(steamer_pitchers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc838279-016c-4dc6-ab32-835ebef2c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "pitcher_stats_fg_scaler = StandardScaler()\n",
    "steamer_pitchers_df2[pitcher_stats_fg] = pitcher_stats_fg_scaler.fit_transform(steamer_pitchers_df2[pitcher_stats_fg])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, f\"pitcher_stats_fg_scaler_{todaysdate}.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pitcher_stats_fg_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12235cc-1303-4d1d-aeee-348847b4864d",
   "metadata": {},
   "source": [
    "##### Create compatible dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45f0de-4ea8-4ecb-aaae-6148088de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column steamer_date column in hitters_df equal to the highest number <= a number in this list of uniques\n",
    "steamer_dates = list(steamer_pitchers_df2['date'].unique())\n",
    "\n",
    "# Define a function to find the largest number in \"steamer_dates\" less than or equal to a given \"date\"\n",
    "def find_steamer_date(date):\n",
    "    max_steamer_date = max(filter(lambda d: d <= date, steamer_dates), default=None)\n",
    "    return max_steamer_date\n",
    "\n",
    "# Apply the function to create the \"steamer_date\" column in your DataFrame\n",
    "pitchers_df[\"steamer_date\"] = pitchers_df[\"date\"].apply(find_steamer_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260568f8-f4c5-4624-8737-f00f7235b886",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc859ce-02aa-43f9-96e9-51ce8e30f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steamer stats we want to keep\n",
    "pitcher_stats_fg_plus = ['mlbamid', 'steamerid', 'date'] + pitcher_stats_fg2 \n",
    "# Merge\n",
    "pitchers_merged_df = pd.merge(pitchers_df, steamer_pitchers_df2[pitcher_stats_fg_plus], left_on=['pitcher', 'steamer_date'], right_on=['mlbamid', 'date'], how='inner')\n",
    "# Only keep those without missing data\n",
    "pitchers_merged_df = pitchers_merged_df.dropna(subset=pitcher_inputs).dropna(subset=pitcher_stats_fg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258abfc-58ec-43e1-902f-8403c7fb2941",
   "metadata": {},
   "source": [
    "##### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1a55c-dafd-4bf1-bc7b-403148737881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the architecture of the neural network\n",
    "layers = (30, 30, 30)\n",
    "\n",
    "pitcher_imputations_model_filename = f\"pitcher_imputations_model_{todaysdate}.sav\"\n",
    "print(pitcher_imputations_model_filename)\n",
    "\n",
    "# Create the MLPRegressor model\n",
    "pitcher_imputation_model = MLPRegressor(hidden_layer_sizes=layers, activation='relu', random_state=10, learning_rate_init=0.0001, max_iter=10)\n",
    "\n",
    "# Prepare data for training\n",
    "X_train = pitchers_merged_df[pitcher_stats_fg_imp]\n",
    "y_train = pitchers_merged_df[pitcher_inputs]\n",
    "\n",
    "# Train the model\n",
    "pitcher_imputation_model.fit(X_train, y_train)\n",
    "\n",
    "# Update y_train with imputed values\n",
    "pitchers_merged_df.loc[:, pitcher_inputs] = pitcher_imputation_model.predict(X_train)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(pitcher_imputation_model, open(os.path.join(model_path, pitcher_imputations_model_filename), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
