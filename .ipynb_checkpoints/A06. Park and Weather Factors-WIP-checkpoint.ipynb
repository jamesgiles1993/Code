{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# A06. Park and Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66c3d1-2a3f-4bc4-a77a-728a13dbbe93",
   "metadata": {},
   "source": [
    "Note: doing rollilng pas. Not sure if calendar date would be better. Has advantages with seasonality, but better hitting weather should be reflected in numerator and denominator even without that, so it might not matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b3626-a33a-4c54-bb97-e138778f4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\A02. MLB API.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2af29b-1396-412b-8f77-a1b5387395f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264d42db-933f-4e80-8ca0-c3380d6c6812",
   "metadata": {},
   "source": [
    "Calculate rolling average for a list of stats in a list <br>\n",
    "Note: This is NOT shifted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf6449-dd1d-4548-a9af-b67ce52e3e89",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6a652-424b-4398-9e68-969bd31994b6",
   "metadata": {},
   "source": [
    "Calculate Park Factors <br>\n",
    "Park Factor = Rate at Park (both teams) / Rate in Team's Away Games (both teams) <br>\n",
    "For example: Fenway Park HR Factor = HR rate at Fenway / HR rate in games where Red Sox are away team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd0a0a-c717-4075-a83c-5b45cd1096a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a4f3eef-a9f7-40ef-80dd-85ad1f9b3cbc",
   "metadata": {},
   "source": [
    "### Period Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a0132-0c07-4a88-9c18-1a717db8af94",
   "metadata": {},
   "source": [
    "Average of stats over period of interest, used as base for calculating multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ca187-5395-4ea1-b363-7d19e576b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_averages(df):\n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Keep recent years\n",
    "    df = df[df['game_date'] >= '01-01-2015']\n",
    "\n",
    "    \n",
    "    period_avgs = pd.DataFrame(df[events_list].mean()).T\n",
    "\n",
    "    \n",
    "    return period_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62568bb-2a20-427a-9297-75ace80ea4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8014006f-96d7-4585-9057-227ced68ddd6",
   "metadata": {},
   "source": [
    "### Game Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e666ca-838c-4618-81aa-c72b3ce5d445",
   "metadata": {},
   "source": [
    "Events that occurred in game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45aa75-f1b8-4306-af33-348b6a290d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(df):\n",
    "    game_avgs = df.groupby(['gamePk', 'game_date', 'venue_id', 'x_vect', 'y_vect', 'temperature'])[events_list].mean().reset_index()\n",
    "\n",
    "    return game_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2432b3c-374b-4bea-a8fc-9751ff92e5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ccf61d-9004-4235-8012-4eaec25581a6",
   "metadata": {},
   "source": [
    "### Player Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d800807-ddd1-4eba-bc31-a4d09f0dcde7",
   "metadata": {},
   "source": [
    "Averages of stats for players in game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c2155-abe4-4ffe-abe5-511502274f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_averages(df):\n",
    "    # Note: these are already shifted if using A02. create_pa_inputs. We want the first PA for players in each game.\n",
    "    # Stats to average\n",
    "    batter_inputs_short = [f\"{event}_b_long\" for event in events_list]\n",
    "    pitcher_inputs_short = [f\"{event}_p_long\" for event in events_list]\n",
    "\n",
    "    # Apply stats from first at bat to entire game\n",
    "    # First at bat has stats through end of last game\n",
    "    # This ensures that no stats generated in-game are reflected\n",
    "    # Note: we're doing this instead of dropping duplicates to properly weight by PA\n",
    "    df[batter_inputs_short] = df.groupby(['gamePk', 'batter'])[batter_inputs_short].transform('first')\n",
    "    df[pitcher_inputs_short] = df.groupby(['gamePk', 'pitcher'])[pitcher_inputs_short].transform('first')\n",
    "    \n",
    "    # Calculate player averages by game\n",
    "    batter_avgs = df.groupby(['gamePk'])[batter_inputs_short].mean().reset_index()\n",
    "    pitcher_avgs = df.groupby(['gamePk'])[pitcher_inputs_short].mean().reset_index()\n",
    "\n",
    "    # Merge together\n",
    "    player_avgs = pd.merge(batter_avgs, pitcher_avgs, on='gamePk', how='inner')\n",
    "\n",
    "    return player_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1948c7-a448-4156-a1bc-533b4da65062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eade1fe-2ccf-4d24-abe6-8ce3fa1d2078",
   "metadata": {},
   "source": [
    "### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde60c6-9dfe-4d4b-91b6-574d0a164bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def league_averages(df, league_window, league_window_min, base_year=2014):    \n",
    "    # Calculate rolling average of stats\n",
    "    league_avgs = df[events_list].rolling(window=league_window, min_periods=league_window_min).mean()\n",
    "    league_avgs.columns = [f'{col}_league' for col in league_avgs.columns]\n",
    "\n",
    "    # Keep column names in a list\n",
    "    column_names = league_avgs.columns\n",
    "\n",
    "    # Add game date onto stats\n",
    "    league_avgs = pd.concat([df[['game_date']], league_avgs], axis=1)\n",
    "\n",
    "    # Drop duplicates, keeping last\n",
    "    league_avgs.drop_duplicates('game_date', keep='last', inplace=True)\n",
    "\n",
    "    # Shift so dates reflect stats through the end of the prior date\n",
    "    league_avgs[column_names] = league_avgs[column_names].shift(1)\n",
    "\n",
    "    # Create date variables\n",
    "    league_avgs['game_date'] = pd.to_datetime(league_avgs['game_date'])\n",
    "    league_avgs['month'] = league_avgs['game_date'].dt.month\n",
    "    league_avgs['day'] = league_avgs['game_date'].dt.day\n",
    "    \n",
    "    # Subset base year\n",
    "    base_year_df = league_avgs[league_avgs['game_date'].dt.year == base_year]\n",
    "    \n",
    "    # Merge on base year\n",
    "    league_avgs = pd.merge(league_avgs, base_year_df, on=['month', 'day'], how='left', suffixes=(\"\", \"_base\"))\n",
    "    \n",
    "    # Identify columns that contain '_base' in their names\n",
    "    base_columns = [col for col in league_avgs.columns if '_base' in col]\n",
    "\n",
    "    # Apply forward fill to those columns\n",
    "    league_avgs[base_columns] = league_avgs[base_columns].ffill()\n",
    "\n",
    "    # Create multipliers\n",
    "    for column in column_names:\n",
    "        league_avgs[column] = league_avgs[column] / league_avgs[f'{column}_base']\n",
    "        league_avgs[column].fillna(1, inplace=True)\n",
    "    \n",
    "    keep_list = ['game_date'] + list(column_names)\n",
    "    \n",
    "    league_avgs = league_avgs[keep_list]\n",
    "    \n",
    "    return league_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8434c-0b34-4055-9f83-81619e947eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51ff7bbb-7029-47bb-ab38-0b3ea6960fa5",
   "metadata": {},
   "source": [
    "### Park Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba7e1a-578b-4577-9121-b6f353c281f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_averages(df, park_window, park_window_min):\n",
    "    # Calculate rolling averages by park \n",
    "    park_avgs = df.groupby('venue_id')[events_list].rolling(window=park_window, min_periods=park_window_min).mean()\n",
    "   \n",
    "    # Reset index to align with original DataFrame\n",
    "    park_avgs = park_avgs.reset_index(level=0, drop=False)\n",
    "    \n",
    "    # Rename columns to indicate they are park averages\n",
    "    for column in park_avgs[events_list]:\n",
    "        park_avgs.rename(columns={column: f\"{column}_park\"}, inplace=True)\n",
    "\n",
    "    # Sort to return to correct ordering\n",
    "    park_avgs.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    # Add in date\n",
    "    park_avgs = pd.concat([df[['game_date', 'home_name']], park_avgs], axis=1)\n",
    "\n",
    "    # Only keep one observation per park\n",
    "    park_avgs.drop_duplicates(['game_date', 'venue_id'], keep='last', inplace=True)\n",
    "\n",
    "    \n",
    "    column_names = [column for columns in park_avgs.columns if \"_park\" in column]\n",
    "    \n",
    "    # Shift so dates reflect stats through the end of the prior date\n",
    "    park_avgs.groupby(['game_date', 'venue_id'])[column_names].shift(1)\n",
    "    \n",
    "    return park_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ce0dc-a836-43ed-99f2-5f8dfaa6d0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc175a30-0a3c-408d-a79c-1bea36a0e0d8",
   "metadata": {},
   "source": [
    "### Team Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676c68f-6e8f-44ff-b784-3e19456954ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_averages(df, park_window, park_window_min):\n",
    "    # Calculate rolling averages by park \n",
    "    team_avgs = df.groupby('away_name')[events_list].rolling(window=park_window, min_periods=park_window_min).mean()\n",
    "   \n",
    "    # Reset index to align with original DataFrame\n",
    "    team_avgs = team_avgs.reset_index(level=0, drop=False)\n",
    "    \n",
    "    # Rename columns to indicate they are park averages\n",
    "    for column in team_avgs[events_list]:\n",
    "        team_avgs.rename(columns={column: f\"{column}_team\"}, inplace=True)\n",
    "\n",
    "    # Sort to return to correct ordering\n",
    "    team_avgs.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    # Add in date\n",
    "    team_avgs = pd.concat([df[['game_date']], team_avgs], axis=1)\n",
    "\n",
    "    # Only keep one observation per park\n",
    "    team_avgs.drop_duplicates(['game_date', 'away_name'], keep='last', inplace=True)\n",
    "    \n",
    "    return team_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e9cd5-9e20-4bb4-a40a-abed3d4f01b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be0f1a89-9b10-4a36-9305-8deea95894bd",
   "metadata": {},
   "source": [
    "### Calculate Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52af922-04b1-4d3a-be7a-922a5eb3499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify last date in team_avgs before given dates in park_avgs\n",
    "# latest_dates = []\n",
    "\n",
    "# for index, row in park_avgs.iterrows():\n",
    "#     # Filter team_avgs based on criteria\n",
    "#     filtered_team_avgs = team_avgs[(team_avgs['away_name'] == row['home_name']) & (team_avgs['game_date'] < row['game_date'])]\n",
    "    \n",
    "#     # Find the latest date in the filtered dataframe\n",
    "#     if not filtered_team_avgs.empty:\n",
    "#         latest_date = filtered_team_avgs['game_date'].max()\n",
    "#         latest_dates.append(latest_date)\n",
    "#     else:\n",
    "#         latest_dates.append(pd.NaT)  # Append NaT if no matching date found\n",
    "\n",
    "# # Add the latest_dates to park_avgs\n",
    "# park_avgs['last_road_date'] = latest_dates\n",
    "\n",
    "# # Merge \n",
    "# factor_df = pd.merge(park_avgs, team_avgs, left_on=['home_name', 'last_road_date'], right_on=['away_name', 'game_date'], how='left', suffixes=(\"\", \"_\"))\n",
    "\n",
    "# # Loop over event rates and calculate factors\n",
    "# for event in events_list:\n",
    "#     factor_df[f'{event}_factor'] = factor_df[f'{event}_park'].astype(float) / factor_df[f'{event}_team'].astype(float)\n",
    "\n",
    "# factor_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b361208-6427-4536-907a-bcf9ccc61a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "614ba2a5-5462-45e8-b124-de94768ad6e9",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e105f88-6836-4e16-a62f-4b9ee5225efb",
   "metadata": {},
   "source": [
    "Calculates Park x Weather multipliers and all necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be6f7b-dff7-464a-9bda-70dd98307251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, park_window, park_window_min, league_window, league_window_min, base_year, batSide=\"L\"):\n",
    "    # Only keep regular season games\n",
    "    df = df[df['game_type_x'] == \"R\"]\n",
    "    \n",
    "    # Only look at one side of the plate\n",
    "    df = df[df['batSide'] == batSide]\n",
    "    \n",
    "    # Reset index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Create uniform Cleveland name (only necessary for away team, but do both)\n",
    "    df['away_name'] = np.where(df['away_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", df['away_name'])\n",
    "    df['home_name'] = np.where(df['home_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", df['home_name'])\n",
    "\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Convert outputs to numeric (some are boolean)\n",
    "    df[events_list] = df[events_list].astype('float64')\n",
    "    \n",
    "\n",
    "    ### Game Averages\n",
    "    game_avgs = game_averages(df)\n",
    "\n",
    "    ### Player Averages\n",
    "    player_avgs = player_averages(df)\n",
    "\n",
    "\n",
    "    ### League Averages\n",
    "    league_avgs = league_averages(df, league_window, league_window_min, base_year)\n",
    "\n",
    "\n",
    "    ### Park Averages\n",
    "    park_avgs = park_averages(df, park_window, park_window_min)\n",
    "    park_avgs['home_name'] = np.where(park_avgs['home_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", park_avgs['home_name'])\n",
    "\n",
    "    ### Team Averages\n",
    "    team_avgs = team_averages(df, park_window, park_window_min)\n",
    "    team_avgs['away_name'] = np.where(team_avgs['away_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", team_avgs['away_name'])\n",
    "\n",
    "\n",
    "    ### Calculate Park Factors from Park Averages and Team Averages\n",
    "    # Identify last date in team_avgs before given dates in park_avgs\n",
    "    # This is so we can identify what the team was doing on the road leading up to their game at home\n",
    "    latest_dates = []\n",
    "    \n",
    "    for index, row in park_avgs.iterrows():\n",
    "        # Filter team_avgs based on criteria\n",
    "        filtered_team_avgs = team_avgs[(team_avgs['away_name'] == row['home_name']) & (team_avgs['game_date'] < row['game_date'])]\n",
    "        \n",
    "        # Find the latest date in the filtered dataframe\n",
    "        if not filtered_team_avgs.empty:\n",
    "            latest_date = filtered_team_avgs['game_date'].max()\n",
    "            latest_dates.append(latest_date)\n",
    "        else:\n",
    "            latest_dates.append(pd.NaT)  # Append NaT if no matching date found\n",
    "\n",
    "    # Add the latest_dates to park_avgs\n",
    "    park_avgs['last_road_date'] = latest_dates\n",
    "\n",
    "    # Merge \n",
    "    factor_df = pd.merge(park_avgs, team_avgs, left_on=['home_name', 'last_road_date'], right_on=['away_name', 'game_date'], how='left', suffixes=(\"\", \"_\"))\n",
    "    \n",
    "    # Calculate Park Factors\n",
    "    for event in events_list:\n",
    "        factor_df[f'{event}_factor'] = factor_df[f'{event}_park'].astype(float) / factor_df[f'{event}_team'].astype(float)\n",
    "\n",
    "    # Keep relevant columns\n",
    "    park_columns = [column for column in factor_df if \"_factor\" in column] + [column for column in factor_df if \"_park\" in column] + [column for column in factor_df if \"_team\" in column]\n",
    "    keep_columns = ['home_name', 'game_date', 'venue_id'] + park_columns\n",
    "\n",
    "    factor_df = factor_df[keep_columns]\n",
    "\n",
    "    # Cleveland has two names in the data. Need to treat as one.\n",
    "    factor_df['home_name'] = np.where(factor_df['home_name'] == \"Cleveland Indians\", \"Cleveland Guardian\", factor_df['home_name'])\n",
    "\n",
    "    factor_df['game_date'] = pd.to_datetime(factor_df['game_date'])\n",
    "\n",
    "    \n",
    "    ### Merge\n",
    "    dataset = pd.merge(game_avgs, player_avgs, on='gamePk', how='inner')\n",
    "    dataset = pd.merge(dataset, league_avgs, on='game_date', how='inner')\n",
    "    dataset = pd.merge(dataset, factor_df, on=['game_date', 'venue_id'], how='inner')\n",
    "\n",
    "    # Sort\n",
    "    dataset.sort_values('game_date', ascending=True, inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebab6f-8b6f-4967-96b5-46fa5de55c61",
   "metadata": {},
   "source": [
    "Subsets and cleans dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccf113-6fef-4466-9239-7e561ff66ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "    # Select active ballparks\n",
    "    active_dataset = dataset[dataset['venue_id'].astype(int).isin(list(team_map['VENUE_ID']))].reset_index(drop=True)\n",
    "    \n",
    "    # Restrict to 2015-\n",
    "    active_dataset = active_dataset[active_dataset['game_date'] >= '01-01-2015']\n",
    "    \n",
    "    # Create venue_id dummies\n",
    "    active_dataset['venue_id_copy'] = active_dataset['venue_id'].copy()\n",
    "    active_dataset = pd.get_dummies(active_dataset, columns=['venue_id_copy'], drop_first=False, prefix=\"venue\", prefix_sep=\"_\")\n",
    "    \n",
    "    # Create interactions of weather and park variables\n",
    "    weather_interactions = []\n",
    "    \n",
    "    for venue in team_map['VENUE_ID']:\n",
    "        for weather in ['x_vect', 'y_vect', 'temperature']:\n",
    "            active_dataset[f'venue_{venue}_{weather}'] = active_dataset[f'venue_{venue}'] * active_dataset[weather]\n",
    "            weather_interactions.append(f'venue_{venue}_{weather}')\n",
    "\n",
    "            \n",
    "    # weather_interactions = weather_interactions + [f'venue_{venue}' for venue in list(team_map['VENUE_ID'].unique())] + ['x_vect', 'y_vect', 'temperature']\n",
    "            \n",
    "    return active_dataset, weather_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43710809-bf8f-43ce-aedb-5a35e8226691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f69a71-9917-484a-b54a-358caf8c22c0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e483b01-4632-40a4-9321-4ca3ca581e9d",
   "metadata": {},
   "source": [
    "Runs model, training it if selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038851b-dcc6-4986-8668-412cfc86427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, event, batSide, weather_interactions, train=False):\n",
    "    # Define the dependent variable (y) and independent variables (X)\n",
    "    y = df.dropna()[f'{event}']\n",
    "\n",
    "    # Select model inputs\n",
    "    X_columns = [f'{event}_b_long', f'{event}_p_long', f'{event}_league', f'{event}_factor'] + weather_interactions\n",
    "    \n",
    "    # Drop missings\n",
    "    X = df.dropna()[X_columns]\n",
    "    # Convert columns to numeric\n",
    "    X[X_columns] = X[X_columns].astype(float)\n",
    "\n",
    "    # Add a constant to the independent variables matrix to include an intercept in the model\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Add a constant to the independent variables matrix to include an intercept in the model\n",
    "    if train == True:  \n",
    "        # Fit the linear regression model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "    else:\n",
    "        # Select model      \n",
    "        model = globals().get(f'{event}_{str.lower(batSide)}_model')\n",
    "\n",
    "    return model, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782d0ea-c143-4f99-9033-08bb066355df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# def run_model(df, event, batSide, weather_interactions, train=False):\n",
    "#     # Define the dependent variable (y) and independent variables (X)\n",
    "#     y = df.dropna()[f'{event}']\n",
    "\n",
    "#     # Select model inputs\n",
    "#     X_columns = [f'{event}_b_long', f'{event}_p_long', f'{event}_league', f'{event}_factor'] + weather_interactions\n",
    "    \n",
    "#     # Drop missings\n",
    "#     X = df.dropna()[X_columns]\n",
    "#     # Convert columns to numeric\n",
    "#     X[X_columns] = X[X_columns].astype(float)\n",
    "\n",
    "#     # Add a constant to the independent variables matrix to include an intercept in the model\n",
    "#     X = sm.add_constant(X)\n",
    "    \n",
    "#     if train == True:  \n",
    "#         # Fit the neural network model\n",
    "#         model = MLPRegressor(hidden_layer_sizes=(50,50,50,50), max_iter=1000)  # You can adjust parameters as needed\n",
    "#         model.fit(X, y)\n",
    "\n",
    "#     else:\n",
    "#         # Select model      \n",
    "#         model = globals().get(f'{event}_{str.lower(batSide)}_model')\n",
    "\n",
    "#     return model, X\n",
    "\n",
    "# # WHY DIVIDE BY ACVG WHEN IT SHOULD BE BASE YEAR?\n",
    "# # Also shouold league be based on a moving target?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07cf61d-fb31-4765-b2c1-f91e98bc41ba",
   "metadata": {},
   "source": [
    "Apply predictions to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f9a04-01d1-40c4-98df-b5ecf6e30e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(active_dataset, batSide, weather_interactions, train=False):\n",
    "    model_dictionary = {}\n",
    "    for event in events_list:\n",
    "        print(event)\n",
    "        # Train model\n",
    "        model, X = run_model(active_dataset, event, batSide, weather_interactions, train)\n",
    "        # If we trained a new model,\n",
    "        if train == True:\n",
    "            # Save model\n",
    "            pickle.dump(model, open(os.path.join(model_path, f\"Weather Model - {event} {batSide} {todaysdate}\"), 'wb'))\n",
    "            # Save to dictionary\n",
    "            # model_dictionary[event] = model.summary()\n",
    "    \n",
    "        # Replace with average\n",
    "        X[f'{event}_b_long'] = period_avgs[f'{event}'][0]\n",
    "        X[f'{event}_p_long'] = period_avgs[f'{event}'][0]\n",
    "    \n",
    "        # Predict\n",
    "        X[f'{event}_pred'] = model.predict(X)\n",
    "        X[f'{event}_mult'] = X[f'{event}_pred'] / period_avgs[f'{event}'][0]\n",
    "    \n",
    "        # Copy predicted rate and multiplier to active_dataset\n",
    "        active_dataset[f'{event}_pred'] = X[f'{event}_pred'].copy()    \n",
    "        active_dataset[f'{event}_mult'] = X[f'{event}_mult'].copy()\n",
    "\n",
    "    return active_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a079a3-20eb-4c7b-b78e-cfdbc01446e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8a3b07-f812-4a86-87e6-c79adcbb62f1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc8064-68e4-4b02-a342-5eaa6e51441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset\n",
    "# complete_dataset = create_pa_inputs(park_factors, team_map, 2013, 2024, short=50, long=300, adjust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6f4bd-29e0-4583-bcf5-748190a363b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate averages of each stat\n",
    "# period_avgs = period_averages(complete_dataset)\n",
    "# period_avgs.to_csv(os.path.join(baseball_path, \"Period Averages.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ecb1d-ed9e-4fde-bcfc-121236531d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c482d9c-de50-4309-bca9-8e53e0a10c69",
   "metadata": {},
   "source": [
    "##### LHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99047f-a973-4403-a7db-ca675d2d7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batSide = \"L\"\n",
    "# dataset_l = create_dataset(complete_dataset, park_window=8000, park_window_min=4000, league_window=50000, league_window_min=50000, base_year=2014, batSide=batSide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf1dc0-ebf0-4107-b253-6bc5dfce63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_dataset_l, weather_interactions = clean_dataset(dataset_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f9977-af7f-4215-aa2a-e327b85458b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_dataset_l = run_predictions(active_dataset_l, batSide, weather_interactions, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb82432-5f11-4ab2-bd9b-1b5b5d6b9b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7551b794-9ffe-4fb6-8640-7e788602c3f2",
   "metadata": {},
   "source": [
    "##### RHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090ffc1-e822-48af-a47c-a85d6db53c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batSide = \"R\"\n",
    "# dataset_r = create_dataset(complete_dataset, park_window=8000, park_window_min=4000, league_window=50000, league_window_min=50000, base_year=2014, batSide=batSide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f35774-4b7b-4260-829c-604e655502f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_dataset_r, weather_interactions = clean_dataset(dataset_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51c74e-7346-49f1-bab2-dbc856370a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_dataset_r = run_predictions(active_dataset_r, batSide, weather_interactions, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486dae7-96fc-4508-8a40-c8403658b3b6",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f80bc4-557e-42ef-b6f4-03af804f9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['gamePk', 'game_date', 'x_vect', 'y_vect', 'temperature']\n",
    "# league_avg_columns = [column for column in active_dataset_r if \"league\" in column]\n",
    "# factor_columns = [column for column in active_dataset_r if \"factor\" in column]\n",
    "# multiplier_columns = [column for column in active_dataset_r if \"mult\" in column]\n",
    "# venue_columns = [column for column in active_dataset_r if \"venue_\" in column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76fde0a-1cf3-48f0-9456-256d29e6bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplier_dataset = pd.merge(active_dataset_l[columns + venue_columns + league_avg_columns + factor_columns + multiplier_columns], active_dataset_r[columns + league_avg_columns + venue_columns + factor_columns + multiplier_columns], on=columns+venue_columns, how='left', suffixes=(\"_l\", \"_r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74274d1-0b01-440d-87d9-77de6b3e2005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f64c49-0823-4bc0-9f89-50b5bd603094",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f01e84-fc60-4836-84dc-bc74339e0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplier_dataset.to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b658e8-9062-4ab0-ac81-e1d89583addc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
