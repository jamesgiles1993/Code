{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333deb8-8bf3-4890-a652-0eee019ab5f8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd4f0ff-0070-4710-b2b9-caccc2aa1f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running imports...\n",
      "Imports in.\n"
     ]
    }
   ],
   "source": [
    "if \"running_pipeline\" not in globals():\n",
    "    print(\"Running imports...\")\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\"\n",
    "    print(\"Imports in.\")\n",
    "else:\n",
    "    print(\"Imports already in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4a660-9bd9-438f-a1c5-82a22c48fedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6910f3-e291-4228-8e49-67fd0f2af155",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7f0435-66c0-4ef9-8cd6-a5af0717bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_pipeline\" not in globals():\n",
    "    write_complete_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e388e4ae-f454-4342-bd6f-04b9583802a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"20240101\"\n",
    "end_date = todaysdate\n",
    "write_complete_dataset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a444b67-20ef-49ed-a823-4743182bfb15",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1b4fc0-42eb-457e-9562-3fc39d0076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = create_games(start_date, end_date, team_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9e9c20-a6bb-4085-9f38-b43077302024",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = game_df[game_df['status'] != \"Postponed\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308ec75-c581-4908-a6fb-1beb2e8a582a",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cb519-2136-4190-979b-80ef52acc92e",
   "metadata": {},
   "source": [
    "##### Create Matchup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522388e3-aa51-4fcb-9b4b-cb62917e7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matchup_file(game_df, row, complete_dataset, steamer_hitters_df, steamer_pitchers_df, team_map):\n",
    "    game_id = game_df['game_id'][row]\n",
    "    game_datetime = game_df['game_datetime'][row]\n",
    "    game_date = game_df['game_date'][row]\n",
    "    date = int(game_date.replace(\"-\", \"\"))\n",
    "    away_id = game_df['away_id'][row]\n",
    "    home_id = game_df['home_id'][row]\n",
    "    # Retrieve Baseball Reference team abbreviation\n",
    "    team_map_cut = team_map[['teamId', 'BBREFTEAM']].set_index('teamId')\n",
    "    \n",
    "    away_team = team_map_cut.loc[away_id]['BBREFTEAM']\n",
    "    home_team = team_map_cut.loc[home_id]['BBREFTEAM']\n",
    "    \n",
    "    for team in away_team, home_team:\n",
    "        # Read in rosters\n",
    "        roster_df = pd.read_csv(os.path.join(baseball_path, \"A05. Rosters\", \"2. Rosters\", f\"Rosters {date}\", f\"Roster {team} {date}.csv\"), encoding='iso-8859-1')\n",
    "\n",
    "        # Read in batting orders\n",
    "        order_df = pd.read_csv(os.path.join(baseball_path, \"A05. Rosters\", \"1. Batting Orders\", f\"Batting Orders {date}\", f\"Batting Order {team} {game_id}.csv\"), encoding='iso-8859-1')        \n",
    "        \n",
    "        # Read in bullpens\n",
    "        bullpen_df = pd.read_csv(os.path.join(baseball_path, \"A04. Bullpens\", f\"Bullpens {date}\", f\"Bullpen {team} {date}.csv\"), encoding='iso-8859-1')  \n",
    "        \n",
    "        # Merge batting order onto roster\n",
    "        team_df = pd.merge(roster_df, order_df[['id', 'fullName', 'position', 'status', 'order']], on='id', how='outer', suffixes=(\"\",\"2\"))\n",
    "        \n",
    "        # Fill in missings\n",
    "        team_df['batSide'].fillna('Right', inplace=True)\n",
    "        team_df['pitchHand'].fillna('Right', inplace=True)\n",
    "        team_df['fullName'].fillna(team_df['fullName2'], inplace=True)\n",
    "        team_df['position'].fillna(team_df['position2'], inplace=True)\n",
    "        \n",
    "        # Merge pitcher leverage onto roster\n",
    "        ### Testing\n",
    "        team_df['fullName'] = team_df['fullName'].apply(remove_accents)\n",
    "        ### Testing\n",
    "        team_df = pd.merge(team_df, bullpen_df[['Name', 'Leverage']], left_on='fullName', right_on='Name', how='left')\n",
    "        \n",
    "        # Add weather\n",
    "        box = create_box(game_id)\n",
    "        team_df['weather'] = box[0]\n",
    "        team_df['wind'] = box[1]\n",
    "        team_df['park'] = box[2]\n",
    "        team_df = clean_weather(team_df)\n",
    "\n",
    "        # Add venue\n",
    "        team_df['venue_id'] = game_df['venue_id'][row]\n",
    "        \n",
    "        # Add starters\n",
    "        team_df['away_starter'] = game_df['away_probable_pitcher'][row]\n",
    "        team_df['home_starter'] = game_df['home_probable_pitcher'][row]\n",
    "\n",
    "        team_df['away_starter'] = team_df['away_starter'].apply(remove_accents)\n",
    "        team_df['home_starter'] = team_df['home_starter'].apply(remove_accents)\n",
    "        \n",
    "        \n",
    "        # Assign Leverage of 1 to starting pitcher\n",
    "        team_df['Leverage'] = np.where((team_df['fullName'] == team_df['away_starter']) | (team_df['fullName'] == team_df['home_starter']), 1, team_df['Leverage'])\n",
    "\n",
    "        # Determine batting order\n",
    "        team_df['order'] = pd.to_numeric(team_df['order'], errors='coerce')\n",
    "        team_df['batting_order'] = np.nan\n",
    "        for i in range(9):\n",
    "            team_df['batting_order'] = np.where(team_df['order'] == (i+1)*100, i+1, team_df['batting_order'])\n",
    "\n",
    "        ### Batters\n",
    "        batter_df = team_df[team_df['position'] != \"Pitcher\"]\n",
    "\n",
    "        ## Dataset\n",
    "        # Vs. LHP\n",
    "        vs_l = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "        vs_l = vs_l[vs_l['pitchHand'] == \"L\"]\n",
    "        vs_l.drop_duplicates(subset='batter', keep='last', inplace=True)\n",
    "\n",
    "        # Merge in stats\n",
    "        batter_df = pd.merge(batter_df, vs_l[['batter'] + batter_inputs + ['imp_b', 'pa_b', 'pa_b_long']], left_on='id', right_on='batter', how='left')\n",
    "\n",
    "        # Vs. RHP\n",
    "        vs_r = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "        vs_r = vs_r[vs_r['pitchHand'] == \"R\"]\n",
    "        vs_r.drop_duplicates(subset='batter', keep='last', inplace=True)\n",
    "\n",
    "        # Merge in stats\n",
    "        batter_df = pd.merge(batter_df, vs_r[['batter'] + batter_inputs + ['imp_b', 'pa_b', 'pa_b_long']], left_on='id', right_on='batter', how='left', suffixes=(\"_l\", \"_r\"))\n",
    "\n",
    "        ## Steamer \n",
    "        # Keep last observation before date (may switch to <= if I find projections are up early)\n",
    "        steamer_hitters_last_df = steamer_hitters_df[steamer_hitters_df['date'] <= int(date)]\n",
    "        steamer_hitters_last_df.drop_duplicates(subset='mlbamid', keep='last', inplace=True)\n",
    "\n",
    "        # Merge\n",
    "        batter_df = pd.merge(batter_df, steamer_hitters_last_df, left_on='id', right_on='mlbamid', how='left', suffixes=(\"\", \"_fg\"))\n",
    "\n",
    "        # Remove redundant variables\n",
    "        batter_df.drop(columns={'batter_l', 'batter_r', 'firstname', 'lastname', 'mlbamid', 'fullName2', 'position2'}, inplace=True)\n",
    "\n",
    "        # Clean\n",
    "        # batter_df = clean_order(batter_df)\n",
    "\n",
    "        # Move 'batting_order' to the desired position\n",
    "        batter_df.insert(batter_df.columns.get_loc('order') + 1, 'batting_order', batter_df.pop('batting_order'))\n",
    "\n",
    "        # Sort\n",
    "        batter_df.sort_values('batting_order', inplace=True)\n",
    "\n",
    "\n",
    "        ### Pitchers\n",
    "        pitcher_df = team_df[(team_df['position'] == \"Pitcher\") | (team_df['position'] == \"Two-Way Player\")]\n",
    "\n",
    "        ## Dataset\n",
    "        # Vs. LHB\n",
    "        vs_l = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "        vs_l = vs_l[vs_l['batSide'] == \"L\"]\n",
    "        vs_l.drop_duplicates(subset='pitcher', keep='last', inplace=True)\n",
    "\n",
    "        # Merge in stats\n",
    "        pitcher_df = pd.merge(pitcher_df, vs_l[['pitcher'] + pitcher_inputs + ['imp_p', 'pa_p', 'pa_p_long']], left_on='id', right_on='pitcher', how='left')\n",
    "\n",
    "        # Vs. RHB\n",
    "        vs_r = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "        vs_r = vs_r[vs_r['batSide'] == \"R\"]\n",
    "        vs_r.drop_duplicates(subset='pitcher', keep='last', inplace=True)\n",
    "\n",
    "        # Merge in stats\n",
    "        pitcher_df = pd.merge(pitcher_df, vs_r[['pitcher'] + pitcher_inputs + ['imp_p', 'pa_p', 'pa_p_long']], left_on='id', right_on='pitcher', how='left', suffixes=(\"_l\", \"_r\"))\n",
    "\n",
    "        ## Steamer \n",
    "        # Keep last observation before date (may switch to <= if I find projections are up early)\n",
    "        steamer_pitchers_last_df = steamer_pitchers_df[steamer_pitchers_df['date'] <= int(date)]\n",
    "        steamer_pitchers_last_df.drop_duplicates(subset='mlbamid', keep='last', inplace=True)\n",
    "\n",
    "        # Merge\n",
    "        pitcher_df = pd.merge(pitcher_df, steamer_pitchers_last_df, left_on='id', right_on='mlbamid', how='left', suffixes=(\"\", \"_fg\"))\n",
    "\n",
    "        # Remove redundant variables\n",
    "        pitcher_df.drop(columns={'pitcher_l', 'pitcher_r', 'firstname', 'lastname', 'mlbamid', 'fullName2', 'position2'}, inplace=True)\n",
    "\n",
    "        # Move 'batting_order' to the desired position\n",
    "        pitcher_df.insert(pitcher_df.columns.get_loc('order') + 1, 'batting_order', pitcher_df.pop('batting_order'))\n",
    "\n",
    "        # Sort\n",
    "        pitcher_df.sort_values('Leverage', inplace=True)\n",
    "\n",
    "        if team == away_team:\n",
    "            away_batter_df = batter_df.copy()\n",
    "            away_pitcher_df = pitcher_df.copy()\n",
    "        else:\n",
    "            home_batter_df = batter_df.copy()\n",
    "            home_pitcher_df = pitcher_df.copy()\n",
    "\n",
    "    # Drop duplicates: \n",
    "    away_batter_df.drop_duplicates('id', keep='last', inplace=True)\n",
    "    away_batter_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    home_batter_df.drop_duplicates('id', keep='last', inplace=True)\n",
    "    home_batter_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    away_pitcher_df.drop_duplicates('id', keep='last', inplace=True)\n",
    "    away_pitcher_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    home_pitcher_df.drop_duplicates('id', keep='last', inplace=True)\n",
    "    home_pitcher_df.reset_index(drop=True, inplace=True)\n",
    "       \n",
    "        \n",
    "    return away_batter_df, away_pitcher_df, home_batter_df, home_pitcher_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18cd04-b548-4a64-95fc-1059bae92f64",
   "metadata": {},
   "source": [
    "##### Create Matchup Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f803b624-5b13-4af6-ab8f-3276035da579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matchup_files(game_df, row, complete_dataset, steamer_hitters_df, steamer_pitchers_df, team_map):\n",
    "\n",
    "    # Extract IDs\n",
    "    game_id = game_df['game_id'][row]\n",
    "    away_id = game_df['away_id'][row]\n",
    "    home_id = game_df['home_id'][row]\n",
    "\n",
    "    # Retrieve Baseball Reference team abbreviation\n",
    "    team_map_cut = team_map[['teamId', 'BBREFTEAM']].set_index('teamId')\n",
    "    away_team = team_map_cut.loc[away_id]['BBREFTEAM']\n",
    "    home_team = team_map_cut.loc[home_id]['BBREFTEAM']    \n",
    "\n",
    "    # Extract date\n",
    "    game_date = game_df['game_date'][row]\n",
    "    game_date = game_date.replace(\"-\", \"\")\n",
    "    game_datetime = game_df['game_datetime'][row]\n",
    "\n",
    "    # Convert string to datetime object\n",
    "    utc_datetime = datetime.datetime.strptime(game_datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # Define the UTC timezone\n",
    "    utc_timezone = pytz.timezone(\"UTC\")\n",
    "\n",
    "    # Set the UTC timezone for the datetime object\n",
    "    utc_datetime = utc_timezone.localize(utc_datetime)\n",
    "\n",
    "    # Convert to Eastern Standard Time (EST)\n",
    "    est_timezone = pytz.timezone(\"US/Eastern\")\n",
    "    est_datetime = utc_datetime.astimezone(est_timezone)\n",
    "\n",
    "    # Format the result\n",
    "    formatted_time = est_datetime.strftime(\"%H%M\")\n",
    "\n",
    "\n",
    "    # Create position dfs\n",
    "    away_batter_df, away_pitcher_df, home_batter_df, home_pitcher_df = create_matchup_file(game_df, row, complete_dataset, steamer_hitters_df, steamer_pitchers_df, team_map)\n",
    "\n",
    "    \n",
    "    # Create folder, if it doesn't exist\n",
    "    os.makedirs(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {game_date}'), exist_ok=True)\n",
    "\n",
    "    # File name\n",
    "    matchup_file = f\"{away_team}@{home_team} {game_id} {formatted_time}\"\n",
    "\n",
    "    # Write to Excel\n",
    "    away_batter_df.to_excel(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {game_date}', f'{matchup_file}.xlsx'), sheet_name=\"AwayBatters\", engine='openpyxl', index=False)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {game_date}', f'{matchup_file}.xlsx'), mode='a', engine='openpyxl') as writer:  \n",
    "        home_batter_df.to_excel(writer, sheet_name='HomeBatters', index=False)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {game_date}', f'{matchup_file}.xlsx'), mode='a', engine='openpyxl') as writer:  \n",
    "        away_pitcher_df.to_excel(writer, sheet_name='AwayPitchers', index=False)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {game_date}', f'{matchup_file}.xlsx'), mode='a', engine='openpyxl') as writer:  \n",
    "        home_pitcher_df.to_excel(writer, sheet_name='HomePitchers', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210150f-f4c2-47ef-918d-bf4d1deba9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae3e2d03-1d8e-4c30-910f-ec08162cd440",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b58351-d1b9-4e03-addc-430b64196f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee73dc2-29c1-4422-a57f-c031f24e6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in park factors\n",
    "multiplier_df = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5989b7-f686-4506-b88c-6e59e878f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 26s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if write_complete_dataset == True:\n",
    "    # # Read in dataset \n",
    "    # complete_dataset = create_pa_inputs(multiplier_df, 2015, 2024, 50, 300, True)\n",
    "    complete_dataset = pd.read_csv(os.path.join(baseball_path, \"nn_dataset.csv\"))\n",
    "    \n",
    "    # Subset\n",
    "    complete_dataset = complete_dataset.query('date > 20210301')\n",
    "\n",
    "    # Read in Steamer hitters\n",
    "    steamer_hitters_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_hitters_weekly_log.csv\"), encoding='iso-8859-1')\n",
    "    steamer_hitters_df_current = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_hitters.csv\"), encoding='iso-8859-1')\n",
    "    steamer_hitters_df = pd.concat([steamer_hitters_df, steamer_hitters_df_current], axis=0)\n",
    "    steamer_hitters_df['proj_year'].fillna(2024, inplace=True)\n",
    "    steamer_hitters_df['proj_date'].fillna(todaysdate_dash, inplace=True)\n",
    "    steamer_hitters_df = clean_steamer_hitters(steamer_hitters_df)\n",
    "\n",
    "    # Read in Steamer pitchers\n",
    "    steamer_pitchers_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_pitchers_weekly_log.csv\"), encoding='iso-8859-1')\n",
    "    steamer_pitchers_df_current = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_pitchers.csv\"), encoding='iso-8859-1')\n",
    "    steamer_pitchers_df = pd.concat([steamer_pitchers_df, steamer_pitchers_df_current], axis=0)\n",
    "    steamer_pitchers_df['proj_year'].fillna(2024, inplace=True)\n",
    "    steamer_pitchers_df['proj_date'].fillna(todaysdate_dash, inplace=True)\n",
    "    steamer_pitchers_df = clean_steamer_pitchers(steamer_pitchers_df)\n",
    "\n",
    "    # Write to CSV (we'll read these later in B.)\n",
    "    complete_dataset.to_csv(os.path.join(baseball_path, \"Complete Dataset.csv\"), index=False)\n",
    "    steamer_hitters_df.to_csv(os.path.join(baseball_path, \"Steamer Hitters.csv\"), index=False)\n",
    "    steamer_pitchers_df.to_csv(os.path.join(baseball_path, \"Steamer Pitchers.csv\"), index=False)\n",
    "    \n",
    "else:\n",
    "    complete_dataset = pd.read_csv(os.path.join(baseball_path, \"Complete Dataset.csv\"))\n",
    "    steamer_hitters_df = pd.read_csv(os.path.join(baseball_path, \"Steamer Hitters.csv\"))\n",
    "    steamer_pitchers_df = pd.read_csv(os.path.join(baseball_path, \"Steamer Pitchers.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0b3da-ca40-46f2-a193-25c6ad8d8ed9",
   "metadata": {},
   "source": [
    "Shrink datasets for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bb6b04-b27c-4041-beea-7fe06885af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.drop(columns={'description', 'batterName', 'pitcherName', 'postOnFirst', 'postOnSecond', 'postOnThird', 'preOnFirst', 'preOnSecond', 'preOnThird', 'pitch_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1033c32-7b5e-4a23-86de-ea4f8d1fd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink datasets for faster processing\n",
    "# complete_dataset.drop(columns={'description', 'batterName', 'pitcherName', 'postOnFirst', 'postOnSecond', 'postOnThird', 'preOnFirst', 'preOnSecond', 'preOnThird', 'pitch_name'}, inplace=True)\n",
    "# complete_dataset = complete_dataset[complete_dataset['date'].astype(int) > game_df[\"date\"].astype(int).min()-10000]\n",
    "# steamer_hitters_df = steamer_hitters_df[steamer_hitters_df['date'].astype(int) > game_df[\"date\"].astype(int).min()-10000]\n",
    "# steamer_pitchers_df = steamer_pitchers_df[steamer_pitchers_df['date'].astype(int) > game_df[\"date\"].astype(int).min()-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b755de9-07c5-4a95-8641-83ce821569f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  4.4min\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\james\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 161, in _feed\n    send_bytes(obj_)\n  File \"C:\\Users\\james\\anaconda3\\envs\\MLB\\lib\\multiprocessing\\connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"C:\\Users\\james\\anaconda3\\envs\\MLB\\lib\\multiprocessing\\connection.py\", line 280, in _send_bytes\n    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)\nOSError: [WinError 1450] Insufficient system resources exist to complete the requested service\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLB\\lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(game_df))\n",
    "empty_list = Parallel(n_jobs=4, verbose=True)(delayed(create_matchup_files)(game_df, row, complete_dataset, steamer_hitters_df, steamer_pitchers_df, team_map) for row in range(len(game_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bc735-68e0-4098-b42f-86876e63010b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
