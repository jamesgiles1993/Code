{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979729d9-6cbc-49e6-b1f9-1ed4752677c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# M01. Park and Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f428d-fca4-4ef3-a971-3ffb018c9271",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff0bfd-7406-4d5f-94c2-fd55848b29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Functions.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8d7ee-374e-4e41-ae92-6fb846c28972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c846f6-f4bd-4907-addc-3591f2d5e7a7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b17e9d-8046-49d5-96c5-3f208dd40a34",
   "metadata": {},
   "source": [
    "##### MLB Stats API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04543a-92ca-4260-9d83-9dac3c9c4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = 2015, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577881e-1443-43ba-b36b-9a8d55d0ea80",
   "metadata": {},
   "source": [
    "Merge MLB Stats API and Statcast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd9f75-7287-4028-a443-d41e849419a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = merge_datasets(start_year, end_year)\n",
    "df = clean_weather(df)\n",
    "df = create_events(df)\n",
    "df = create_variables(df)\n",
    "df = start_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57545c85-c4df-4ac1-a981-c7ccec24474e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be631e83-1ffc-489f-b7d8-600332cbf744",
   "metadata": {},
   "source": [
    "##### Open Meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb82074-b8b9-4e23-9e78-d9dd454d06fb",
   "metadata": {},
   "source": [
    "Read in Open Meteo weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aff0e1-0630-4ef0-8edf-924930b6fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weather_df = pd.concat(map(pd.read_csv, glob.glob(r\"C:\\Users\\james\\Documents\\MLB\\Database\\A06. Weather\\1. Open Meteo\\*.csv\")), ignore_index=True)[\n",
    "       ['game_id', 'year', 'venue_name', 'location.defaultCoordinates.latitude', 'location.defaultCoordinates.longitude', \n",
    "        'fieldInfo.leftLine', 'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter', 'fieldInfo.rightCenter', 'location.elevation', 'location.azimuthAngle', 'fieldInfo.roofType', 'active', \n",
    "        'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'surface_pressure', 'wind_speed_10m', 'wind_direction_10m', 'weather_code', 'precipitation_probability']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64524c4-f4c3-4e76-a651-3a426d1add0b",
   "metadata": {},
   "source": [
    "Calculate wind vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7349a-419d-49a8-8742-c2c59a070add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors(row, azimuth_column, wind_column, speed_column):\n",
    "    angle = row[wind_column] - row[azimuth_column]\n",
    "    \n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row[speed_column] * -1\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row[speed_column] * -1\n",
    "\n",
    "    return pd.Series([x_vect, y_vect], index=['x_vect', 'y_vect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97e4de-f49c-4fa9-a9ee-155e099500e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[['meteo_x_vect', 'meteo_y_vect']] = weather_df.apply(lambda row: calculate_vectors(row, 'location.azimuthAngle', 'wind_direction_10m', 'wind_speed_10m'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2918d-8ff6-49e3-9ab9-64b0daf13423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6acdcd6e-e422-47b6-a85e-e0153ac5c751",
   "metadata": {},
   "source": [
    "##### Baserunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b285dc8-0196-4126-bc7a-5a9395622d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "steamer_hitters_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_hitters_weekly_log.csv\"), encoding='iso-8859-1', usecols=['proj_date', 'mlbamid', 'PA', 'UBR'], dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24144c-28a7-4960-9860-b0d30caa7630",
   "metadata": {},
   "source": [
    "Convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7eb49-f974-420f-9882-39c3127aa527",
   "metadata": {},
   "outputs": [],
   "source": [
    "steamer_hitters_df[['PA', 'UBR']] = steamer_hitters_df[['PA', 'UBR']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf4318-29af-4665-a825-3d4e939c3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "steamer_hitters_df['proj_date'] = pd.to_datetime(steamer_hitters_df['proj_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c80d7a-37ce-43c3-991b-dae3d5b161eb",
   "metadata": {},
   "source": [
    "Calculate UBR per 600 Plate Appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c96a0-8aa6-4e03-a5e1-689eff99ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "steamer_hitters_df['UBR600'] = steamer_hitters_df['UBR'] / steamer_hitters_df['PA'] * 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7692fdd-8b50-4934-8257-4e22ab608845",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725ff67-53b9-4714-9a1b-bda074902278",
   "metadata": {},
   "source": [
    "Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aeeb6a-5473-4095-aff2-1e737bf709c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = df.merge(weather_df.drop(columns=['year']), left_on=['gamePk'], right_on=['game_id'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3558a-b20b-4d35-856d-dc2a106a2bc5",
   "metadata": {},
   "source": [
    "Use weather column from MLB data to adjust for domes/roofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98307d00-947e-4c6e-b8c3-14add6ca1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = complete_dataset['weather'].str.contains('Roof|Dome', case=False, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c58b5-d2a5-49c3-b158-5d4d5b88e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.loc[mask, 'temperature'] = 70\n",
    "complete_dataset.loc[mask, 'x_vect'] = 0\n",
    "complete_dataset.loc[mask, 'y_vect'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a40be-8923-46fa-89b2-5318321403c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.loc[mask, 'temperature_2m'] = 70\n",
    "complete_dataset.loc[mask, 'meteo_x_vect'] = 0\n",
    "complete_dataset.loc[mask, 'meteo_y_vect'] = 0\n",
    "complete_dataset.loc[mask, 'relative_humidity_2m'] = 60\n",
    "complete_dataset.loc[mask, 'dew_point_2m'] = 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ab710-b0ca-4da4-91cf-e693154f3b1a",
   "metadata": {},
   "source": [
    "Baserunning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81749564-0d32-4019-89c1-7f3b93b294b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset['proj_date'] = pd.to_datetime(complete_dataset['date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06cfbd-5b30-4319-9b43-e54cfdbc2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset['mlbamid'] = complete_dataset['batter'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fc2d1-3ada-4454-ac94-4fb381edaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.merge_asof(\n",
    "    complete_dataset.sort_values('proj_date'),\n",
    "    steamer_hitters_df.sort_values('proj_date'),\n",
    "    by='mlbamid',\n",
    "    on='proj_date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8447bed-44eb-4a96-a0d6-5bb1d8fd0c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29a01cc-191b-4c07-8aab-ae19bb621c89",
   "metadata": {},
   "source": [
    "Note:\n",
    "- if y > 198.27 and x < 125.42), it's actually to left\n",
    "- if y > 198.27 and x > 125.42), it's actually to right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd146420-5276-4e4a-b0a7-3d5263a65fef",
   "metadata": {},
   "source": [
    "### Model #1. Expected Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33de55-4d6d-4406-aacd-0dff4564303b",
   "metadata": {},
   "source": [
    "Probability of events given how the baseball was launched, where it was launched to, and some information about the batter, including handedness and base running. Notably excluded park and weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728c479-4554-43be-b920-e8aaa2c1472a",
   "metadata": {},
   "source": [
    "$ \\hat{\\text{eventsModel}} = launch\\_angle + launch\\_speed + to\\_l + to\\_lc + to\\_c + to\\_rc + to\\_r + b\\_L + UBR600 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4b643-304b-426c-b517-792e875ae4ff",
   "metadata": {},
   "source": [
    "##### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2ecad-c36d-43e0-9ff3-d75390649f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_inputs = ['launch_angle', 'launch_speed', 'to_l', 'to_lc', 'to_c', 'to_rc', 'to_r', 'b_L', 'UBR600'] + ['bb', 'hbp', 'so']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0adbc9-85f6-4fa6-9a30-e9898e554dab",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69c238-6d8e-4a6f-b9ab-158b76f3b856",
   "metadata": {},
   "source": [
    "Sent launch data to 0 if not batted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c1f3e-30e9-49df-b504-5ced13a6acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset[['launch_angle', 'launch_speed']] = complete_dataset[['launch_angle', 'launch_speed']].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed38b6f-af3a-4b2e-bc77-e1cd4dacbe2d",
   "metadata": {},
   "source": [
    "Remove atypical events and missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8eff8-6cbf-4750-8013-915e0d8bdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = complete_dataset[~complete_dataset['eventsModel'].isin([\"Cut\"])].dropna(subset=outcome_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562a9ac-3e6e-43e7-b624-0a5de43237d8",
   "metadata": {},
   "source": [
    "Define model input and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170509c2-00fe-46a4-8a19-14be3aecbbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complete_dataset[outcome_inputs].values\n",
    "y = complete_dataset[['eventsModel']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c186089-661b-453d-986b-8db8eb552fb2",
   "metadata": {},
   "source": [
    "##### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2f490-7f40-4bf3-b689-63d51086068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not hasattr(sys.modules['__main__'], '__file__'): # Run if notebook is origin file\n",
    "if 1 == 2: # Force not to run regardless of origin file \n",
    "    # One-hot encode the target\n",
    "    encode_outcome = OneHotEncoder(sparse_output=False)\n",
    "    # Fit and transform\n",
    "    y_encoded = encode_outcome.fit_transform(y)\n",
    "    # Create folder\n",
    "    os.makedirs(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate), exist_ok=True)   \n",
    "    # Save\n",
    "    pickle.dump(encode_outcome, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, \"encode_outcome.pkl\"), 'wb'))\n",
    "else:\n",
    "    y_encoded = encode_outcome.transform(y)\n",
    "\n",
    "# Calculate number of classes (used for model inputs)\n",
    "num_classes = y_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158b352-5aa6-4b0f-a720-a91b8df0851c",
   "metadata": {},
   "source": [
    "##### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed7df9-6d79-42ab-bb15-82f83b5eefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not hasattr(sys.modules['__main__'], '__file__'): # Run if notebook is origin file\n",
    "if 1 == 2: # Force not to run regardless of origin file \n",
    "    # Scale\n",
    "    scale_outcome = StandardScaler()\n",
    "    # Fit and transform\n",
    "    X_scaled = scale_outcome.fit_transform(X)\n",
    "    # Save\n",
    "    pickle.dump(scale_outcome, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, \"scale_outcome.pkl\"), 'wb'))\n",
    "else:\n",
    "    X_scaled = scale_outcome.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae0b5a-b6b1-470c-a98e-5db04cffb09f",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f7752-3414-4e9e-824b-2944dbe3ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not hasattr(sys.modules['__main__'], '__file__'): # Run if notebook is origin file\n",
    "if 1 == 2: # Force not to run regardless of origin file \n",
    "    predict_outcome = Sequential([\n",
    "        Dense(32, input_shape=(X_scaled.shape[1],), activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # softmax for multi-class classification\n",
    "    ])\n",
    "    \n",
    "    predict_outcome.compile(optimizer=Adam(learning_rate=0.00001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',      # watch validation loss\n",
    "        patience=5,              # stop if no improvement after 5 epochs\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    predict_outcome.fit(\n",
    "        X_scaled, y_encoded,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    predict_outcome.save(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, 'predict_outcome.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f1228-6cd3-4b99-ab98-014fd1d3dcd1",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50225246-a67a-4ef8-85cc-ce28eb1981e5",
   "metadata": {},
   "source": [
    "Predicted rates of events based on batted-ball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301431d2-a5f6-43d1-b159-af1edfadaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_outcome.predict(X_scaled)\n",
    "\n",
    "prediction_df = pd.DataFrame(predictions, columns=encode_outcome.categories_[0])\n",
    "prediction_df = prediction_df.add_suffix('_pred_batted')\n",
    "\n",
    "prediction_df = pd.concat([complete_dataset.reset_index(drop=True), prediction_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bbb416-db3e-45a4-b804-c8c8751d095e",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f6edb-a2d1-4d36-aa28-a43580487637",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113ce42-7b1c-48f2-933c-a9f967168cc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust the number of rows and columns\n",
    "n_events = len(events_list)\n",
    "n_cols = 3\n",
    "n_rows = (n_events + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "# Set square plots: each subplot is 5x5 inches\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, event in enumerate(events_list):\n",
    "    ax = axes[i]\n",
    "    pred_col = f\"{event}_pred_batted\"\n",
    "    \n",
    "    if pred_col not in prediction_df.columns:\n",
    "        continue\n",
    "\n",
    "    # Bucket the predicted values into quantiles\n",
    "    prediction_df['bucket'] = pd.qcut(prediction_df[pred_col], q=20, duplicates='drop')\n",
    "\n",
    "    # Compute averages\n",
    "    bucket_avg = prediction_df.groupby('bucket').agg(\n",
    "        avg_pred=(pred_col, 'mean'),\n",
    "        avg_actual=(event, 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(bucket_avg['avg_pred'], label='Predicted')\n",
    "    ax.plot(bucket_avg['avg_actual'], label='Actual')\n",
    "    ax.set_title(f\"{event.upper()} Prediction vs Actual\")\n",
    "    ax.set_xlabel(\"Quantile Bucket\")\n",
    "    ax.set_ylabel(\"Rate\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Remove extra axes if any\n",
    "for j in range(n_events, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26eba8-4df2-4a1b-91cf-801951af40fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3775400-5c1f-4762-ac22-1c8bb1aabfe5",
   "metadata": {},
   "source": [
    "### Calculate PFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d1b63-e7fa-4afd-8b44-f41611891a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list_pred_batted = [f\"{event}_pred_batted\" for event in events_list]\n",
    "pfx_list = [f\"{event}_pfx\" for event in events_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d4e76-6b13-4652-afc5-31e32260c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_pfx_df = prediction_df.groupby(['venue_id', 'gamePk', 'batSide', 'date'])[events_list + events_list_pred_batted].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16813f82-bf9f-4955-b958-2f233a5fa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9bcb4-5850-46e6-b0ff-8e4d287312ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data is sorted appropriately for rolling\n",
    "game_pfx_df = game_pfx_df.sort_values(['venue_id', 'date', 'batSide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4e3f2-8cd9-47d6-8f91-7465fab4aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_pfx_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c951081-5a59-4e60-be80-2217ef98200a",
   "metadata": {},
   "source": [
    "##### Unshifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b2814-b9bd-4586-90ef-511179d5805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rolling average of the last num_games *including* the current row\n",
    "rolling_avgs = (\n",
    "    game_pfx_df\n",
    "    .groupby(['venue_id', 'batSide'], group_keys=False)\n",
    "    .apply(lambda group: group[events_list + events_list_pred_batted].shift(0).rolling(num_games, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# Rename columns to indicate they are rolling averages\n",
    "rolling_avgs.columns = [f'{col}_rolling' for col in events_list + events_list_pred_batted]\n",
    "\n",
    "# Concatenate with the original dataframe\n",
    "unshifted_game_pfx_df = pd.concat([game_pfx_df, rolling_avgs], axis=1)\n",
    "\n",
    "for event in events_list:\n",
    "    unshifted_game_pfx_df[f'{event}_pfx'] = unshifted_game_pfx_df[f'{event}_rolling'] / unshifted_game_pfx_df[f'{event}_pred_batted_rolling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c612c3-cc75-4bfc-92d4-534be19b700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unshifted_game_pfx_df[['venue_id', 'batSide'] + [col for col in unshifted_game_pfx_df if col.endswith(\"pfx\")]].drop_duplicates(subset=['venue_id', 'batSide'], keep='last').to_csv(os.path.join(baseball_path, \"Park Latest.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926c55a-e86c-4e9c-8a09-d693b58168f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5865847-227c-4e04-ad86-b8e28b9c3eab",
   "metadata": {},
   "source": [
    "##### Shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d0eff-e9bb-4802-a05b-fd95c8bb3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rolling average of the last num_games *excluding* the current row\n",
    "rolling_avgs = (\n",
    "    game_pfx_df\n",
    "    .groupby(['venue_id', 'batSide'], group_keys=False)\n",
    "    .apply(lambda group: group[events_list + events_list_pred_batted].shift(1).rolling(num_games, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# Rename columns to indicate they are rolling averages\n",
    "rolling_avgs.columns = [f'{col}_rolling' for col in events_list + events_list_pred_batted]\n",
    "\n",
    "# Concatenate with the original dataframe\n",
    "shifted_game_pfx_df = pd.concat([game_pfx_df, rolling_avgs], axis=1)\n",
    "\n",
    "for event in events_list:\n",
    "    shifted_game_pfx_df[f'{event}_pfx'] = shifted_game_pfx_df[f'{event}_rolling'] / shifted_game_pfx_df[f'{event}_pred_batted_rolling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909911c-bd8b-4966-be50-9e469f67fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1284a047-897c-47dc-8e1f-e4b5edb4e741",
   "metadata": {},
   "source": [
    "### Model #2. Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f068fb-18bb-473d-8bb7-084d0f26c28b",
   "metadata": {},
   "source": [
    "$ \\hat{\\text{eventsModel2}} = \\hat{\\text{eventsModel}} + pfx + meteo\\_x\\_vect + meteo\\_y\\_vect + temperature\\_2m + relative\\_humidity\\_2m + dew\\_point\\_2m + surface\\_pressure + venue\\_id $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06563318-07b1-4b0d-b92b-26e3d00de7bd",
   "metadata": {},
   "source": [
    "The purpose of this model is to estimate rates of events in games based on weather and venue. This model is trained with expected rates based on the actual batted ball data. This allows for us to control for differences in inherent batted ball data across games. The model then predicts with league average rates to determine how a game with typical batted ball data would differ in various weather and venue conditions. <br>\n",
    "Ideally, we would then compare these predicted rates to league average rates to determine park x weather factors, multipliers that estimate how much more or less likely given events are on the game-level than under average conditions. <br>\n",
    "However, this is hard. <br>\n",
    "Instead, predicted rates are used to assign park-specific quantiles to games based on weather conditions. For instance, a weather conditions that predict a 0.05 projected home run rate at Fenway may get a game assigned to the top quantile of games at that park. From this point, multipliers will be calculated by averaging actual home runs rates at similar games and dividing by batted-ball predicted home run rates at those same games. The result can be interpreted as a multiplier that determines how much more or less likely were home runs at games with similar weather conditions compared to their batted ball likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70325a82-52d2-41ea-90a1-feffc8f9a587",
   "metadata": {},
   "source": [
    "##### Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4774653-6605-44b4-ba74-352f5357b75a",
   "metadata": {},
   "source": [
    "Meteo weather inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da575162-d402-460b-a580-f01c803618aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_weather_list = ['meteo_x_vect', 'meteo_y_vect', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'surface_pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a7d69-ba13-4811-9d17-26f170c3dab8",
   "metadata": {},
   "source": [
    "Parks with sufficient samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfe8cf-caf3-487f-ae3d-9a278838bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_dummy_list = [f'venue_{id}' for id in sorted(prediction_df['venue_id'].value_counts()[lambda x: x > 20000].index.tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc175b7-8580-42cc-9b3e-ef4fbed95a9a",
   "metadata": {},
   "source": [
    "Select inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587b1cb-cfb1-4ef3-9cfa-6df6e2e7349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_inputs = events_list_pred_batted + pfx_list + meteo_weather_list + venue_dummy_list + ['b_L']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c41b96-d57f-4eb6-a13b-e60a5ebd8e5b",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e808e87-ebc3-47f3-925a-e412caae499d",
   "metadata": {},
   "source": [
    "Merge in park factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd02fa0-4e95-44b6-8774-c46bce8c8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2 = prediction_df.merge(shifted_game_pfx_df[['gamePk', 'batSide'] + pfx_list], on=['gamePk', 'batSide'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94483b-87e0-4bb0-bac3-c059b246a830",
   "metadata": {},
   "source": [
    "Create venue dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b750f-e27b-49b6-a806-3ce5f023c28d",
   "metadata": {},
   "source": [
    "Note: not all venue dummies may be included in venue_dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8d608-c4f1-4cc3-8e3c-a196a67dad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2['venue_id2'] = sample_df2['venue_id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364ec90-f0bd-4fc2-ba7b-995023ffb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2 = pd.get_dummies(sample_df2, columns=['venue_id2'], prefix='venue', drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be669683-d7de-4838-ad6b-43bb24057a52",
   "metadata": {},
   "source": [
    "Set pfx to 1 if not in venue sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf312c-1c00-44ff-95fd-6489c5d66577",
   "metadata": {},
   "source": [
    "Note: we may want to set this in shifted_game_pfx_df and default to a rolling value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9463340-14d9-4b4b-8d51-2923c7d73bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2['sample_venue'] = sample_df2[venue_dummy_list].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b4644-845d-4658-8215-82d556517e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pfx in pfx_list:\n",
    "    sample_df2[pfx] = np.where(sample_df2['sample_venue'] == 0, 1, sample_df2[pfx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406fb3a-d19b-43c4-bfc6-5ddb899919ff",
   "metadata": {},
   "source": [
    "Drop if missing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25f0a7-0b1d-4cb0-b817-11e1af8771eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2.dropna(subset=wfx_inputs, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967b47d-8734-439a-aab6-397142c0ecb6",
   "metadata": {},
   "source": [
    "Group by game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4198b83-ebf5-42b2-b041-97f92faf8f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2 = sample_df2.groupby(['gamePk', 'date', 'venue_id', 'batSide'])[wfx_inputs + events_list].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0283694-8e6a-45bb-a9c2-b973caa63d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2['b_L'] = (sample_df2['batSide'] == \"L\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4db6d-f7da-40b3-9dd3-661266a590ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df2 = sample_df2[sample_df2['date'] > 20180101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812e92a-3c3f-46fe-9dd3-a752f4b519c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = sample_df2[wfx_inputs].values\n",
    "y = sample_df2[events_list].values\n",
    "\n",
    "# Number of classes\n",
    "num_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b3ac3-5534-4fc1-9707-ddd0c8883bc6",
   "metadata": {},
   "source": [
    "###### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06b545-6099-412b-8141-25847fcc4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Scale\n",
    "    scale_wfx = StandardScaler()\n",
    "    # Fit and transform\n",
    "    X_scaled = scale_wfx.fit_transform(X)\n",
    "    # Create directory\n",
    "    os.makedirs(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate), exist_ok=True)\n",
    "    # Save\n",
    "    pickle.dump(scale_wfx, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, \"scale_wfx.pkl\"), 'wb'))\n",
    "else:\n",
    "    X_scaled = scale_wfx.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4cb291-ee37-477c-b637-cdc3698dfdbe",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96009bd6-5591-48db-be15-ad970ba242d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "    from tensorflow.keras.losses import KLDivergence\n",
    "\n",
    "    class VotingEnsemble:\n",
    "        def __init__(self, models):\n",
    "            self.models = models\n",
    "\n",
    "        def predict(self, X):\n",
    "            predictions = np.array([model.predict(X, verbose=0) for model in self.models])\n",
    "            return np.mean(predictions, axis=0)\n",
    "\n",
    "    ensemble_size = 5\n",
    "    ensemble_models = []\n",
    "    model_dir = os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(ensemble_size):\n",
    "        model = Sequential([\n",
    "            Dense(128, input_shape=(X_scaled.shape[1],), activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            # Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            # Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      # loss='categorical_crossentropy',\n",
    "                      loss=keras.losses.KLDivergence(),\n",
    "                      metrics=[KLDivergence()])\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_scaled, y,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Create folder\n",
    "        model_path_i = os.path.join(model_dir, f'predict_wfx_{i}.keras')\n",
    "        model.save(model_path_i)\n",
    "        ensemble_models.append(model)\n",
    "\n",
    "    # Wrap ensemble in predict_wfx for compatibility\n",
    "    predict_wfx = VotingEnsemble(ensemble_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f353999-b9e4-4d7e-a181-eda915931906",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a2f32-db44-4056-91cb-6f36a325a767",
   "metadata": {},
   "source": [
    "Save event averages for use in predictions in A06. Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f298e6d-547c-4b09-85b2-8c61a77a2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_df = pd.DataFrame(sample_df2[events_list].mean()).T\n",
    "# average_df.to_csv(os.path.join(baseball_path, \"Event Averages.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91663ac-cab6-4c20-94ed-b4c4e7aad828",
   "metadata": {},
   "source": [
    "Before predicting, replace with mean predicted event rates (based on batted ball data) to determine how weather would affect an average batted-ball game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1212e6-1502-49f5-a765-36885a4404bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df3 = sample_df2.copy()\n",
    "for event in events_list:\n",
    "    sample_df3[f'{event}_pred_batted'] = sample_df3[event].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44d785-ee00-4611-8e1b-00bae5fe9f00",
   "metadata": {},
   "source": [
    "Now actually predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73aca2-a8e0-4151-874c-508520a7a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X2 = sample_df3[wfx_inputs].values\n",
    "y2 = sample_df3[events_list].values\n",
    "\n",
    "# Scale the features\n",
    "X2_scaled = scale_wfx.transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf8e8e-05f0-43d5-962a-20e5b9ffe8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predict_wfx.predict(X2_scaled)\n",
    "prediction_df2 = pd.DataFrame(predictions2, columns=events_list)\n",
    "\n",
    "prediction_df2 = prediction_df2.add_suffix('_pred_weather')\n",
    "\n",
    "prediction_df2 = pd.concat([prediction_df2, sample_df3.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09aee6d-53ab-425a-bab0-aca15afb52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff36da-0aa5-40c3-a840-cd4948395166",
   "metadata": {},
   "source": [
    "Calculate WFX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e2fb1-cc1d-4cb1-97c7-8e47c380b5b4",
   "metadata": {},
   "source": [
    "Predicted, based on weather, over predicted, based on batted-ball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe2e17-2206-4a3a-a33a-9ef07ed37413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    prediction_df2[f'{event}_wfx_unadj'] = prediction_df2[f'{event}_pred_weather'] / prediction_df2[f'{event}_pred_batted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729539b-8a69-4f15-b842-939eb3e72aac",
   "metadata": {},
   "source": [
    "Highlight outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44687b9-ce3e-4808-9692-ea39c8556629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and get top/bottom 500\n",
    "top_500 = prediction_df2.nlargest(500, 'hr_wfx_unadj')\n",
    "bottom_500 = prediction_df2.nsmallest(500, 'hr_wfx_unadj')\n",
    "\n",
    "# Get value counts\n",
    "top_counts = top_500['venue_id'].value_counts().head(5)\n",
    "bottom_counts = bottom_500['venue_id'].value_counts().head(5)\n",
    "\n",
    "# Combine into a 5x4 DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'Top Venue': top_counts.index,\n",
    "    'Top Count': top_counts.values,\n",
    "    'Bottom Venue': bottom_counts.index,\n",
    "    'Bottom Count': bottom_counts.values\n",
    "})\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9b526-f824-4efe-85ed-873cd3b927aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df2.sort_values('hr_wfx_unadj', ascending=False).head(100)[['meteo_y_vect', 'temperature_2m']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8a297-6145-40cd-8e4d-96e81f90860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df2.sort_values('hr_wfx_unadj', ascending=False).tail(100)[['meteo_y_vect', 'temperature_2m']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eeddf5-6d3b-4cf1-adca-7fc26a5d5b8e",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee20131-0a48-4b9d-9f1e-e5f1b9f5e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the number of rows and columns\n",
    "n_events = len(events_list)\n",
    "n_cols = 3\n",
    "n_rows = (n_events + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "# Set square plots: each subplot is 5x5 inches\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, event in enumerate(events_list):\n",
    "    ax = axes[i]\n",
    "    pred_col = f\"{event}_pred_weather\"\n",
    "    \n",
    "    if pred_col not in prediction_df2.columns:\n",
    "        continue\n",
    "\n",
    "    # Bucket the predicted values into quantiles\n",
    "    prediction_df2['bucket'] = pd.qcut(prediction_df2[pred_col], q=10, duplicates='drop')\n",
    "\n",
    "    # Compute averages\n",
    "    bucket_avg = prediction_df2.groupby('bucket').agg(\n",
    "        avg_pred=(pred_col, 'mean'),\n",
    "        avg_actual=(event, 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(bucket_avg['avg_pred'], label='Predicted')\n",
    "    ax.plot(bucket_avg['avg_actual'], label='Actual')\n",
    "    ax.set_title(f\"{event.upper()} Prediction vs Actual\")\n",
    "    ax.set_xlabel(\"Quantile Bucket\")\n",
    "    ax.set_ylabel(\"Rate\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Set y-axis limits: from 1/3 of the max to the max\n",
    "    y_max = max(bucket_avg['avg_pred'].max(), bucket_avg['avg_actual'].max())\n",
    "    y_min = y_max / 2\n",
    "\n",
    "    # Create 10 evenly spaced ticks from y_min to y_max\n",
    "    ticks = np.linspace(y_min, y_max, 10)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_yticks(np.round(ticks, 5))  # round for cleaner labels\n",
    "\n",
    "# Remove extra axes if any\n",
    "for j in range(n_events, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0d642-061e-4462-856c-cca919fbe97d",
   "metadata": {},
   "source": [
    "Drop bucket column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c14cc-e6d9-4044-817d-2d8f8ff451d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df2.drop(columns={'bucket'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53f026-2dd3-4389-af5c-f3790e4ba415",
   "metadata": {},
   "source": [
    "##### Calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be921193-fc50-40f4-940e-d9501911ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 50  # number of last past games to consider (your desired threshold)\n",
    "\n",
    "# Convert pandas DataFrame to Polars DataFrame\n",
    "# IMPORTANT: Ensure prediction_df2 is sorted by your game time column (e.g., 'gamePk', 'date')\n",
    "# BEFORE this cell if it's not already. For example:\n",
    "# prediction_df2 = prediction_df2.sort_values(by='gamePk').copy()\n",
    "df = pl.from_pandas(prediction_df2.copy())\n",
    "\n",
    "# Initialize lists to store results for new columns\n",
    "results_mean = {f'{event}_mean': [] for event in events_list}\n",
    "results_pred_batted_mean = {f'{event}_pred_batted_mean': [] for event in events_list}\n",
    "\n",
    "# Iterate through each row of the Polars DataFrame\n",
    "for i in range(len(df)):\n",
    "    current_row = df.row(i, named=True)  # Get the current row data as a dictionary\n",
    "    past_data = df.slice(0, i) # Get all data from previous rows in the DataFrame (already chronological)\n",
    "\n",
    "    # Process for each event type\n",
    "    for event in events_list:\n",
    "        true_col = event            # Column for '{event}_mean' (your actual event result)\n",
    "        model_col = f'{event}_pred_batted' # Column for '{event}_pred_mean' (your model's event prediction)\n",
    "\n",
    "        # --- Step 1: Attempt strict filtering (venue_id AND batSide) ---\n",
    "        past_subset_strict_filter = past_data.filter(\n",
    "            (pl.col('venue_id') == current_row['venue_id']) &\n",
    "            (pl.col('batSide') == current_row['batSide'])\n",
    "        )\n",
    "\n",
    "        games_to_average = None # Initialize variable to hold the final set of games\n",
    "\n",
    "        # --- Step 2: Check count and apply conditional logic ---\n",
    "        if len(past_subset_strict_filter) < n:\n",
    "            # If strict filter yields less than 'n' games, fallback to broader filter\n",
    "            # print(f\"Row {i}, Event {event}: Less than {n} games with strict filter ({len(past_subset_strict_filter)} found). Falling back to batSide only.\")\n",
    "            past_subset_broad_filter = past_data.filter(\n",
    "                pl.col('batSide') == current_row['batSide']\n",
    "            )\n",
    "            games_to_average = past_subset_broad_filter\n",
    "        else:\n",
    "            # If strict filter yields 'n' or more games, use those\n",
    "            games_to_average = past_subset_strict_filter\n",
    "\n",
    "        # --- Handle cases where even the broadest filter yields no games ---\n",
    "        if games_to_average.is_empty():\n",
    "            results_mean[f'{event}_mean'].append(np.nan)\n",
    "            results_pred_batted_mean[f'{event}_pred_batted_mean'].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # --- Step 3: Take the last 'n' games from the chosen subset and average ---\n",
    "        final_games_for_avg = games_to_average.tail(n)\n",
    "\n",
    "        # Compute the mean of the relevant columns\n",
    "        results_mean[f'{event}_mean'].append(final_games_for_avg[true_col].mean())\n",
    "        results_pred_batted_mean[f'{event}_pred_batted_mean'].append(final_games_for_avg[model_col].mean())\n",
    "\n",
    "# Combine all collected results into a single dictionary\n",
    "final_results_combined = {}\n",
    "final_results_combined.update(results_mean)\n",
    "final_results_combined.update(results_pred_batted_mean)\n",
    "\n",
    "# Add the newly computed columns to the Polars DataFrame\n",
    "for col_name, values_list in final_results_combined.items():\n",
    "    df = df.with_columns(pl.Series(name=col_name, values=values_list))\n",
    "\n",
    "# Convert the final Polars DataFrame back to a pandas DataFrame\n",
    "prediction_df2 = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045a4b2-e24e-4fc9-9bc7-5dfc69a0bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017545c0-4a56-4a3d-890a-c26d532e25b5",
   "metadata": {},
   "source": [
    "##### Calculate WFX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9059b7e8-998d-4556-8661-ef0ed9ebb33a",
   "metadata": {},
   "source": [
    "WFX = Actual for similar games / Predicted (using batted-ball data) in similar games "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb5af3-6a61-4442-ae8c-ea2a3a121df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    prediction_df2[f'{event}_wfx_adj'] = prediction_df2[f'{event}_mean'] / prediction_df2[f'{event}_pred_batted_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58341b17-0d36-4c46-a6b9-98500e23ce0a",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4eba4f-3546-4c21-b2f7-252314b9268a",
   "metadata": {},
   "source": [
    "What questions are you trying to answer?\n",
    "    - Do multipliers predict hr rates?\n",
    "    - Are multipliers \"fair\" across venue and hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964a321-ed28-48fb-bb58-66e8839f82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df2['residual'] = prediction_df2['hr'] - prediction_df2['hr_wfx_adj']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='venue_id', y='residual', hue='batSide', data=prediction_df2)\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Residuals by Venue and Bat Side\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8af0d8-3f24-42a5-b976-23c7bb0c46e1",
   "metadata": {},
   "source": [
    "##### WFX Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee33d3-c46d-466f-9a39-4bbef7dd60ba",
   "metadata": {},
   "source": [
    "Convert from long to wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e7be0-ce55-43e9-b1e0-9fbd5f6e054d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_shifted_game_wfx_df = prediction_df2[prediction_df2['batSide'] == \"L\"]\n",
    "r_shifted_game_wfx_df = prediction_df2[prediction_df2['batSide'] == \"R\"]\n",
    "\n",
    "wfx_df = pd.merge(l_shifted_game_wfx_df, r_shifted_game_wfx_df, on=['venue_id', 'gamePk', 'date'], how='left', suffixes=(\"_l\", \"_r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e158180-3aa2-4a8a-9f92-85125e3bcfd9",
   "metadata": {},
   "source": [
    "Write all game WFX to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5df586-df91-454b-b889-f4d710f2fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_df[['venue_id', 'gamePk', 'date'] + [col for col in wfx_df if \"wfx\" in col] + [col for col in wfx_df if \"pred\" in col] + [f'{event}_l' for event in events_list] + [f'{event}_r' for event in events_list]].to_csv(os.path.join(baseball_path, \"Park and Weather Factors.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac24e9-c18b-4b08-bf13-32fa702db62f",
   "metadata": {},
   "source": [
    "Write individual-game WFX to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358df03-a62d-40b4-a318-ce76d8198fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in wfx_df['date'].unique():\n",
    "    wfx_df[wfx_df['date'] == date][['venue_id', 'gamePk', 'date'] + [col for col in wfx_df if \"wfx\" in col]].to_csv(os.path.join(baseball_path, \"A06. Weather\", \"3. Park and Weather Factors\", f\"Park and Weather Factors {date}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ed3ac-c7c6-4fae-9f9b-1674f5a258d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "600ebb94-4f33-49d3-ac2e-2573b3c6e8e2",
   "metadata": {},
   "source": [
    "##### Player Stat DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06416f-d27c-4684-b6d7-8f4a52450816",
   "metadata": {},
   "source": [
    "This comes from Model #1 and is completely independent of Model #2. Placement is for convenience, not necessarily logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5df6c-4538-4ba3-b621-54a269236380",
   "metadata": {},
   "source": [
    "Replace actual event rates with predicted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef4af8-d2de-495c-853a-230cd3074336",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[events_list] = prediction_df[events_list_pred_batted].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1c5cd-a1b5-44f2-9374-ff4e109f2628",
   "metadata": {},
   "source": [
    "Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f9f9f-56f3-47fe-9daa-03c0fa4bc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bucket' in list(prediction_df.columns):\n",
    "    prediction_df.drop(columns=['bucket'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5a30d-eb2c-4a8b-899d-96d2f1044981",
   "metadata": {},
   "source": [
    "Calculate rolling stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b262d81-299d-43d1-93d0-10530eb1f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 50, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191f6a0-a881-44e4-963c-f5ec2704fbfb",
   "metadata": {},
   "source": [
    "Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c54d4-9a01-4b5d-abc8-6511adc48eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_short = rolling_pas(prediction_df, short, events_list)\n",
    "print(f\"Short took {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677b965-362f-4f69-b107-ba580f2970cf",
   "metadata": {},
   "source": [
    "Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ebd3d-1c04-498d-92e6-e2b2ca4df51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_long = rolling_pas(prediction_df, long, events_list)\n",
    "df_long = df_long.add_suffix(\"_long\")\n",
    "print(f\"Long took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# We only need the rolling stats from long (the rest are in df_short)\n",
    "long_stats = batter_stats_long + pitcher_stats_long\n",
    "df_long = df_long[long_stats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6643cdc-7de6-4065-802f-56586f6ac3b9",
   "metadata": {},
   "source": [
    "Merge long stats onto rolling (and other) stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d3999-4224-48a7-b9d7-60dd8c7aeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([df_short, df_long], axis=1)\n",
    "final_dataset.reset_index(drop=True, inplace=True)\n",
    "final_dataset.sort_values(['date', 'gamePk', 'atBatIndex'], ascending=True, inplace=True)\n",
    "final_dataset.drop(columns=events_list + ['Cut'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b3408-e180-42c5-8a1c-015260476a6f",
   "metadata": {},
   "source": [
    "Add event dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8144d0e-45d9-424a-9dac-e16b77199ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dummies = pd.get_dummies(final_dataset['eventsModel']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd2fae-3989-4076-9419-680e56537aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([final_dataset, event_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d84e2e-886c-48fe-b5b5-98ff468a93e1",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a62456-5e08-4122-a601-c73c38099a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.replace([float('inf'), float('-inf')], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262ddcc-6966-4f0d-b3f4-6d867754030c",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769aa93-0cf0-4482-b7f7-b45136e99283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_dataset.to_csv(os.path.join(baseball_path, \"Final Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6652cf6-8cfb-4644-84ed-ca1dcb1735e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e652a-78ec-487a-b00d-fdfa9846dfcb",
   "metadata": {},
   "source": [
    "### Required Follow-Ups:\n",
    "- Model #1. Expected Outcomes\n",
    "    - Model #2. Expected Outcomes\n",
    "    - B01. Matchups\n",
    "    - M02. Stat Imputations\n",
    "    - M03. Plate Appearances\n",
    "- Model #2. Park and Weather Factors\n",
    "    - M03. Plate Appearances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d539fdb-626a-4458-81d6-2b3492df2876",
   "metadata": {},
   "source": [
    "Note: You should avoid rerunning Model #1. Expected Outcomes as much as possible. Likely shouldn't need many updates anyway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
