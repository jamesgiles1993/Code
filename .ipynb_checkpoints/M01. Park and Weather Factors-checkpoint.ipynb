{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# M01. Park and Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running imports...\n",
      "Imports in.\n"
     ]
    }
   ],
   "source": [
    "if \"running_pipeline\" not in globals():\n",
    "    print(\"Running imports...\")\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    print(\"Imports in.\")\n",
    "else:\n",
    "    print(\"Imports already in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce0f6-2eb6-4ecd-841b-2e99fc13c281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02405859-9235-419d-bb06-52eb83ede249",
   "metadata": {},
   "source": [
    "### Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208611b0-2531-45b0-b7e7-ae2db5db9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_dataset = create_pa_inputs(None, start_year=2013, end_year=2024, short=50, long=300, adjust=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b42d3-21c5-4cda-92ea-748580d3884b",
   "metadata": {},
   "source": [
    "### Base Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13121744-f9d8-4d98-8797-267f410948e6",
   "metadata": {},
   "source": [
    "Calculate average stats in a given base year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a8511a-581b-47b4-8345-36b219d3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rates(df, base_year=2014):\n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Select period of interest\n",
    "    df = df[df['game_date'].dt.year == base_year]\n",
    "\n",
    "    # Calculate averages over period of interest\n",
    "    base_rate_df = pd.DataFrame(df[events_list].mean()).T\n",
    "\n",
    "    \n",
    "    return base_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a930b6-360b-42d0-99c2-8000eae19653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b183ab-f43b-4c9c-a209-b2ee24c3f190",
   "metadata": {},
   "source": [
    "### Game Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed47d7-2bce-4d17-ac4e-eb608eda8884",
   "metadata": {},
   "source": [
    "Average rates within the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4daf7da9-72c3-4c3c-985e-c57db1a405d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(df):    \n",
    "    # Calculate averages by game\n",
    "    game_avgs = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'])[events_list].mean().reset_index()\n",
    "\n",
    "    # Add the 'pas' column to count the number of observations in each group\n",
    "    game_avgs['pas'] = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']).size().values\n",
    "\n",
    "    # Sort by date\n",
    "    game_avgs.sort_values(['game_date'], ascending=True, inplace=True)\n",
    "    \n",
    "    return game_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eabeca8c-8017-45b2-8942-5d90250eea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_average_df = game_averages(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986d09a-63b8-4a36-aaa4-198d8132474f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66a92c3e-8bbf-4687-9079-68d7346bede0",
   "metadata": {},
   "source": [
    "### Player Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82edc50-a380-4caf-87ab-650d932a1e4a",
   "metadata": {},
   "source": [
    "Average stats of all the players in the game, coming into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6dc16b3-2c4b-4f89-b1e5-32f0356f818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_averages(df):\n",
    "    # Note: these are already shifted if using create_pa_inputs from A02. We want the first PA for players in each game.\n",
    "    # Stats to average\n",
    "    batter_inputs_short = [f\"{event}_b_long\" for event in events_list]\n",
    "    pitcher_inputs_short = [f\"{event}_p_long\" for event in events_list]\n",
    "\n",
    "    # Apply stats from first at bat to entire game\n",
    "    # First at bat has stats through end of last game\n",
    "    # This ensures that no stats generated in-game are reflected\n",
    "    # Note: we're doing this instead of dropping duplicates to properly weight by PA\n",
    "    df[batter_inputs_short] = df.groupby(['gamePk', 'batter'])[batter_inputs_short].transform('first')\n",
    "    df[pitcher_inputs_short] = df.groupby(['gamePk', 'pitcher'])[pitcher_inputs_short].transform('first')\n",
    "    \n",
    "    # Calculate player averages by game\n",
    "    batter_avgs = df.groupby(['gamePk'])[batter_inputs_short].mean().reset_index()\n",
    "    pitcher_avgs = df.groupby(['gamePk'])[pitcher_inputs_short].mean().reset_index()\n",
    "\n",
    "    # Concatenate together\n",
    "    player_avgs = pd.concat([batter_avgs, pitcher_avgs.drop(columns=['gamePk'])], axis=1)\n",
    "    \n",
    "    \n",
    "    return player_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a49f1d09-dc1a-4d72-80a1-2fc6f5c0e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_average_df = player_averages(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c102aa6-f77b-4476-aa85-7902f00e3ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff9a52f-86cd-4886-ac94-23bb9b111af7",
   "metadata": {},
   "source": [
    "### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9617b005-f5ae-4267-bf08-551051f5cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def league_average(complete_dataset, days=30):\n",
    "    # Calculate daily sum of events\n",
    "    league_avg = complete_dataset.groupby('game_date')[events_list].sum().reset_index()\n",
    "    # Calculate total events\n",
    "    league_avg['pas'] = league_avg[events_list].sum(axis=1)\n",
    "    \n",
    "    # Loop over events (and pas)\n",
    "    for event in events_list + ['pas']:\n",
    "        # Create sum column\n",
    "        league_avg[f'{event}_sum'] = None\n",
    "        # Add up daily sums for the last {days} days\n",
    "        for i in range(days, len(league_avg)):\n",
    "            # Calculate the sum of the last {days} values excluding the current row\n",
    "            league_avg.loc[i, f'{event}_sum'] = league_avg[f'{event}'].iloc[i-days:i].sum()\n",
    "\n",
    "    # Calculate average\n",
    "    for event in events_list:\n",
    "        league_avg[f'{event}_lg'] = league_avg[f'{event}_sum'] / league_avg['pas_sum']\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    return league_avg[[\"game_date\"] + [col for col in league_avg if \"_lg\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8dbf862-3c02-4922-9b6f-0b22d6faf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# league_average_df = league_average(complete_dataset, 30)\n",
    "# league_average_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16902835-6773-4224-b6da-f0a54c913e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5fb155b-c601-4d64-a88e-c75dce2b7c7f",
   "metadata": {},
   "source": [
    "### Park Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947da453-0d62-4929-b686-70e4a2e6a5d6",
   "metadata": {},
   "source": [
    "Helper function used to calculate weighted rolling averages, used to average game stats by PAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "428848b8-f65e-459c-b09c-3e8f1e16af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rolling_avg(values, weights, window, min_periods):\n",
    "    # Compute the weighted rolling average using a sliding window\n",
    "    result = (\n",
    "        pd.Series(values)\n",
    "        .rolling(window=window, min_periods=min_periods)\n",
    "        .apply(lambda x: np.sum(x * weights[-len(x):]) / np.sum(weights[-len(x):]), raw=True)\n",
    "    )\n",
    "    return result.shift(1)  # Shift by 1 to exclude the current row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8674102-da42-42e1-9610-d8cf2c36cb21",
   "metadata": {},
   "source": [
    "##### Park Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344bd9-99ba-4b5e-82bd-ff0a4f568eb1",
   "metadata": {},
   "source": [
    "Average of stats over last park_window games - excluding game of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e31d871-22da-4911-a98b-c3ec81d8d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_averages(game_avgs, park_window, park_window_min):\n",
    "    # Sort by venue and date\n",
    "    park_avgs = game_avgs.sort_values(['game_date'], ascending=[True])\n",
    "    \n",
    "    # Calculate rolling averages by park\n",
    "    park_avgs[events_list] = park_avgs.groupby('venue_id').apply(\n",
    "    lambda group: pd.DataFrame({event: weighted_rolling_avg(group[event], group['pas'], park_window, park_window_min) for event in events_list})).reset_index(level=0, drop=True)\n",
    "    \n",
    "    \n",
    "    return park_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3941156-6fff-405a-ae97-fdb0d6605d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_average_df = park_averages(game_average_df, 243, 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66fd3b-2e2b-4113-abd4-b6a07962caa8",
   "metadata": {},
   "source": [
    "##### Team Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b74003-2b91-4d48-816c-3cc638496986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_averages(game_avgs, park_window, park_window_min):\n",
    "    # Sort by venue and date\n",
    "    team_avgs = game_avgs.sort_values(['game_date'], ascending=[True])\n",
    "    \n",
    "    # Calculate rolling averages by park\n",
    "    team_avgs[events_list] = team_avgs.groupby('away_name').apply(\n",
    "    lambda group: pd.DataFrame({event: weighted_rolling_avg(group[event], group['pas'], park_window, park_window_min) for event in events_list})).reset_index(level=0, drop=True)\n",
    "    \n",
    "    return team_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0702842-daa3-4530-b836-6559e7f1f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_average_df = team_averages(game_average_df, 243, 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aa6c4-b995-464b-9450-0d7581c12282",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71d24c5e-2448-403b-b822-a8a282cf9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_park_factors(park_avgs, team_avgs):\n",
    "    # Sort by game_date\n",
    "    park_avgs = park_avgs.sort_values('game_date')\n",
    "    team_avgs = team_avgs.sort_values('game_date')\n",
    "\n",
    "    # Create uniform team_name variable equal to name of interest\n",
    "    park_avgs['team_name'] = park_avgs['home_name'].copy()\n",
    "    team_avgs['team_name'] = team_avgs['away_name'].copy()\n",
    "\n",
    "    # Set to datetime\n",
    "    park_avgs['game_date'] = pd.to_datetime(park_avgs['game_date'])\n",
    "    team_avgs['game_date'] = pd.to_datetime(team_avgs['game_date'])\n",
    "    \n",
    "    # Perform merge_asof\n",
    "    park_factor_df = pd.merge_asof(park_avgs, team_avgs, left_on='game_date', right_on='game_date', by='team_name', direction='backward', suffixes=('_park', '_team'))\n",
    "\n",
    "    # Calculate park factors\n",
    "    for stat in events_list:\n",
    "        park_factor_df[f'{stat}_pfx'] = park_factor_df[f'{stat}_park'] / park_factor_df[f'{stat}_team'] \n",
    "        \n",
    "    park_factor_df.rename(columns={'gamePk_park': 'gamePk'}, inplace=True)\n",
    "    keep_columns = ['gamePk'] + [col for col in park_factor_df.columns if col.endswith('pfx')]\n",
    "    \n",
    "    return park_factor_df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c26a8cfd-1241-481f-8a33-7788417e22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df = create_park_factors(park_average_df, team_average_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf439c-b77e-4230-aee2-8a4397b7b229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac0f210-8aa6-4479-8c1b-f6cc194edac0",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd52ba2-27f4-4260-97ad-a2e7a2892d29",
   "metadata": {},
   "source": [
    "Merge together game averages, player averages, and park factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba144343-39bf-499c-b1d9-2a6a68bf46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_df(complete_dataset, league_average_df, park_factor_df):\n",
    "    # Merge on league averages\n",
    "    analysis_df = pd.merge(complete_dataset, league_average_df, on=['game_date'], how='inner')\n",
    "    # Merge on park factors\n",
    "    analysis_df = pd.merge(analysis_df, park_factor_df, on='gamePk', how='inner')\n",
    "   \n",
    "    \n",
    "    # Extract dummies from venues\n",
    "    venue_dummy_df = pd.get_dummies(analysis_df['venue_id'], prefix='venue')\n",
    "    # Extract dummy column names\n",
    "    venue_dummies = list(venue_dummy_df.columns)\n",
    "    \n",
    "    # Add in dummies\n",
    "    analysis_df = pd.concat([analysis_df, venue_dummy_df], axis=1)\n",
    "    \n",
    "    # Select variables to keep\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    # Select relevant variables and drop missings\n",
    "    analysis_df = analysis_df[[\"eventsModel\", 'gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + variables + [col for col in analysis_df if col.endswith(\"_lg\")]].dropna()\n",
    "    \n",
    "    # Remove cut\n",
    "    analysis_df = analysis_df[analysis_df['eventsModel'] != \"Cut\"]\n",
    "    \n",
    "    \n",
    "    return analysis_df, venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f24e6f67-06ac-43c8-9436-0175a0a197ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_df, venue_dummies = create_analysis_df(game_average_df, player_average_df, park_factor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7fdbb-d18c-47e7-a955-d7343aabe217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48eade1f-3053-4c35-a636-5354a1e434df",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0a386-0116-4447-ab3f-115dbc57b38c",
   "metadata": {},
   "source": [
    "$\\hat{event}$ = event_b_long + event_p_long + event_pfx + x_vect + y_vect + temperature + venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54645574-4eaa-408a-81dc-4411e6b1890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(analysis_df, venue_dummies, batSide, layers):\n",
    "    # Identify inputs\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    # Prepare\n",
    "    X = analysis_df[variables].values  # Independent variables\n",
    "    y = analysis_df['eventsModel'].values  # Dependent variable\n",
    "\n",
    "    # Define three neural network models with slightly different configurations\n",
    "    nn_model_1 = MLPClassifier(hidden_layer_sizes=layers,activation='relu',solver='adam',max_iter=10,random_state=1)\n",
    "    nn_model_2 = MLPClassifier(hidden_layer_sizes=layers,activation='relu',solver='adam',max_iter=10,random_state=2)\n",
    "    nn_model_3 = MLPClassifier(hidden_layer_sizes=layers,activation='relu',solver='adam',max_iter=10,random_state=3)\n",
    "\n",
    "    # Create a Voting Classifier with the three models\n",
    "    voting_model = VotingClassifier([('nn1', nn_model_1),('nn2', nn_model_2),('nn3', nn_model_3)], voting='soft')\n",
    "\n",
    "    # Train the Voting Regressor\n",
    "    voting_model.fit(X, y)\n",
    "\n",
    "    # Create directory for saving the model\n",
    "    os.makedirs(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate), exist_ok=True)\n",
    "\n",
    "    # Save the Voting Classifier\n",
    "    with open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, f\"predict_wfx_{batSide.lower()}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(voting_model, f)\n",
    "\n",
    "    print(f\"Voting model for {batSide}HB saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec61ea85-c5aa-452f-a66b-affb10eb57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_models(analysis_df, venue_dummies, \"L\", (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea2b95-d991-4585-a228-05565c31324c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb6dc96-4509-4489-af81-044f59255702",
   "metadata": {},
   "source": [
    "### Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d4818e2-732d-4ba0-a0e7-5d91b220c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(df, base_rate_df, model_date, batSide):\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    \n",
    "    # Make predictions\n",
    "    # Path to the saved model for\n",
    "    saved_model_path = os.path.join(model_path, \"M01. Park and Weather Factors\", model_date, f\"predict_wfx_{batSide.lower()}.pkl\")\n",
    "\n",
    "    # Load the model\n",
    "    with open(saved_model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Create input dataframe\n",
    "    X = df.copy()\n",
    "\n",
    "    for event in events_list:\n",
    "        # Use league averages to predict (NOT BASE RATES) \n",
    "        X[f'{event}_b_long'] = X[f'{event}_lg'].astype(float).copy()\n",
    "        X[f'{event}_p_long'] = X[f'{event}_lg'].astype(float).copy()\n",
    "\n",
    "        \n",
    "    # Identify inputs\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "\n",
    "    # Extract the feature data\n",
    "    X = X[variables]\n",
    "    \n",
    "    # Predict using the loaded model\n",
    "    class_list = list(model.classes_)\n",
    "    prediction_columns = [f\"{event}_pred\" for event in class_list]\n",
    "    prediction_df = pd.DataFrame(model.predict_proba(X), columns=prediction_columns)\n",
    "    \n",
    "    # Append \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, prediction_df], axis=1)\n",
    "\n",
    "    # Calculate wfx\n",
    "    for event in events_list:\n",
    "        # Compare to base year (NOT LEAGUE AVERAGE)\n",
    "        df[f'{event}_wfx'] = df[f'{event}_pred'] / base_rate_df[event][0]\n",
    "    \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce64b02-0685-4239-a3a8-a5685d09eda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab1d5cc3-2fb9-4e6d-b4bd-bf0d59ab1335",
   "metadata": {},
   "source": [
    "### Multiplier Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd0976c3-eb4d-42c0-8d1b-f959879c9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "model_date = \"20241204\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064024c-dd4c-4f9b-bd43-ec70f2db8f0c",
   "metadata": {},
   "source": [
    "##### Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6230d-e586-41a9-bb83-8518be94d322",
   "metadata": {},
   "source": [
    "Note: You only have to prepare once even if you retrain the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685842f-2890-4ded-a58e-238de1ff535a",
   "metadata": {},
   "source": [
    "Read in complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "201fe6d4-0455-4ff8-842b-4a017fc12ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 18s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "complete_dataset = create_pa_inputs(None, 2013, 2024, short=50, long=300, adjust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "248c58b5-d2a5-49c3-b158-5d4d5b88e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset['temperature'] = complete_dataset.apply(lambda row: 70 if 'Roof' in row['weather'] or 'Dome' in row['weather'] else row['temperature'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f0dfa-636c-4859-be79-2b0fb5c4450b",
   "metadata": {},
   "source": [
    "Generate or read base rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab4d4fe7-d269-4232-adee-a744201c2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate base rates (base year = 2014)\n",
    "# Only needs to be run once\n",
    "# Generate:\n",
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)\n",
    "\n",
    "# Read: \n",
    "base_rate_df = pd.read_csv(os.path.join(baseball_path, \"Base Rates.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1871e553-13cc-4154-ac4a-3286123271b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "# List of dataframes\n",
    "analysis_df_list = []\n",
    "# Loop over batter sides\n",
    "for batSide in ['L', 'R']:\n",
    "    print(batSide)\n",
    "    # Subset complete dataset\n",
    "    complete_dataset_side = complete_dataset[complete_dataset['batSide'] == batSide]\n",
    "    # Calculate game averages (average rates within a particular games)\n",
    "    game_average_df = game_averages(complete_dataset_side)\n",
    "    # # Calculate player averages (average rates of all players coming into the game)\n",
    "    # player_average_df = player_averages(complete_dataset_side)\n",
    "    # Calculate league averages (average rates of all PAs over last n days coming into the day)\n",
    "    league_average_df = league_average(complete_dataset_side, days=30)\n",
    "    # Average rates at park over last n games (both teams)\n",
    "    park_average_df = park_averages(game_average_df, 243, 81)\n",
    "    # Average rates at away games over last n games (both teams)\n",
    "    team_average_df = team_averages(game_average_df, 243, 81)\n",
    "    # Park factors\n",
    "    park_factor_df = create_park_factors(park_average_df, team_average_df)\n",
    "    # Create dataframe that can be used to train and analyze data\n",
    "    analysis_df, venue_dummies = create_analysis_df(complete_dataset, league_average_df, park_factor_df)\n",
    "    analysis_df_list.append(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb30ccb-56a6-4485-a9cb-4febf7e966ae",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98feac7a-d065-49fe-88f9-5f5ff295b359",
   "metadata": {},
   "source": [
    "Rerun this when you want to retrain models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9d74153-bbbf-4a6c-b6f1-805a19fa3f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 6s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wfx_df_list = []\n",
    "for batSide in ['L', 'R']:\n",
    "    if batSide == 'L':\n",
    "        analysis_df = analysis_df_list[0].copy()\n",
    "    else:\n",
    "        analysis_df = analysis_df_list[1].copy()\n",
    "    \n",
    "    # Drop missings\n",
    "    analysis_df = analysis_df.dropna()\n",
    "    \n",
    "    # Train models\n",
    "    if train == True:\n",
    "        train_models(analysis_df, venue_dummies, batSide, layers=(38,38,38,38,38))\n",
    "        \n",
    "    # Create dataset with wfx\n",
    "    wfx_df = run_predictions(analysis_df, base_rate_df, model_date, batSide)\n",
    "    wfx_df_list.append(wfx_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fc486-cbe9-4f7b-8555-97fb45b8b9bb",
   "metadata": {},
   "source": [
    "Separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6284240f-872c-4c39-8002-e803713b69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhb_df = wfx_df_list[0].copy()\n",
    "rhb_df = wfx_df_list[1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccc099-80a9-4644-94d1-1ac52f123842",
   "metadata": {},
   "source": [
    "Scale predictions:\n",
    "- Numerator: Predicted rate\n",
    "- Denominator: Sum of all event predicted rates (should be close to one, but won't be exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb262ac4-48b3-4f4d-bd94-b36aa24fc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predictions\n",
    "pred_list = ['b1_pred', 'b2_pred', 'b3_pred', 'hr_pred', 'bb_pred', 'hbp_pred', 'so_pred', 'fo_pred', 'go_pred', 'lo_pred', 'po_pred']\n",
    "\n",
    "# Sum of prediction odds\n",
    "lhb_df['pred_sum'] = lhb_df[pred_list].sum(axis=1)\n",
    "rhb_df['pred_sum'] = rhb_df[pred_list].sum(axis=1)\n",
    "\n",
    "# Scaled\n",
    "for event in pred_list:\n",
    "    lhb_df[event] = lhb_df[event] / lhb_df['pred_sum']\n",
    "    rhb_df[event] = rhb_df[event] / rhb_df['pred_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae24fa-a3e6-498f-8158-fca4a75c9e8e",
   "metadata": {},
   "source": [
    "Columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd5640ce-d9c7-4297-b999-441dc32599aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']\n",
    "pfx_list = [col for col in wfx_df_list[0].columns if col.endswith('pfx')]\n",
    "wfx_list = [col for col in wfx_df_list[0].columns if col.endswith('wfx')]\n",
    "pred_list = [col for col in wfx_df_list[0].columns if col.endswith('_pred')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0e3cc2c-4936-4fac-aa42-29b549694d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dummies = pd.get_dummies(lhb_df['eventsModel']).astype(int)\n",
    "lhb_df2 = pd.concat([lhb_df, event_dummies], axis=1)\n",
    "lhb_df2 = lhb_df2.groupby(keep_list)[events_list + pfx_list + wfx_list + pred_list].mean(numeric_only=True).reset_index()\n",
    "\n",
    "event_dummies = pd.get_dummies(rhb_df['eventsModel']).astype(int)\n",
    "rhb_df2 = pd.concat([rhb_df, event_dummies], axis=1)\n",
    "rhb_df2 = rhb_df2.groupby(keep_list)[events_list + pfx_list + wfx_list + pred_list].mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a630e3e-d4f6-4c6c-bfb6-e1566135e0b6",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "27d8fcdc-2452-4b26-894e-56a6919dc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df = pd.merge(lhb_df2, rhb_df2, on=keep_list, how='inner', suffixes=('_l', '_r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae2e9f-eb9e-4667-a15e-ed33e825daf9",
   "metadata": {},
   "source": [
    "Read in game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84d3f68a-3793-4078-9366-00a8efc725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.3 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "game_df = create_games(\"20220101\", todaysdate, team_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c70e183e-1cd3-4712-aabb-a44ab9101784",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['date'] = game_df['date'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c712c8f-7478-4906-99d4-2f74af978ba1",
   "metadata": {},
   "source": [
    "Add date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c835b-1e3f-42e7-95b0-32257d43b791",
   "metadata": {},
   "source": [
    "Note: game_date currently in multiplier_df will have original date in cases of postponements. date in game_df will have the correct date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1590dfa-eb75-4ca6-b3bb-35a15ff0967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df = multiplier_df.merge(game_df[['game_id', 'date']], left_on='gamePk', right_on=['game_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274f582-d5cd-4a02-ba3f-5fdd53ea4169",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a36869b1-172c-4dfa-94c1-64cbdb0e9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df.sort_values(['date', 'gamePk'], ascending=[True, True]).to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a3489-5e5f-4b07-b847-4a12ee398261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7768c7ee-bde2-4a6e-ba41-20ac0cb8f23c",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f3178-069e-48c3-80d9-d3073aaf9de9",
   "metadata": {},
   "source": [
    "##### Rates by Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4377c-d362-46b5-907a-45ea1caaf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))  # 3 rows, 2 columns\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the events and their corresponding axes\n",
    "for idx, event in enumerate(events_list):\n",
    "    ax = axes[idx]  # Select the appropriate subplot\n",
    "    \n",
    "    # Step 1: Create quantile buckets for the current event\n",
    "    lhb_df2['quantile'] = pd.qcut(lhb_df2[f'{event}_pred'], q=10, labels=False)  # 10 quantiles (adjust q as needed)\n",
    "    \n",
    "    # Step 2: Group by quantiles and calculate the mean\n",
    "    quantile_means = lhb_df2.groupby('quantile').agg({f'{event}_pred': 'mean', event: 'mean'}).reset_index()\n",
    "    \n",
    "    # Step 3: Plot the predictions and actuals\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[f'{event}_pred'], label=f'Average {event}_pred', marker='o')\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[event], label=f'Average {event}', marker='x')\n",
    "    \n",
    "    # Add subplot details\n",
    "    ax.set_title(f'{event} Predictions vs Actuals')\n",
    "    ax.set_xlabel('Quantile')\n",
    "    ax.set_ylabel('Average Value')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4fc73-83bb-4565-89fc-ebb25c34da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))  # 3 rows, 2 columns\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the events and their corresponding axes\n",
    "for idx, event in enumerate(events_list):\n",
    "    ax = axes[idx]  # Select the appropriate subplot\n",
    "    \n",
    "    # Step 1: Create quantile buckets for the current event\n",
    "    rhb_df2['quantile'] = pd.qcut(rhb_df2[f'{event}_pred'], q=10, labels=False)  # 10 quantiles (adjust q as needed)\n",
    "    \n",
    "    # Step 2: Group by quantiles and calculate the mean\n",
    "    quantile_means = rhb_df2.groupby('quantile').agg({f'{event}_pred': 'mean', event: 'mean'}).reset_index()\n",
    "    \n",
    "    # Step 3: Plot the predictions and actuals\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[f'{event}_pred'], label=f'Average {event}_pred', marker='o')\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[event], label=f'Average {event}', marker='x')\n",
    "    \n",
    "    # Add subplot details\n",
    "    ax.set_title(f'{event} Predictions vs Actuals')\n",
    "    ax.set_xlabel('Quantile')\n",
    "    ax.set_ylabel('Average Value')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b7a6f-68c9-4be1-8788-2e712abb4501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9677be-a9c6-4baa-8fca-5a1bb7ca3604",
   "metadata": {},
   "source": [
    "##### Yearly Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0df0e-4177-4145-ad18-549c095b421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhb_df2['year'] = lhb_df2['game_date'].str[:4]\n",
    "event = 'hr'\n",
    "lhb_df2.groupby('year')[[event, f'{event}_pred',  f'{event}_pfx', f'{event}_wfx']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480728a-c4ef-4512-912f-f591b47b81df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ba8413-fce2-4ec9-9f9e-a2cbcdbeaba7",
   "metadata": {},
   "source": [
    "##### Park Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f0974-bdb9-4fed-a17a-243c5a8a0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhb_df2['safe'] = rhb_df2[['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']].sum(axis=1)\n",
    "rhb_df2['out'] = rhb_df2[['so', 'go', 'lo', 'po', 'fo']].sum(axis=1)\n",
    "rhb_df2['safe_pred'] = rhb_df2[['b1_pred', 'b2_pred', 'b3_pred', 'hr_pred', 'bb_pred', 'hbp_pred']].sum(axis=1)\n",
    "rhb_df2['out_pred'] = rhb_df2[['so_pred', 'go_pred', 'lo_pred', 'po_pred', 'fo_pred']].sum(axis=1)\n",
    "\n",
    "lhb_df2['safe'] = lhb_df2[['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']].sum(axis=1)\n",
    "lhb_df2['out'] = lhb_df2[['so', 'go', 'lo', 'po', 'fo']].sum(axis=1)\n",
    "lhb_df2['safe_pred'] = lhb_df2[['b1_pred', 'b2_pred', 'b3_pred', 'hr_pred', 'bb_pred', 'hbp_pred']].sum(axis=1)\n",
    "lhb_df2['out_pred'] = lhb_df2[['so_pred', 'go_pred', 'lo_pred', 'po_pred', 'fo_pred']].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db0ccd-5ce7-48f7-8487-b5a61dfca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_error = rhb_df2[rhb_df2['venue_id'].astype('int').isin(team_map['VENUE_ID'])].groupby('venue_id')[['safe', 'safe_pred']].mean()\n",
    "park_error['diff'] = park_error['safe'] - park_error['safe_pred']\n",
    "park_error.sort_values('diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338c5e0-a6e8-41cc-a99a-b029c8cf3475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3404f8d9-448c-4b36-b294-ac336399b0ab",
   "metadata": {},
   "source": [
    "##### Park and Park x Weather Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e2dfe-f97e-435a-8aad-b3a05216dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhb_df2.drop_duplicates('venue_id', keep='last')[['venue_id'] + [col for col in rhb_df2.columns if col.endswith(\"_pfx\")] + [col for col in rhb_df2.columns if col.endswith(\"_wfx\")]].sort_values('venue_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034e2c3-d64c-4111-b64c-0aaf1e0a1b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511398-2b77-4594-9155-2c5aaf04465d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2a6a58-f18b-48da-8b63-7da94a3ee58f",
   "metadata": {},
   "source": [
    "### Generate Park and Weather Factors files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9ddb0af-72f3-43f8-9ae0-093e9fd902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b4175ca8-9421-4ca8-94b9-fb45bcbac3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220407\n",
      "20220408\n",
      "20220409\n",
      "20220410\n",
      "20220411\n",
      "20220412\n",
      "20220413\n",
      "20220414\n",
      "20220415\n",
      "20220416\n",
      "20220417\n",
      "20220418\n",
      "20220419\n",
      "20220420\n",
      "20220421\n",
      "20220422\n",
      "20220423\n",
      "20220424\n",
      "20220425\n",
      "20220426\n",
      "20220427\n",
      "20220428\n",
      "20220429\n",
      "20220430\n",
      "20220501\n",
      "20220502\n",
      "20220503\n",
      "20220504\n",
      "20220505\n",
      "20220506\n",
      "20220507\n",
      "20220508\n",
      "20220509\n",
      "20220510\n",
      "20220511\n",
      "20220512\n",
      "20220513\n",
      "20220514\n",
      "20220515\n",
      "20220516\n",
      "20220517\n",
      "20220518\n",
      "20220519\n",
      "20220520\n",
      "20220521\n",
      "20220522\n",
      "20220523\n",
      "20220524\n",
      "20220525\n",
      "20220526\n",
      "20220527\n",
      "20220528\n",
      "20220529\n",
      "20220530\n",
      "20220531\n",
      "20220601\n",
      "20220602\n",
      "20220603\n",
      "20220604\n",
      "20220605\n",
      "20220606\n",
      "20220607\n",
      "20220608\n",
      "20220609\n",
      "20220610\n",
      "20220611\n",
      "20220612\n",
      "20220613\n",
      "20220614\n",
      "20220615\n",
      "20220616\n",
      "20220617\n",
      "20220618\n",
      "20220619\n",
      "20220620\n",
      "20220621\n",
      "20220622\n",
      "20220623\n",
      "20220624\n",
      "20220625\n",
      "20220626\n",
      "20220627\n",
      "20220628\n",
      "20220629\n",
      "20220630\n",
      "20220701\n",
      "20220702\n",
      "20220703\n",
      "20220704\n",
      "20220705\n",
      "20220706\n",
      "20220707\n",
      "20220708\n",
      "20220709\n",
      "20220710\n",
      "20220711\n",
      "20220712\n",
      "20220713\n",
      "20220714\n",
      "20220715\n",
      "20220716\n",
      "20220717\n",
      "20220721\n",
      "20220722\n",
      "20220723\n",
      "20220724\n",
      "20220725\n",
      "20220726\n",
      "20220727\n",
      "20220728\n",
      "20220729\n",
      "20220730\n",
      "20220731\n",
      "20220801\n",
      "20220802\n",
      "20220803\n",
      "20220804\n",
      "20220805\n",
      "20220806\n",
      "20220807\n",
      "20220808\n",
      "20220809\n",
      "20220810\n",
      "20220811\n",
      "20220812\n",
      "20220813\n",
      "20220814\n",
      "20220815\n",
      "20220816\n",
      "20220817\n",
      "20220818\n",
      "20220819\n",
      "20220820\n",
      "20220821\n",
      "20220822\n",
      "20220823\n",
      "20220824\n",
      "20220825\n",
      "20220826\n",
      "20220827\n",
      "20220828\n",
      "20220829\n",
      "20220830\n",
      "20220831\n",
      "20220901\n",
      "20220902\n",
      "20220903\n",
      "20220904\n",
      "20220905\n",
      "20220906\n",
      "20220907\n",
      "20220908\n",
      "20220909\n",
      "20220910\n",
      "20220911\n",
      "20220912\n",
      "20220913\n",
      "20220914\n",
      "20220915\n",
      "20220916\n",
      "20220917\n",
      "20220918\n",
      "20220919\n",
      "20220920\n",
      "20220921\n",
      "20220922\n",
      "20220923\n",
      "20220924\n",
      "20220925\n",
      "20220926\n",
      "20220927\n",
      "20220928\n",
      "20220929\n",
      "20220930\n",
      "20221001\n",
      "20221002\n",
      "20221003\n",
      "20221004\n",
      "20221005\n",
      "20230330\n",
      "20230331\n",
      "20230401\n",
      "20230402\n",
      "20230403\n",
      "20230404\n",
      "20230405\n",
      "20230406\n",
      "20230407\n",
      "20230408\n",
      "20230409\n",
      "20230410\n",
      "20230411\n",
      "20230412\n",
      "20230413\n",
      "20230414\n",
      "20230415\n",
      "20230416\n",
      "20230417\n",
      "20230418\n",
      "20230419\n",
      "20230420\n",
      "20230421\n",
      "20230422\n",
      "20230423\n",
      "20230424\n",
      "20230425\n",
      "20230426\n",
      "20230427\n",
      "20230428\n",
      "20230429\n",
      "20230430\n",
      "20230501\n",
      "20230502\n",
      "20230503\n",
      "20230504\n",
      "20230505\n",
      "20230506\n",
      "20230507\n",
      "20230508\n",
      "20230509\n",
      "20230510\n",
      "20230511\n",
      "20230512\n",
      "20230513\n",
      "20230514\n",
      "20230515\n",
      "20230516\n",
      "20230517\n",
      "20230518\n",
      "20230519\n",
      "20230520\n",
      "20230521\n",
      "20230522\n",
      "20230523\n",
      "20230524\n",
      "20230525\n",
      "20230526\n",
      "20230527\n",
      "20230528\n",
      "20230529\n",
      "20230530\n",
      "20230531\n",
      "20230601\n",
      "20230602\n",
      "20230603\n",
      "20230604\n",
      "20230605\n",
      "20230606\n",
      "20230607\n",
      "20230608\n",
      "20230609\n",
      "20230610\n",
      "20230611\n",
      "20230612\n",
      "20230613\n",
      "20230614\n",
      "20230615\n",
      "20230616\n",
      "20230617\n",
      "20230618\n",
      "20230619\n",
      "20230620\n",
      "20230621\n",
      "20230622\n",
      "20230623\n",
      "20230624\n",
      "20230625\n",
      "20230626\n",
      "20230627\n",
      "20230628\n",
      "20230629\n",
      "20230630\n",
      "20230701\n",
      "20230702\n",
      "20230703\n",
      "20230704\n",
      "20230705\n",
      "20230706\n",
      "20230707\n",
      "20230708\n",
      "20230709\n",
      "20230714\n",
      "20230715\n",
      "20230716\n",
      "20230717\n",
      "20230718\n",
      "20230719\n",
      "20230720\n",
      "20230721\n",
      "20230722\n",
      "20230723\n",
      "20230724\n",
      "20230725\n",
      "20230726\n",
      "20230727\n",
      "20230728\n",
      "20230729\n",
      "20230730\n",
      "20230731\n",
      "20230801\n",
      "20230802\n",
      "20230803\n",
      "20230804\n",
      "20230805\n",
      "20230806\n",
      "20230807\n",
      "20230808\n",
      "20230809\n",
      "20230810\n",
      "20230811\n",
      "20230812\n",
      "20230813\n",
      "20230814\n",
      "20230815\n",
      "20230816\n",
      "20230817\n",
      "20230818\n",
      "20230819\n",
      "20230820\n",
      "20230821\n",
      "20230822\n",
      "20230823\n",
      "20230824\n",
      "20230825\n",
      "20230826\n",
      "20230827\n",
      "20230828\n",
      "20230829\n",
      "20230830\n",
      "20230831\n",
      "20230901\n",
      "20230902\n",
      "20230903\n",
      "20230904\n",
      "20230905\n",
      "20230906\n",
      "20230907\n",
      "20230908\n",
      "20230909\n",
      "20230910\n",
      "20230911\n",
      "20230912\n",
      "20230913\n",
      "20230914\n",
      "20230915\n",
      "20230916\n",
      "20230917\n",
      "20230918\n",
      "20230919\n",
      "20230920\n",
      "20230921\n",
      "20230922\n",
      "20230923\n",
      "20230924\n",
      "20230925\n",
      "20230926\n",
      "20230927\n",
      "20230928\n",
      "20230929\n",
      "20230930\n",
      "20231001\n",
      "20231002\n",
      "20240328\n",
      "20240329\n",
      "20240330\n",
      "20240331\n",
      "20240401\n",
      "20240402\n",
      "20240403\n",
      "20240404\n",
      "20240405\n",
      "20240406\n",
      "20240407\n",
      "20240408\n",
      "20240409\n",
      "20240410\n",
      "20240411\n",
      "20240412\n",
      "20240413\n",
      "20240414\n",
      "20240415\n",
      "20240416\n",
      "20240417\n",
      "20240418\n",
      "20240419\n",
      "20240420\n",
      "20240421\n",
      "20240422\n",
      "20240423\n",
      "20240424\n",
      "20240425\n",
      "20240426\n",
      "20240427\n",
      "20240428\n",
      "20240429\n",
      "20240430\n",
      "20240501\n",
      "20240502\n",
      "20240503\n",
      "20240504\n",
      "20240505\n",
      "20240506\n",
      "20240507\n",
      "20240508\n",
      "20240509\n",
      "20240510\n",
      "20240511\n",
      "20240512\n",
      "20240513\n",
      "20240514\n",
      "20240515\n",
      "20240516\n",
      "20240517\n",
      "20240518\n",
      "20240519\n",
      "20240520\n",
      "20240521\n",
      "20240522\n",
      "20240523\n",
      "20240524\n",
      "20240525\n",
      "20240526\n",
      "20240527\n",
      "20240528\n",
      "20240529\n",
      "20240530\n",
      "20240531\n",
      "20240601\n",
      "20240602\n",
      "20240603\n",
      "20240604\n",
      "20240605\n",
      "20240606\n",
      "20240607\n",
      "20240608\n",
      "20240609\n",
      "20240610\n",
      "20240611\n",
      "20240612\n",
      "20240613\n",
      "20240614\n",
      "20240615\n",
      "20240616\n",
      "20240617\n",
      "20240618\n",
      "20240619\n",
      "20240620\n",
      "20240621\n",
      "20240622\n",
      "20240623\n",
      "20240624\n",
      "20240625\n",
      "20240626\n",
      "20240627\n",
      "20240628\n",
      "20240629\n",
      "20240630\n",
      "20240701\n",
      "20240702\n",
      "20240703\n",
      "20240704\n",
      "20240705\n",
      "20240706\n",
      "20240707\n",
      "20240708\n",
      "20240709\n",
      "20240710\n",
      "20240711\n",
      "20240712\n",
      "20240713\n",
      "20240714\n",
      "20240719\n",
      "20240720\n",
      "20240721\n",
      "20240722\n",
      "20240723\n",
      "20240724\n",
      "20240725\n",
      "20240726\n",
      "20240727\n",
      "20240728\n",
      "20240729\n",
      "20240730\n",
      "20240731\n",
      "20240801\n",
      "20240802\n",
      "20240803\n",
      "20240804\n",
      "20240805\n",
      "20240806\n",
      "20240807\n",
      "20240808\n",
      "20240809\n",
      "20240810\n",
      "20240811\n",
      "20240812\n",
      "20240813\n",
      "20240814\n",
      "20240815\n",
      "20240816\n",
      "20240817\n",
      "20240818\n",
      "20240819\n",
      "20240820\n",
      "20240821\n",
      "20240822\n",
      "20240823\n",
      "20240824\n",
      "20240825\n",
      "20240826\n",
      "20240827\n",
      "20240828\n",
      "20240829\n",
      "20240830\n",
      "20240831\n",
      "20240901\n",
      "20240902\n",
      "20240903\n",
      "20240904\n",
      "20240905\n",
      "20240906\n",
      "20240907\n",
      "20240908\n",
      "20240909\n",
      "20240910\n",
      "20240911\n",
      "20240912\n",
      "20240913\n",
      "20240914\n",
      "20240915\n",
      "20240916\n",
      "20240917\n",
      "20240918\n",
      "20240919\n",
      "20240920\n",
      "20240921\n",
      "20240922\n",
      "20240923\n",
      "20240924\n",
      "20240925\n",
      "20240926\n",
      "20240927\n",
      "20240928\n",
      "20240929\n",
      "20240930\n"
     ]
    }
   ],
   "source": [
    "# Select columns to keep\n",
    "keep_columns = ['gamePk', 'game_date', 'date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'] + [col for col in multiplier_df.columns if \"_wfx\" in col]\n",
    "    \n",
    "multiplier_df.sort_values('date', inplace=True)\n",
    "for date in multiplier_df[pd.to_datetime(multiplier_df['game_date']).dt.year >= 2022]['date'].unique():\n",
    "    print(date)\n",
    "    if date > \"20220101\":\n",
    "        # Subset by date\n",
    "        daily_weather_df = multiplier_df[multiplier_df['date'] == date][keep_columns]\n",
    "\n",
    "        # Write to CSV\n",
    "        daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"4. Park and Weather Factors\", f\"{date} Park and Weather Factors.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0627957-32c5-40e1-a759-d9b92f28ff3b",
   "metadata": {},
   "source": [
    "### Note: Rerun A11. Matchups.ipynb if new historic Park x Weather Effects are generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
