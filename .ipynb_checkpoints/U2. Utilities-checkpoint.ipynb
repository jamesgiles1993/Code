{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdd389f-17f6-489f-b015-f012060f521a",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502d9b4-e3f2-485d-9ed5-1ad7b3cba55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean names\n",
    "def remove_accents(old):\n",
    "    new = re.sub(r'[àáâãäå]', 'a', old)\n",
    "    new = re.sub(r'[èéêë]', 'e', new)\n",
    "    new = re.sub(r'[ìíîï]', 'i', new)\n",
    "    new = re.sub(r'[òóôõö]', 'o', new)\n",
    "    new = re.sub(r'[ùúûü]', 'u', new)\n",
    "    new = re.sub(r'[ñ]', 'n', new)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274cdff-0d58-463e-8b23-51d7b96c2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean names for consistency\n",
    "# This is really only used now to clean DK Salaries\n",
    "# You should add players to this if they're not merging on salary information\n",
    "def name_clean(df, name_col):\n",
    "    df['Name'] = df[f'{name_col}']\n",
    "    df['Name'] = np.where(df['Name'] == \"Kike Hernandez\", \"Enrique Hernandez\", df['Name'])\n",
    "    df['Name'] = np.where(df['Name'] == \"Michael A. Taylor\", \"Michael Taylor\", df['Name'])\n",
    "    # Note: to get all the de la Cruz's of the world right as last names, we need to manually add the Ji Mans to be one word so they're the first name\n",
    "    df['Name'] = np.where(df['Name'] == \"Ji Man Choi\", \"Ji-Man Choi\", df['Name'])\n",
    "    df['Name'] = np.where(df['Name'] == \"Ji Hwan Bae\", \"Ji-Hwan Bae\", df['Name']) # he technically has no dash, but we need it so it treats last name properly\n",
    "    df['Name'] = np.where(df['Name'] == \"Hyun Jin Ryu\", \"Hyun-Jin Ryu\", df['Name'])\n",
    "    \n",
    "    \n",
    "    df['Name'] = df.apply(lambda x: remove_accents(x['Name']), axis=1)  # remove accents\n",
    "    df['Name'] = df['Name'].str.replace(r'[^a-zA-Z0-9 ]', '')\n",
    "    df['Name'] = df['Name'].str.replace(\"Jr\", \"\")\n",
    "    df['Name'] = df['Name'].str.replace(\"Sr\", \"\")\n",
    "    df['Name'] = df['Name'].str.replace(\"II\", \"\")\n",
    "    df['Name'] = df['Name'].str.replace(\"III\", \"\")\n",
    "    df['Name'] = df['Name'].str.replace(\".\", \"\")\n",
    "    \n",
    "    \n",
    "    df['Name'] = df['Name'].str.strip()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40716f9-8fc0-4c4a-94de-1c469741eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean FanGraphs ID\n",
    "# Once had individual manual replacements, now could probably be done without\n",
    "def fix_fangraphs(chadwick):      \n",
    "    chadwick['key_fangraphs'] = chadwick['key_fangraphs'].astype('str')\n",
    "    chadwick['key_fangraphs'] = chadwick['key_fangraphs'].str.replace(r'\\.\\d', \"\", regex=True)\n",
    "\n",
    "    return chadwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4472e-77fb-424c-8f0f-4cc196048035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reads in select variables from the Chadwick Register\n",
    "# You should add keys to this if they're not merging with FanGraphs data\n",
    "def read_chadwick(keep_list):\n",
    "    # Separated across these suffixes\n",
    "    chadwick_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "    # Create a list with a dataframe for each suffix\n",
    "    dataframe_list = []\n",
    "    # Loop over suffix\n",
    "    for char in chadwick_list:\n",
    "        # Read in that csv, keeping relevant variables\n",
    "        df = pd.read_csv(\"https://raw.githubusercontent.com/chadwickbureau/register/master/data/people-{}.csv\".format(char), low_memory=False, encoding='utf-8')[keep_list]\n",
    "        # Drop if missing key_mlbam\n",
    "        df.dropna(subset=['key_mlbam'], axis=0, inplace=True)\n",
    "        # Add to dataframe list\n",
    "        dataframe_list.append(df)\n",
    "    # Append all dataframes together\n",
    "    chadwick = pd.concat(dataframe_list, axis=0).reset_index()\n",
    "    \n",
    "    # Edit missing fangraphs IDs (if all else fails)\n",
    "    chadwick = fix_fangraphs(chadwick)\n",
    "    \n",
    "    \n",
    "    chadwick['name_last'].fillna(\"Missing\", inplace=True)\n",
    "    chadwick['name_first'].fillna(\"Mr\", inplace=True)\n",
    "    \n",
    "    chadwick['name_first'] = chadwick['name_first'].str.replace(\" \", \"\")\n",
    "    chadwick['name_first'] = chadwick['name_first'].str.replace(\".\", \"\")\n",
    "    chadwick['name_last'] = chadwick['name_last'].str.replace(\" \", \"\")\n",
    "    chadwick['name_last'] = chadwick['name_last'].str.replace(\".\", \"\")\n",
    "    \n",
    "    # Remove accents\n",
    "    chadwick['name_last'] = chadwick.apply(lambda x: remove_accents(x['name_last']), axis=1)  # remove accents\n",
    "    chadwick['name_first'] = chadwick.apply(lambda x: remove_accents(x['name_first']), axis=1)  # remove accents\n",
    "\n",
    "    # Remove non-alpha numeric characters\n",
    "    chadwick['name_first'] = chadwick['name_first'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", \"\", x))\n",
    "    chadwick['name_last'] = chadwick['name_last'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", \"\", x))\n",
    "    \n",
    "    # Return big dataframe\n",
    "    return chadwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea85038-1fe8-460b-be74-8085ec33b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searches for player IDs\n",
    "def new_ids(player, team, website):\n",
    "    # Google player plus fangraphs\n",
    "    search = player + ' ' + team + ' player page ' + website\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    \n",
    "    # Get info from URL\n",
    "    content = requests.get(url, headers=headers, params=parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.find(id = 'search')\n",
    "    first_link = text.find('a')\n",
    "    \n",
    "    # FanGraph's ID is found a little differently\n",
    "    if website == \"fangraphs\":\n",
    "        website_id = first_link['href'].split(\"/\")[5]\n",
    "\n",
    "    # This should work for RotoWire and MLB.com\n",
    "    else:\n",
    "        website_id = first_link['href'].split(\"-\")[-1]  \n",
    "    \n",
    "    # If it's a minor leaguer code, add quotes\n",
    "    if website_id.startswith(\"sa\"):\n",
    "        website_id = \"'\" + website_id + \"'\"\n",
    "        \n",
    "    \n",
    "    return website_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cefef4-564a-472e-97ee-d6145ec7baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in contest history\n",
    "def contest_history(entry_min=8, date_min=\"20220301\", date_max=\"20991231\"):\n",
    "    history = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"draftkings-contest-entry-history.csv\"))\n",
    "    history = history[history['Sport'] == \"MLB\"]\n",
    "    history = history[history['Contest_Entries'] >= entry_min]\n",
    "    history.drop_duplicates('Contest_Key', inplace=True)\n",
    "    \n",
    "    history['date'] = pd.to_datetime(history['Contest_Date_EST']).dt.strftime('%Y%m%d')\n",
    "    history = history[history['date'] > date_min]\n",
    "    history = history[history['date'] < date_max]\n",
    "\n",
    "    history = history.reset_index(drop=True)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db95e2d-e0e2-4dbd-bd8f-372612f56a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the share of each outcome as a share of either outs or not outs\n",
    "def pa_share(df, hand=\"\"):\n",
    "    safe_list = ['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']\n",
    "    out_list  = ['so', 'fo', 'go', 'lo', 'po']\n",
    "    \n",
    "    # Share reaching base\n",
    "    df[f'safe_share_b{hand}'] = 0\n",
    "    df[f'safe_share_p{hand}'] = 0    \n",
    "    df[f'safe_share_b_long{hand}'] = 0\n",
    "    df[f'safe_share_p_long{hand}'] = 0    \n",
    "    \n",
    "    for stat in safe_list:\n",
    "        df[f'safe_share_b{hand}'] = df[f'safe_share_b{hand}'] + df[f'{stat}_b{hand}']\n",
    "        df[f'safe_share_p{hand}'] = df[f'safe_share_p{hand}'] + df[f'{stat}_p{hand}']\n",
    "        df[f'safe_share_b_long{hand}'] = df[f'safe_share_b_long{hand}'] + df[f'{stat}_b_long{hand}']\n",
    "        df[f'safe_share_p_long{hand}'] = df[f'safe_share_p_long{hand}'] + df[f'{stat}_p_long{hand}']\n",
    "        \n",
    "    # Share out\n",
    "    df[f'out_share_b{hand}'] = 1 - df[f'safe_share_b{hand}']\n",
    "    df[f'out_share_p{hand}'] = 1 - df[f'safe_share_p{hand}']\n",
    "    df[f'out_share_b_long{hand}'] = 1 - df[f'safe_share_b_long{hand}']\n",
    "    df[f'out_share_p_long{hand}'] = 1 - df[f'safe_share_p_long{hand}']\n",
    "    \n",
    "    # Calculate stats as percent of on base\n",
    "    for stat in safe_list:\n",
    "        df[f'{stat}_b{hand}'] = df[f'{stat}_b{hand}'] / df[f'safe_share_b{hand}']\n",
    "        df[f'{stat}_p{hand}'] = df[f'{stat}_p{hand}'] / df[f'safe_share_p{hand}']\n",
    "        df[f'{stat}_b_long{hand}'] = df[f'{stat}_b_long{hand}'] / df[f'safe_share_b_long{hand}']\n",
    "        df[f'{stat}_p_long{hand}'] = df[f'{stat}_p_long{hand}'] / df[f'safe_share_p_long{hand}']\n",
    "    \n",
    "    # Calculate stats as percent of on base\n",
    "    for stat in out_list:\n",
    "        df[f'{stat}_b{hand}'] = df[f'{stat}_b{hand}'] / df[f'out_share_b{hand}']\n",
    "        df[f'{stat}_p{hand}'] = df[f'{stat}_p{hand}'] / df[f'out_share_p{hand}']\n",
    "        df[f'{stat}_b_long{hand}'] = df[f'{stat}_b_long{hand}'] / df[f'out_share_b_long{hand}']\n",
    "        df[f'{stat}_p_long{hand}'] = df[f'{stat}_p_long{hand}'] / df[f'out_share_p_long{hand}']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fa3bf-8e24-4fab-b9ec-38d7c8a05b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the share of each outcome as a share of either outs or not outs\n",
    "def pa_share(df, position=\"_b\", period=\"\", hand=\"\"):\n",
    "    safe_list = ['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']\n",
    "    out_list  = ['so', 'fo', 'go', 'lo', 'po']\n",
    "    \n",
    "    # Share reaching base\n",
    "    df[f'safe_share{position}{period}{hand}'] = 0\n",
    "    \n",
    "    for stat in safe_list:\n",
    "        df[f'safe_share{position}{period}{hand}'] = df[f'safe_share{position}{period}{hand}'] + df[f'{stat}{position}{period}{hand}']\n",
    "        \n",
    "    # Share out\n",
    "    df[f'out_share{position}{period}{hand}'] = 1 - df[f'safe_share{position}{period}{hand}']\n",
    "    \n",
    "    # Calculate stats as percent of on base\n",
    "    for stat in safe_list:\n",
    "        df[f'{stat}{position}{period}{hand}'] = df[f'{stat}{position}{period}{hand}'] / df[f'safe_share{position}{period}{hand}']\n",
    "        \n",
    "    # Calculate stats as percent of on base\n",
    "    for stat in out_list:\n",
    "        df[f'{stat}{position}{period}{hand}'] = df[f'{stat}{position}{period}{hand}'] / df[f'out_share{position}{period}{hand}']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d71ef-7186-4f80-8cde-2f469e242edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in schedule to extract gamePKs based on teams and dates\n",
    "def read_schedule():\n",
    "    # 2022 games\n",
    "    games2022 = statsapi.schedule(start_date=\"03/04/2022\", end_date=\"11/05/2022\")\n",
    "    # 2023 games\n",
    "    games2023 = statsapi.schedule(start_date=\"03/04/2023\", end_date=\"11/01/2023\")\n",
    "    # 2024 games\n",
    "    games2024 = statsapi.schedule(start_date=\"03/04/2024\", end_date=\"11/01/2024\")\n",
    "    # Add 'em together\n",
    "    games = games2022 + games2023 + games2024\n",
    "    \n",
    "    return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740bfd4-633d-404a-9579-4d2bd410226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in games (either from Stats API or pickle)\n",
    "def read_and_save_games(team_map, generate=False):\n",
    "    if generate == True:\n",
    "        # This may generate an HTTP error, particularly in 2022. It's not on my end. Trying multiple times usually works.\n",
    "        games = read_schedule()\n",
    "\n",
    "        # Save to a pickle file\n",
    "        with open(os.path.join(baseball_path, \"games.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(games, file)\n",
    "            \n",
    "    else:\n",
    "        # Load the pickled file\n",
    "        with open(os.path.join(baseball_path, \"games.pkl\"), \"rb\") as file:\n",
    "            games = pickle.load(file)\n",
    "    \n",
    "    # Create dataframe\n",
    "    game_df = pd.DataFrame(games)\n",
    "    # Create date variable\n",
    "    game_df['date'] = game_df['game_date'].str.replace(\"-\",\"\")\n",
    "    # Create year variable\n",
    "    game_df['year'] = game_df['game_date'].str[0:4]\n",
    "    # Select subsample of games to run (exclude spring training, all-star games, exhibitions, and cancelled games\n",
    "    game_df = game_df.query('game_type != \"S\" and game_type != \"A\" and game_type != \"E\" and status != \"Cancelled\"').reset_index(drop=True)\n",
    "            \n",
    "    # Select columns of interest\n",
    "    team_map = team_map[['FULLNAME', 'BBREFTEAM']]\n",
    "    \n",
    "    # Merge in BBREFTEAM\n",
    "    game_df = game_df.merge(team_map, left_on='away_name', right_on='FULLNAME', how='left')\n",
    "    game_df = game_df.merge(team_map, left_on='home_name', right_on='FULLNAME', how='left')\n",
    "        \n",
    "    # Rename\n",
    "    game_df.rename(columns={'BBREFTEAM_x':'away_team', 'BBREFTEAM_y':'home_team'},inplace=True)\n",
    "    game_df.drop(columns={'FULLNAME_x', 'FULLNAME_y', 'away_name', 'home_name'},inplace=True)\n",
    "\n",
    "    # Convert to numeric\n",
    "    game_df['away_score'] = game_df['away_score'].astype('int')\n",
    "    game_df['home_score'] = game_df['home_score'].astype('int')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    game_df.drop_duplicates('game_id', inplace=True, keep='last')\n",
    "    game_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9c159-af02-49ea-98d1-083be9c556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_code(start_time='2023-08-09T07:24:30'):\n",
    "    pause_until = datetime.datetime.fromisoformat(start_time) # or whatever timestamp you gonna need\n",
    "    print((pause_until - datetime.datetime.now()).total_seconds())\n",
    "    time.sleep((pause_until - datetime.datetime.now()).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a832ee-1a24-41a7-8bca-e7582a7b1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_code(start_time='2023-08-09T07:24:30', timezone='EST'):\n",
    "    est_timezone = pytz.timezone('America/New_York')  # Eastern Standard Time (EST)\n",
    "    \n",
    "    # Convert start_time to datetime object in EST timezone\n",
    "    naive_datetime = datetime.datetime.fromisoformat(start_time)\n",
    "    est_start_time = est_timezone.localize(naive_datetime)\n",
    "\n",
    "    # Convert EST time to UTC\n",
    "    utc_start_time = est_start_time.astimezone(pytz.utc)\n",
    "\n",
    "    time_difference = utc_start_time - datetime.datetime.now(pytz.utc)\n",
    "    total_seconds = time_difference.total_seconds()\n",
    "    \n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    \n",
    "    est_time_str = est_start_time.strftime(\"%I:%M%p\")\n",
    "    time_until_str = f\"{est_time_str}. {hours} hours, {minutes} minutes, and {seconds} seconds.\"\n",
    "    \n",
    "    print(\"Time until\", time_until_str)\n",
    "\n",
    "    # Loop with a small sleep interval, checking for interruption\n",
    "    try:\n",
    "        while total_seconds > 0:\n",
    "            time.sleep(1)  # Sleep for 1 second\n",
    "            total_seconds -= 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    ### Set date (may be different in morning)\n",
    "    # Today's Date\n",
    "    # YYYY-MM-DD (datetime)\n",
    "    todaysdate_dt = datetime.date.today()\n",
    "    \n",
    "    # YYYY-MM-DD (string)\n",
    "    todaysdate_dash = str(todaysdate_dt)\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    todaysdate_slash = todaysdate_dash.split(\"-\")\n",
    "    todaysdate_slash = todaysdate_slash[1] + \"/\" + todaysdate_slash[2] + \"/\" + todaysdate_slash[0]\n",
    "    \n",
    "    # YYYYMMDD\n",
    "    todaysdate = todaysdate_dash.replace(\"-\", \"\")\n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    todaysdate_dash = todaysdate[:4] + \"-\" + todaysdate[4:6] + \"-\" + todaysdate[6:]\n",
    "\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now()\n",
    "    \n",
    "    # Subtract one day from the current date to get yesterday's date\n",
    "    yesterday_dt = current_date - datetime.timedelta(days=1)\n",
    "    \n",
    "    # Format yesterday's date as \"YYYYMMDD\"\n",
    "    yesterdaysdate = yesterday_dt.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    yesterdaysdate_slash = yesterdaysdate[4:6] + \"/\" + yesterdaysdate[6:8] + \"/\" + yesterdaysdate[0:4] \n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    yesterdaysdate_dash = yesterdaysdate[:4] + \"-\" + yesterdaysdate[4:6] + \"-\" + yesterdaysdate[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eac81-0adf-4785-9162-db06ee2a0828",
   "metadata": {},
   "source": [
    "A universal function that takes the job of team_map for renaming? \n",
    "just like, look at column with name abbrevs, if CHW, CWS -> CWS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
