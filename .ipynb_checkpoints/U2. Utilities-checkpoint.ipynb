{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdd389f-17f6-489f-b015-f012060f521a",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502d9b4-e3f2-485d-9ed5-1ad7b3cba55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean names\n",
    "def remove_accents(old):\n",
    "    new = re.sub(r'[àáâãäå]', 'a', old)\n",
    "    new = re.sub(r'[èéêë]', 'e', new)\n",
    "    new = re.sub(r'[ìíîï]', 'i', new)\n",
    "    new = re.sub(r'[òóôõö]', 'o', new)\n",
    "    new = re.sub(r'[ùúûü]', 'u', new)\n",
    "    new = re.sub(r'[ñ]', 'n', new)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274cdff-0d58-463e-8b23-51d7b96c2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean names for consistency\n",
    "# # This is really only used now to clean DK Salaries\n",
    "# # You should add players to this if they're not merging on salary information\n",
    "# def name_clean(df, name_col):\n",
    "#     df['Name'] = df[f'{name_col}']\n",
    "#     df['Name'] = np.where(df['Name'] == \"Kike Hernandez\", \"Enrique Hernandez\", df['Name'])\n",
    "#     df['Name'] = np.where(df['Name'] == \"Michael A. Taylor\", \"Michael Taylor\", df['Name'])\n",
    "#     # Note: to get all the de la Cruz's of the world right as last names, we need to manually add the Ji Mans to be one word so they're the first name\n",
    "#     df['Name'] = np.where(df['Name'] == \"Ji Man Choi\", \"Ji-Man Choi\", df['Name'])\n",
    "#     df['Name'] = np.where(df['Name'] == \"Ji Hwan Bae\", \"Ji-Hwan Bae\", df['Name']) # he technically has no dash, but we need it so it treats last name properly\n",
    "#     df['Name'] = np.where(df['Name'] == \"Hyun Jin Ryu\", \"Hyun-Jin Ryu\", df['Name'])\n",
    "    \n",
    "    \n",
    "#     df['Name'] = df.apply(lambda x: remove_accents(x['Name']), axis=1)  # remove accents\n",
    "#     df['Name'] = df['Name'].str.replace(r'[^a-zA-Z0-9 ]', '')\n",
    "#     df['Name'] = df['Name'].str.replace(\"Jr\", \"\")\n",
    "#     df['Name'] = df['Name'].str.replace(\"Sr\", \"\")\n",
    "#     df['Name'] = df['Name'].str.replace(\"II\", \"\")\n",
    "#     df['Name'] = df['Name'].str.replace(\"III\", \"\")\n",
    "#     df['Name'] = df['Name'].str.replace(\".\", \"\")\n",
    "    \n",
    "    \n",
    "#     df['Name'] = df['Name'].str.strip()\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40716f9-8fc0-4c4a-94de-1c469741eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean FanGraphs ID\n",
    "# # Once had individual manual replacements, now could probably be done without\n",
    "# def fix_fangraphs(chadwick):      \n",
    "#     chadwick['key_fangraphs'] = chadwick['key_fangraphs'].astype('str')\n",
    "#     chadwick['key_fangraphs'] = chadwick['key_fangraphs'].str.replace(r'\\.\\d', \"\", regex=True)\n",
    "\n",
    "#     return chadwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4472e-77fb-424c-8f0f-4cc196048035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Reads in select variables from the Chadwick Register\n",
    "# # You should add keys to this if they're not merging with FanGraphs data\n",
    "# def read_chadwick(keep_list):\n",
    "#     # Separated across these suffixes\n",
    "#     chadwick_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "#     # Create a list with a dataframe for each suffix\n",
    "#     dataframe_list = []\n",
    "#     # Loop over suffix\n",
    "#     for char in chadwick_list:\n",
    "#         # Read in that csv, keeping relevant variables\n",
    "#         df = pd.read_csv(\"https://raw.githubusercontent.com/chadwickbureau/register/master/data/people-{}.csv\".format(char), low_memory=False, encoding='utf-8')[keep_list]\n",
    "#         # Drop if missing key_mlbam\n",
    "#         df.dropna(subset=['key_mlbam'], axis=0, inplace=True)\n",
    "#         # Add to dataframe list\n",
    "#         dataframe_list.append(df)\n",
    "#     # Append all dataframes together\n",
    "#     chadwick = pd.concat(dataframe_list, axis=0).reset_index()\n",
    "    \n",
    "#     # Edit missing fangraphs IDs (if all else fails)\n",
    "#     chadwick = fix_fangraphs(chadwick)\n",
    "    \n",
    "    \n",
    "#     chadwick['name_last'].fillna(\"Missing\", inplace=True)\n",
    "#     chadwick['name_first'].fillna(\"Mr\", inplace=True)\n",
    "    \n",
    "#     chadwick['name_first'] = chadwick['name_first'].str.replace(\" \", \"\")\n",
    "#     chadwick['name_first'] = chadwick['name_first'].str.replace(\".\", \"\")\n",
    "#     chadwick['name_last'] = chadwick['name_last'].str.replace(\" \", \"\")\n",
    "#     chadwick['name_last'] = chadwick['name_last'].str.replace(\".\", \"\")\n",
    "    \n",
    "#     # Remove accents\n",
    "#     chadwick['name_last'] = chadwick.apply(lambda x: remove_accents(x['name_last']), axis=1)  # remove accents\n",
    "#     chadwick['name_first'] = chadwick.apply(lambda x: remove_accents(x['name_first']), axis=1)  # remove accents\n",
    "\n",
    "#     # Remove non-alpha numeric characters\n",
    "#     chadwick['name_first'] = chadwick['name_first'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", \"\", x))\n",
    "#     chadwick['name_last'] = chadwick['name_last'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", \"\", x))\n",
    "    \n",
    "#     # Return big dataframe\n",
    "#     return chadwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea85038-1fe8-460b-be74-8085ec33b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Searches for player IDs\n",
    "# def new_ids(player, team, website):\n",
    "#     # Google player plus fangraphs\n",
    "#     search = player + ' ' + team + ' player page ' + website\n",
    "#     url = 'https://www.google.com/search'\n",
    "\n",
    "#     headers = {\n",
    "#         'Accept' : '*/*',\n",
    "#         'Accept-Language': 'en-US,en;q=0.5',\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "#     }\n",
    "#     parameters = {'q': search}\n",
    "\n",
    "    \n",
    "#     # Get info from URL\n",
    "#     content = requests.get(url, headers=headers, params=parameters).text\n",
    "#     soup = BeautifulSoup(content, 'html.parser')\n",
    "#     text = soup.find(id = 'search')\n",
    "#     first_link = text.find('a')\n",
    "    \n",
    "#     # FanGraph's ID is found a little differently\n",
    "#     if website == \"fangraphs\":\n",
    "#         website_id = first_link['href'].split(\"/\")[5]\n",
    "\n",
    "#     # This should work for RotoWire and MLB.com\n",
    "#     else:\n",
    "#         website_id = first_link['href'].split(\"-\")[-1]  \n",
    "    \n",
    "#     # If it's a minor leaguer code, add quotes\n",
    "#     if website_id.startswith(\"sa\"):\n",
    "#         website_id = \"'\" + website_id + \"'\"\n",
    "        \n",
    "    \n",
    "#     return website_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cefef4-564a-472e-97ee-d6145ec7baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reads in contest history\n",
    "# def contest_history(entry_min=8, date_min=\"20220301\", date_max=\"20991231\"):\n",
    "#     history = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"draftkings-contest-entry-history.csv\"))\n",
    "#     history = history[history['Sport'] == \"MLB\"]\n",
    "#     history = history[history['Contest_Entries'] >= entry_min]\n",
    "#     history.drop_duplicates('Contest_Key', inplace=True)\n",
    "    \n",
    "#     history['date'] = pd.to_datetime(history['Contest_Date_EST']).dt.strftime('%Y%m%d')\n",
    "#     history = history[history['date'] > date_min]\n",
    "#     history = history[history['date'] < date_max]\n",
    "\n",
    "#     history = history.reset_index(drop=True)\n",
    "\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db95e2d-e0e2-4dbd-bd8f-372612f56a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the share of each outcome as a share of either outs or not outs\n",
    "# def pa_share(df, hand=\"\"):\n",
    "#     safe_list = ['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']\n",
    "#     out_list  = ['so', 'fo', 'go', 'lo', 'po']\n",
    "    \n",
    "#     # Share reaching base\n",
    "#     df[f'safe_share_b{hand}'] = 0\n",
    "#     df[f'safe_share_p{hand}'] = 0    \n",
    "#     df[f'safe_share_b_long{hand}'] = 0\n",
    "#     df[f'safe_share_p_long{hand}'] = 0    \n",
    "    \n",
    "#     for stat in safe_list:\n",
    "#         df[f'safe_share_b{hand}'] = df[f'safe_share_b{hand}'] + df[f'{stat}_b{hand}']\n",
    "#         df[f'safe_share_p{hand}'] = df[f'safe_share_p{hand}'] + df[f'{stat}_p{hand}']\n",
    "#         df[f'safe_share_b_long{hand}'] = df[f'safe_share_b_long{hand}'] + df[f'{stat}_b_long{hand}']\n",
    "#         df[f'safe_share_p_long{hand}'] = df[f'safe_share_p_long{hand}'] + df[f'{stat}_p_long{hand}']\n",
    "        \n",
    "#     # Share out\n",
    "#     df[f'out_share_b{hand}'] = 1 - df[f'safe_share_b{hand}']\n",
    "#     df[f'out_share_p{hand}'] = 1 - df[f'safe_share_p{hand}']\n",
    "#     df[f'out_share_b_long{hand}'] = 1 - df[f'safe_share_b_long{hand}']\n",
    "#     df[f'out_share_p_long{hand}'] = 1 - df[f'safe_share_p_long{hand}']\n",
    "    \n",
    "#     # Calculate stats as percent of on base\n",
    "#     for stat in safe_list:\n",
    "#         df[f'{stat}_b{hand}'] = df[f'{stat}_b{hand}'] / df[f'safe_share_b{hand}']\n",
    "#         df[f'{stat}_p{hand}'] = df[f'{stat}_p{hand}'] / df[f'safe_share_p{hand}']\n",
    "#         df[f'{stat}_b_long{hand}'] = df[f'{stat}_b_long{hand}'] / df[f'safe_share_b_long{hand}']\n",
    "#         df[f'{stat}_p_long{hand}'] = df[f'{stat}_p_long{hand}'] / df[f'safe_share_p_long{hand}']\n",
    "    \n",
    "#     # Calculate stats as percent of on base\n",
    "#     for stat in out_list:\n",
    "#         df[f'{stat}_b{hand}'] = df[f'{stat}_b{hand}'] / df[f'out_share_b{hand}']\n",
    "#         df[f'{stat}_p{hand}'] = df[f'{stat}_p{hand}'] / df[f'out_share_p{hand}']\n",
    "#         df[f'{stat}_b_long{hand}'] = df[f'{stat}_b_long{hand}'] / df[f'out_share_b_long{hand}']\n",
    "#         df[f'{stat}_p_long{hand}'] = df[f'{stat}_p_long{hand}'] / df[f'out_share_p_long{hand}']\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fa3bf-8e24-4fab-b9ec-38d7c8a05b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the share of each outcome as a share of either outs or not outs\n",
    "# def pa_share(df, position=\"_b\", period=\"\", hand=\"\"):\n",
    "#     safe_list = ['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']\n",
    "#     out_list  = ['so', 'fo', 'go', 'lo', 'po']\n",
    "    \n",
    "#     # Share reaching base\n",
    "#     df[f'safe_share{position}{period}{hand}'] = 0\n",
    "    \n",
    "#     for stat in safe_list:\n",
    "#         df[f'safe_share{position}{period}{hand}'] = df[f'safe_share{position}{period}{hand}'] + df[f'{stat}{position}{period}{hand}']\n",
    "        \n",
    "#     # Share out\n",
    "#     df[f'out_share{position}{period}{hand}'] = 1 - df[f'safe_share{position}{period}{hand}']\n",
    "    \n",
    "#     # Calculate stats as percent of on base\n",
    "#     for stat in safe_list:\n",
    "#         df[f'{stat}{position}{period}{hand}'] = df[f'{stat}{position}{period}{hand}'] / df[f'safe_share{position}{period}{hand}']\n",
    "        \n",
    "#     # Calculate stats as percent of on base\n",
    "#     for stat in out_list:\n",
    "#         df[f'{stat}{position}{period}{hand}'] = df[f'{stat}{position}{period}{hand}'] / df[f'out_share{position}{period}{hand}']\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef76c2-d3bc-4c3e-8137-c553a2110666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9f49f-00f4-4a80-af2b-6109df166942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b476d71-d9b7-436d-97d1-653dff3152f7",
   "metadata": {},
   "source": [
    "##### Pause Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a832ee-1a24-41a7-8bca-e7582a7b1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_code(start_time='2023-08-09T07:24:30', timezone='EST'):\n",
    "    est_timezone = pytz.timezone('America/New_York')  # Eastern Standard Time (EST)\n",
    "    \n",
    "    # Convert start_time to datetime object in EST timezone\n",
    "    naive_datetime = datetime.datetime.fromisoformat(start_time)\n",
    "    est_start_time = est_timezone.localize(naive_datetime)\n",
    "\n",
    "    # Convert EST time to UTC\n",
    "    utc_start_time = est_start_time.astimezone(pytz.utc)\n",
    "\n",
    "    time_difference = utc_start_time - datetime.datetime.now(pytz.utc)\n",
    "    total_seconds = time_difference.total_seconds()\n",
    "    \n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    \n",
    "    est_time_str = est_start_time.strftime(\"%I:%M%p\")\n",
    "    time_until_str = f\"{est_time_str}. {hours} hours, {minutes} minutes, and {seconds} seconds.\"\n",
    "    \n",
    "    print(\"Time until\", time_until_str)\n",
    "\n",
    "    # Loop with a small sleep interval, checking for interruption\n",
    "    try:\n",
    "        while total_seconds > 0:\n",
    "            time.sleep(1)  # Sleep for 1 second\n",
    "            total_seconds -= 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    ### Set date (may be different in morning)\n",
    "    # Today's Date\n",
    "    # YYYY-MM-DD (datetime)\n",
    "    todaysdate_dt = datetime.date.today()\n",
    "    \n",
    "    # YYYY-MM-DD (string)\n",
    "    todaysdate_dash = str(todaysdate_dt)\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    todaysdate_slash = todaysdate_dash.split(\"-\")\n",
    "    todaysdate_slash = todaysdate_slash[1] + \"/\" + todaysdate_slash[2] + \"/\" + todaysdate_slash[0]\n",
    "    \n",
    "    # YYYYMMDD\n",
    "    todaysdate = todaysdate_dash.replace(\"-\", \"\")\n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    todaysdate_dash = todaysdate[:4] + \"-\" + todaysdate[4:6] + \"-\" + todaysdate[6:]\n",
    "\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now()\n",
    "    \n",
    "    # Subtract one day from the current date to get yesterday's date\n",
    "    yesterday_dt = current_date - datetime.timedelta(days=1)\n",
    "    \n",
    "    # Format yesterday's date as \"YYYYMMDD\"\n",
    "    yesterdaysdate = yesterday_dt.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    yesterdaysdate_slash = yesterdaysdate[4:6] + \"/\" + yesterdaysdate[6:8] + \"/\" + yesterdaysdate[0:4] \n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    yesterdaysdate_dash = yesterdaysdate[:4] + \"-\" + yesterdaysdate[4:6] + \"-\" + yesterdaysdate[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7c23d-7760-4eb7-a4f5-958cf1fe3413",
   "metadata": {},
   "source": [
    "##### Identify Pareto-Optimal Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291910d3-199d-4157-8f00-08f74178f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimal(df, objectives, directions):\n",
    "    data = df[objectives].values\n",
    "    num_points = data.shape[0]\n",
    "\n",
    "    # Convert objectives based on direction\n",
    "    for i, direction in enumerate(directions):\n",
    "        if direction == \"Maximize\":\n",
    "            data[:, i] *= -1\n",
    "\n",
    "    # Pareto front mask\n",
    "    pareto_mask = np.ones(num_points, dtype=bool)\n",
    "\n",
    "    # Check for dominance\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            if i != j:\n",
    "                # Row j dominates row i if it's better in at least one objective and not worse in others\n",
    "                if np.all(data[j] <= data[i]) and np.any(data[j] < data[i]):\n",
    "                    pareto_mask[i] = False\n",
    "                    break\n",
    "\n",
    "    # Return the Pareto-optimal rows\n",
    "    return df[pareto_mask].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e55232-7856-4de0-881e-65227fc790d7",
   "metadata": {},
   "source": [
    "##### Create Game DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673590db-9dbb-4e5f-9c2f-7be60560686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_games(start_date, end_date, team_dict):\n",
    "    \"\"\"\n",
    "    Fetch game schedules for a given date range.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date (str): Start date in \"YYYYMMDD\" format.\n",
    "    - end_date (str): End date in \"YYYYMMDD\" format.\n",
    "    \n",
    "    Returns:\n",
    "    - Data Frame: Combined schedule for the specified date range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reformat dates\n",
    "    start_date = start_date[4:6] + \"/\" + start_date[6:8] + \"/\" + start_date[:4] \n",
    "    end_date = end_date[4:6] + \"/\" + end_date[6:8] + \"/\" + end_date[:4] \n",
    "\n",
    "    # Extract year    \n",
    "    start_year = int(start_date.split(\"/\")[-1])\n",
    "    end_year = int(end_date.split(\"/\")[-1])\n",
    "    \n",
    "    # Initialize an empty list to hold game schedules\n",
    "    games = []\n",
    "    \n",
    "    # Iterate through each year in the range and fetch schedules\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Determine the bounds for statsapi.schedule\n",
    "        year_start = start_date if year == start_year else f\"01/01/{year}\"\n",
    "        year_end = end_date if year == end_year else f\"12/31/{year}\"\n",
    "        \n",
    "        # Fetch and append the schedules\n",
    "        games.extend(statsapi.schedule(start_date=year_start, end_date=year_end))\n",
    "    \n",
    "    # Create dataframe\n",
    "    game_df = pd.DataFrame(games)\n",
    "    # Create date variable\n",
    "    game_df['date'] = game_df['game_date'].str.replace(\"-\",\"\")\n",
    "    # Create year variable\n",
    "    game_df['year'] = game_df['game_date'].str[0:4]\n",
    "    # Select subsample of games to run (exclude spring training, all-star games, exhibitions, and cancelled games\n",
    "    game_df = game_df.query('game_type != \"S\" and game_type != \"A\" and game_type != \"E\" and status != \"Cancelled\" and status != \"Postponed\"').reset_index(drop=True)\n",
    "            \n",
    "    # Map in team names\n",
    "    game_df['away_team'] = game_df['away_name'].map(team_dict)\n",
    "    game_df['home_team'] = game_df['home_name'].map(team_dict)\n",
    "\n",
    "    # Convert to numeric\n",
    "    game_df['away_score'] = game_df['away_score'].astype('int')\n",
    "    game_df['home_score'] = game_df['home_score'].astype('int')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    game_df.drop_duplicates('game_id', inplace=True, keep='last')\n",
    "    game_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    game_df.drop(columns=['home_pitcher_note', 'away_pitcher_note', 'national_broadcasts', 'series_status', 'summary'], inplace=True)\n",
    "    \n",
    "    \n",
    "    return game_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaf000-e550-4cc7-964f-e10d5bab23e1",
   "metadata": {},
   "source": [
    "##### Create Contest Guide DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793f2e45-6d3e-4827-8a02-a6feea44c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contests(start_date=None, end_date=None, name=None, entryFee=None, exclusions=['vs', 'Turbo', '@']):\n",
    "    # Read in files\n",
    "    all_files = glob.glob(os.path.join(baseball_path, \"A09. Contest Guides\", \"*.csv\"))\n",
    "    df_list = [pd.read_csv(file, dtype='str') for file in all_files]\n",
    "    contest_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Convert data types\n",
    "    contest_df['game_id'] = contest_df['game_id'].astype(int)\n",
    "    contest_df['date'] = pd.to_datetime(contest_df['date'].astype(str), format='%Y%m%d')\n",
    "\n",
    "    # Filter\n",
    "    if start_date is not None:\n",
    "        contest_df = contest_df[contest_df['date'] >= pd.to_datetime(start_date, format='%Y%m%d')]\n",
    "    if end_date is not None:\n",
    "        contest_df = contest_df[contest_df['date'] <= pd.to_datetime(end_date, format='%Y%m%d')]\n",
    "    if name is not None:\n",
    "        contest_df = contest_df[contest_df['name'].str.contains(name)]\n",
    "    if entryFee is not None:\n",
    "        contest_df = contest_df[contest_df['entryFee'] == entryFee]\n",
    "    if exclusions != []:\n",
    "        for exclusion in exclusions:\n",
    "            contest_df = contest_df[~contest_df['name'].str.contains(exclusion)]\n",
    "\n",
    "    # Convert date to standard format\n",
    "    contest_df['date'] = contest_df['date'].astype(str).str.replace(\"-\", \"\")\n",
    "\n",
    "    # Calculate slate_size \n",
    "    contest_df['slate_size'] = contest_df.groupby('contestKey')['contestKey'].transform('count') \n",
    "\n",
    "   \n",
    "    \n",
    "    return contest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eac81-0adf-4785-9162-db06ee2a0828",
   "metadata": {},
   "source": [
    "##### Create Universal Team Map Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad76028-82f8-409f-8dda-d99b3e842c3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m team_map \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(baseball_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtilities\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeam Map.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "team_map = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Team Map.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc3711-8642-429a-aaec-1fc27aaae7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "team_dict = {}\n",
    "\n",
    "# Filter columns that end with \"TEAM\"\n",
    "team_columns = [col for col in team_map.columns if col.endswith(\"TEAM\") or col.endswith(\"NAME\")]\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for _, row in team_map.iterrows():\n",
    "    bbref_team = row['BBREFTEAM']  # Get the BBREFTEAM value\n",
    "    # Iterate over filtered columns in the row\n",
    "    for column in team_columns:\n",
    "        value = row[column]\n",
    "        if pd.notna(value):  # Skip NaN values\n",
    "            team_dict[value] = bbref_team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593df449-248d-4fad-a220-cbee3a081003",
   "metadata": {},
   "source": [
    "##### Create Venue Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f38761-9163-4724-9a76-c7054f60614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_venue_map(write=False):\n",
    "    # Fetch JSON data from the URL\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract venue details \n",
    "    venues = data.get(\"venues\", data)  \n",
    "    \n",
    "    # Normalize the JSON into a DataFrame\n",
    "    df = pd.json_normalize(venues)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if write == True:\n",
    "        df.sort_values('id').to_csv(os.path.join(baseball_path, \"Utilities\", \"Venue Map.csv\"), index=False)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1d101-312c-440b-a9db-24be17f1aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_map_df = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Venue Map.csv\"))[['id', 'fieldInfo.roofType', 'location.azimuthAngle']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
