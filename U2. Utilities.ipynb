{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdd389f-17f6-489f-b015-f012060f521a",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfd222-ad2e-4306-b1ba-8fd358d7e592",
   "metadata": {},
   "source": [
    "##### Remove Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502d9b4-e3f2-485d-9ed5-1ad7b3cba55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(old):\n",
    "    new = re.sub(r'[àáâãäå]', 'a', old)\n",
    "    new = re.sub(r'[èéêë]', 'e', new)\n",
    "    new = re.sub(r'[ìíîï]', 'i', new)\n",
    "    new = re.sub(r'[òóôõö]', 'o', new)\n",
    "    new = re.sub(r'[ùúûü]', 'u', new)\n",
    "    new = re.sub(r'[ñ]', 'n', new)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b476d71-d9b7-436d-97d1-653dff3152f7",
   "metadata": {},
   "source": [
    "##### Pause Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a832ee-1a24-41a7-8bca-e7582a7b1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_code(start_time='2023-08-09T07:24:30', timezone='EST'):\n",
    "    est_timezone = pytz.timezone('America/New_York')  # Eastern Standard Time (EST)\n",
    "    \n",
    "    # Convert start_time to datetime object in EST timezone\n",
    "    naive_datetime = datetime.datetime.fromisoformat(start_time)\n",
    "    est_start_time = est_timezone.localize(naive_datetime)\n",
    "\n",
    "    # Convert EST time to UTC\n",
    "    utc_start_time = est_start_time.astimezone(pytz.utc)\n",
    "\n",
    "    time_difference = utc_start_time - datetime.datetime.now(pytz.utc)\n",
    "    total_seconds = time_difference.total_seconds()\n",
    "    \n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    \n",
    "    est_time_str = est_start_time.strftime(\"%I:%M%p\")\n",
    "    time_until_str = f\"{est_time_str}. {hours} hours, {minutes} minutes, and {seconds} seconds.\"\n",
    "    \n",
    "    print(\"Time until\", time_until_str)\n",
    "\n",
    "    # Loop with a small sleep interval, checking for interruption\n",
    "    try:\n",
    "        while total_seconds > 0:\n",
    "            time.sleep(1)  # Sleep for 1 second\n",
    "            total_seconds -= 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    ### Set date (may be different in morning)\n",
    "    # Today's Date\n",
    "    # YYYY-MM-DD (datetime)\n",
    "    todaysdate_dt = datetime.date.today()\n",
    "    \n",
    "    # YYYY-MM-DD (string)\n",
    "    todaysdate_dash = str(todaysdate_dt)\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    todaysdate_slash = todaysdate_dash.split(\"-\")\n",
    "    todaysdate_slash = todaysdate_slash[1] + \"/\" + todaysdate_slash[2] + \"/\" + todaysdate_slash[0]\n",
    "    \n",
    "    # YYYYMMDD\n",
    "    todaysdate = todaysdate_dash.replace(\"-\", \"\")\n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    todaysdate_dash = todaysdate[:4] + \"-\" + todaysdate[4:6] + \"-\" + todaysdate[6:]\n",
    "\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now()\n",
    "    \n",
    "    # Subtract one day from the current date to get yesterday's date\n",
    "    yesterday_dt = current_date - datetime.timedelta(days=1)\n",
    "    \n",
    "    # Format yesterday's date as \"YYYYMMDD\"\n",
    "    yesterdaysdate = yesterday_dt.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    yesterdaysdate_slash = yesterdaysdate[4:6] + \"/\" + yesterdaysdate[6:8] + \"/\" + yesterdaysdate[0:4] \n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    yesterdaysdate_dash = yesterdaysdate[:4] + \"-\" + yesterdaysdate[4:6] + \"-\" + yesterdaysdate[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7c23d-7760-4eb7-a4f5-958cf1fe3413",
   "metadata": {},
   "source": [
    "##### Identify Pareto-Optimal Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291910d3-199d-4157-8f00-08f74178f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimal(df, objectives, directions):\n",
    "    data = df[objectives].values\n",
    "    num_points = data.shape[0]\n",
    "\n",
    "    # Convert objectives based on direction\n",
    "    for i, direction in enumerate(directions):\n",
    "        if direction == \"Maximize\":\n",
    "            data[:, i] *= -1\n",
    "\n",
    "    # Pareto front mask\n",
    "    pareto_mask = np.ones(num_points, dtype=bool)\n",
    "\n",
    "    # Check for dominance\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            if i != j:\n",
    "                # Row j dominates row i if it's better in at least one objective and not worse in others\n",
    "                if np.all(data[j] <= data[i]) and np.any(data[j] < data[i]):\n",
    "                    pareto_mask[i] = False\n",
    "                    break\n",
    "\n",
    "    # Return the Pareto-optimal rows\n",
    "    return df[pareto_mask].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e55232-7856-4de0-881e-65227fc790d7",
   "metadata": {},
   "source": [
    "##### Create Game DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673590db-9dbb-4e5f-9c2f-7be60560686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_games(start_date, end_date, team_dict):\n",
    "    \"\"\"\n",
    "    Fetch game schedules for a given date range.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date (str): Start date in \"YYYYMMDD\" format.\n",
    "    - end_date (str): End date in \"YYYYMMDD\" format.\n",
    "    \n",
    "    Returns:\n",
    "    - Data Frame: Combined schedule for the specified date range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reformat dates\n",
    "    start_date = start_date[4:6] + \"/\" + start_date[6:8] + \"/\" + start_date[:4] \n",
    "    end_date = end_date[4:6] + \"/\" + end_date[6:8] + \"/\" + end_date[:4] \n",
    "\n",
    "    # Extract year    \n",
    "    start_year = int(start_date.split(\"/\")[-1])\n",
    "    end_year = int(end_date.split(\"/\")[-1])\n",
    "    \n",
    "    # Initialize an empty list to hold game schedules\n",
    "    games = []\n",
    "    \n",
    "    # Iterate through each year in the range and fetch schedules\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Determine the bounds for statsapi.schedule\n",
    "        year_start = start_date if year == start_year else f\"01/01/{year}\"\n",
    "        year_end = end_date if year == end_year else f\"12/31/{year}\"\n",
    "        \n",
    "        # Fetch and append the schedules\n",
    "        games.extend(statsapi.schedule(start_date=year_start, end_date=year_end))\n",
    "    \n",
    "    # Create dataframe\n",
    "    game_df = pd.DataFrame(games)\n",
    "    # Create date variable\n",
    "    game_df['date'] = game_df['game_date'].str.replace(\"-\",\"\")\n",
    "    # Create year variable\n",
    "    game_df['year'] = game_df['game_date'].str[0:4]\n",
    "    # Select subsample of games to run (exclude spring training, all-star games, exhibitions, and cancelled games\n",
    "    game_df = game_df.query('game_type != \"S\" and game_type != \"A\" and game_type != \"E\" and status != \"Cancelled\" and status != \"Postponed\"').reset_index(drop=True)\n",
    "\n",
    "    # Map in team names\n",
    "    game_df['away_team'] = game_df['away_name'].map(team_dict)\n",
    "    game_df['home_team'] = game_df['home_name'].map(team_dict)\n",
    "\n",
    "    # Convert to numeric\n",
    "    game_df['away_score'] = game_df['away_score'].astype('int')\n",
    "    game_df['home_score'] = game_df['home_score'].astype('int')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    game_df.drop_duplicates('game_id', inplace=True, keep='last')\n",
    "    game_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    game_df.drop(columns=['home_pitcher_note', 'away_pitcher_note', 'national_broadcasts', 'series_status', 'summary'], inplace=True)\n",
    "    \n",
    "    \n",
    "    return game_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaf000-e550-4cc7-964f-e10d5bab23e1",
   "metadata": {},
   "source": [
    "##### Create Contest Guide DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd06fc5-4b09-476e-b85e-c5b8db55213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contests(start_date=None, end_date=None, name=None, entryFee=None, exclusions=['vs', 'Turbo', '@']):\n",
    "    # Get all file paths\n",
    "    all_files = glob.glob(os.path.join(baseball_path, \"A09. Contest Guides\", \"*.csv\"))\n",
    "\n",
    "    # Parallel read\n",
    "    df_list = Parallel(n_jobs=-1)(\n",
    "        delayed(pd.read_csv)(file, dtype='str') for file in all_files\n",
    "    )\n",
    "\n",
    "    # Concatenate\n",
    "    contest_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Convert data types\n",
    "    contest_df['game_id'] = contest_df['game_id'].astype(int)\n",
    "    contest_df['date'] = pd.to_datetime(contest_df['date'].astype(str), format='%Y%m%d')\n",
    "\n",
    "    # Apply filters\n",
    "    if start_date is not None:\n",
    "        contest_df = contest_df[contest_df['date'] >= pd.to_datetime(start_date, format='%Y%m%d')]\n",
    "    if end_date is not None:\n",
    "        contest_df = contest_df[contest_df['date'] <= pd.to_datetime(end_date, format='%Y%m%d')]\n",
    "    if name is not None:\n",
    "        contest_df = contest_df[contest_df['name'].str.contains(name)]\n",
    "    if exclusions != []:\n",
    "        for exclusion in exclusions:\n",
    "            contest_df = contest_df[~contest_df['name'].str.contains(exclusion)]\n",
    "\n",
    "    # Convert date back to string\n",
    "    contest_df['date'] = contest_df['date'].dt.strftime('%Y%m%d')\n",
    "\n",
    "    # Calculate slate_size \n",
    "    contest_df['slate_size'] = contest_df.groupby('contestKey')['contestKey'].transform('count')\n",
    "\n",
    "    return contest_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eac81-0adf-4785-9162-db06ee2a0828",
   "metadata": {},
   "source": [
    "##### Create Universal Team Map Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad76028-82f8-409f-8dda-d99b3e842c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_map = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Team Map.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc3711-8642-429a-aaec-1fc27aaae7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "team_dict = {}\n",
    "\n",
    "# Filter columns that end with \"TEAM\"\n",
    "team_columns = [col for col in team_map.columns if col.endswith(\"TEAM\") or col.endswith(\"NAME\") or col.endswith(\"Id\")]\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for _, row in team_map.iterrows():\n",
    "    bbref_team = row['BBREFTEAM']  # Get the BBREFTEAM value\n",
    "    # Iterate over filtered columns in the row\n",
    "    for column in team_columns:\n",
    "        value = row[column]\n",
    "        if pd.notna(value):  # Skip NaN values\n",
    "            team_dict[value] = bbref_team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593df449-248d-4fad-a220-cbee3a081003",
   "metadata": {},
   "source": [
    "##### Create Venue Map DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f38761-9163-4724-9a76-c7054f60614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_venue_map(write=False):\n",
    "    # Fetch JSON data from the URL\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract venue details \n",
    "    venues = data.get(\"venues\", data)  \n",
    "    \n",
    "    # Normalize the JSON into a DataFrame\n",
    "    df = pd.json_normalize(venues)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if write == True:\n",
    "        df.sort_values('id').to_csv(os.path.join(baseball_path, \"Utilities\", \"Venue Map.csv\"), index=False)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8e957-42ec-4cb1-9d19-249631fd8e85",
   "metadata": {},
   "source": [
    "##### Add Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1d101-312c-440b-a9db-24be17f1aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Venue Map\n",
    "venue_map_df = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Venue Map.csv\"))\n",
    "\n",
    "# George M. Steinbrenner\n",
    "venue_map_df.loc[venue_map_df['id'] == 2523, ['fieldInfo.leftCenter', 'fieldInfo.rightCenter']] = [399.0, 385.0] # Yankee Stadium dimensions\n",
    "# Sutter Health Park\n",
    "venue_map_df.loc[venue_map_df['id'] == 2529, ['fieldInfo.leftCenter', 'fieldInfo.rightCenter']] = [375.0, 368.0] # https://x.com/JonPgh/status/1875224135573594599"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-base)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
