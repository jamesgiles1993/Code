{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# M01. Park and Weather Factors\n",
    "- This calculated Park x Weather Factors\n",
    "- Type: Model\n",
    "- Run Frequency: Daily\n",
    "- Sources:\n",
    "    - MLB API\n",
    "    - Steamer\n",
    "- Created: 12/10/2024\n",
    "- Updated: 12/17/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce0f6-2eb6-4ecd-841b-2e99fc13c281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02405859-9235-419d-bb06-52eb83ede249",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b979a91-7a59-4d4b-b126-73771a2536ca",
   "metadata": {},
   "source": [
    "Create Latest PA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208611b0-2531-45b0-b7e7-ae2db5db9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "complete_dataset = pd.read_csv(os.path.join(baseball_path, \"Complete Dataset - Unadjusted.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799477f-8575-4874-8049-7c49fb663613",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['type', 'id', 'description', 'rbi', 'awayScore', 'homeScore', 'postOnFirst', 'postOnSecond', 'postOnThird', 'runner_id', 'start', 'end', 'movementReason', 'isScoringEvent', 'earned', 'pitch_number', 'pitch_name', 'hc_x', 'hc_y', 'hit_location', 'totalDistance', 'launchSpeed', 'launch_angle', 'launch_speed_angle', 'h', 'tb', 'reached', 'faced', 'outs_total', 'outs_pa', 'b1_inning', 'b2_inning', 'b3_inning', 'hr_inning', 'bb_inning', 'hbp_inning', 'so_inning', 'fo_inning', 'go_inning', 'lo_inning', 'po_inning', 'h_inning', 'tb_inning', 'reached_inning', 'faced_inning', 'rbi_inning', 'outs_pa_inning', 'b1_game', 'b2_game', 'b3_game', 'hr_game', 'bb_game', 'hbp_game', 'so_game', 'fo_game', 'go_game', 'lo_game', 'po_game', 'h_game', 'tb_game', 'reached_game', 'faced_game', 'rbi_game', 'outs_pa_game', 'bottom', 'atBatIndex_min', 'first_ab', 'atBatIndex_max', 'pulled', 'times_faced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677650e-adc0-4da5-bec1-a46aaa7017c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.drop(columns=drop_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac940b8-47d4-4217-9a5f-346471ae74af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514c6742-b36b-4c49-8c4d-b40423c3f013",
   "metadata": {},
   "source": [
    "##### Open Meteo Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c7c7d-f87b-4227-9f84-b68c3d80d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weather_df = pd.concat(map(pd.read_csv, glob.glob(r\"C:\\Users\\james\\Documents\\MLB\\Database\\A06. Weather\\1. Open Meteo\\*.csv\")), ignore_index=True)[\n",
    "       ['game_id', 'year', 'venue_name', 'location.defaultCoordinates.latitude',\n",
    "       'location.defaultCoordinates.longitude', 'fieldInfo.leftLine',\n",
    "       'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter',\n",
    "       'fieldInfo.rightCenter', 'location.elevation', 'location.azimuthAngle',\n",
    "       'fieldInfo.roofType', 'active', 'temperature_2m',\n",
    "       'relative_humidity_2m', 'dew_point_2m', 'surface_pressure',\n",
    "       'wind_speed_10m', 'wind_direction_10m', 'weather_code',\n",
    "       'precipitation_probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf472b-d6d8-4b4f-b14d-06f2a305e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors(row, azimuth_column, wind_column, speed_column):\n",
    "    angle = row[wind_column] - row[azimuth_column]\n",
    "    \n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row[speed_column] * -1\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row[speed_column] * -1\n",
    "\n",
    "    return pd.Series([x_vect, y_vect], index=['x_vect', 'y_vect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca489fb2-c872-4db3-b7f7-b98d185a1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[['meteo_x_vect', 'meteo_y_vect']] = weather_df.apply(lambda row: calculate_vectors(row, 'location.azimuthAngle', 'wind_direction_10m', 'wind_speed_10m'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fe765-1d88-432f-b7af-e10df2d2cc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840428eb-78d5-4ba2-b585-d20b192e0456",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c6c6c-2604-47fa-86a9-f5d1e6459c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = complete_dataset.merge(weather_df, left_on=['gamePk'], right_on=['game_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deea563-4965-499c-ac58-41894b73c64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1b42d3-21c5-4cda-92ea-748580d3884b",
   "metadata": {},
   "source": [
    "### Base Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13121744-f9d8-4d98-8797-267f410948e6",
   "metadata": {},
   "source": [
    "Calculate average stats in a given base year <br>\n",
    "Note: This only has to be run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8511a-581b-47b4-8345-36b219d3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rates(df, base_year=2014):\n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Select period of interest\n",
    "    df = df[df['game_date'].dt.year == base_year]\n",
    "\n",
    "    # Calculate averages over period of interest\n",
    "    base_rate_df = pd.DataFrame(df[events_list].mean()).T\n",
    "\n",
    "    \n",
    "    return base_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a930b6-360b-42d0-99c2-8000eae19653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b183ab-f43b-4c9c-a209-b2ee24c3f190",
   "metadata": {},
   "source": [
    "### Game Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed47d7-2bce-4d17-ac4e-eb608eda8884",
   "metadata": {},
   "source": [
    "Average rates within the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf7da9-72c3-4c3c-985e-c57db1a405d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(df):    \n",
    "    # Calculate averages by game\n",
    "    game_avgs = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'])[events_list].mean().reset_index()\n",
    "\n",
    "    # Add the 'pas' column to count the number of observations in each group\n",
    "    game_avgs['pas'] = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']).size().values\n",
    "\n",
    "    # Sort by date\n",
    "    game_avgs.sort_values(['game_date'], ascending=True, inplace=True)\n",
    "\n",
    "    \n",
    "    return game_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabeca8c-8017-45b2-8942-5d90250eea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_average_df = game_averages(complete_dataset)\n",
    "# game_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a92c3e-8bbf-4687-9079-68d7346bede0",
   "metadata": {},
   "source": [
    "### Player Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82edc50-a380-4caf-87ab-650d932a1e4a",
   "metadata": {},
   "source": [
    "Average stats of all the players in the game, coming into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc16b3-2c4b-4f89-b1e5-32f0356f818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_averages(df):\n",
    "    # Stats to average\n",
    "    batter_inputs_short = [f\"{event}_b_long\" for event in events_list]\n",
    "    pitcher_inputs_short = [f\"{event}_p_long\" for event in events_list]\n",
    "\n",
    "    # Apply stats from last at bat to entire game\n",
    "    df[batter_inputs_short] = df.groupby(['gamePk', 'batter'])[batter_inputs_short].transform('last')\n",
    "    df[pitcher_inputs_short] = df.groupby(['gamePk', 'pitcher'])[pitcher_inputs_short].transform('last')\n",
    "    \n",
    "    # Calculate player averages by game\n",
    "    batter_avgs = df.groupby(['gamePk'])[batter_inputs_short].mean().reset_index()\n",
    "    pitcher_avgs = df.groupby(['gamePk'])[pitcher_inputs_short].mean().reset_index()\n",
    "\n",
    "    # Concatenate together\n",
    "    player_avgs = pd.concat([batter_avgs, pitcher_avgs.drop(columns=['gamePk'])], axis=1)\n",
    "    \n",
    "    \n",
    "    return player_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f1d09-dc1a-4d72-80a1-2fc6f5c0e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_average_df = player_averages(complete_dataset)\n",
    "# player_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9a52f-86cd-4886-ac94-23bb9b111af7",
   "metadata": {},
   "source": [
    "### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617b005-f5ae-4267-bf08-551051f5cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def league_average(complete_dataset, days=30):\n",
    "    # Calculate daily sum of events\n",
    "    league_avg = complete_dataset.groupby('game_date')[events_list].sum().reset_index()\n",
    "    # Calculate total events\n",
    "    league_avg['pas'] = league_avg[events_list].sum(axis=1)\n",
    "    \n",
    "    # Use rolling sum including the current row\n",
    "    for event in events_list + ['pas']:\n",
    "        league_avg[f'{event}_sum'] = league_avg[event].rolling(window=days, min_periods=1).sum()\n",
    "\n",
    "    # Calculate average\n",
    "    for event in events_list:\n",
    "        league_avg[f'{event}_lg'] = league_avg[f'{event}_sum'] / league_avg['pas_sum']\n",
    "\n",
    "        \n",
    "    return league_avg[[\"game_date\"] + [col for col in league_avg if \"_lg\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbf862-3c02-4922-9b6f-0b22d6faf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# league_average_df = league_average(complete_dataset, 30)\n",
    "# league_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb155b-c601-4d64-a88e-c75dce2b7c7f",
   "metadata": {},
   "source": [
    "### Park Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8674102-da42-42e1-9610-d8cf2c36cb21",
   "metadata": {},
   "source": [
    "##### Rolling Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344bd9-99ba-4b5e-82bd-ff0a4f568eb1",
   "metadata": {},
   "source": [
    "Average of stats over last rolling_window games - excluding game of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31d871-22da-4911-a98b-c3ec81d8d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_averages(game_avgs, rolling_window, column):\n",
    "    # Sort by group column and date\n",
    "    rolling_avgs = game_avgs.sort_values([column, 'game_date']).copy()\n",
    "\n",
    "    # Compute rolling sum for `pas`\n",
    "    rolling_avgs['pas_rolling'] = rolling_avgs.groupby(column)['pas'].transform(\n",
    "        lambda x: x.rolling(window=rolling_window, min_periods=1, closed=\"right\").sum()\n",
    "    )\n",
    "\n",
    "    # Define function for rolling weighted average\n",
    "    def weighted_avg(group):\n",
    "        return (\n",
    "            group[events_list]\n",
    "            .rolling(window=rolling_window, min_periods=1, closed=\"right\")\n",
    "            .apply(lambda x: (x * group.loc[x.index, 'pas']).sum() / group.loc[x.index, 'pas'].sum(), raw=False)\n",
    "        )\n",
    "\n",
    "    # Apply rolling weighted average by the given column\n",
    "    rolling_avgs[events_list] = rolling_avgs.groupby(column, group_keys=False).apply(weighted_avg)\n",
    "\n",
    "    \n",
    "    return rolling_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55a7ca-d0e4-4a46-9a46-69395cd4f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to account for small sample parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3941156-6fff-405a-ae97-fdb0d6605d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_average_df = rolling_averages(game_average_df, 243, 'venue_id')\n",
    "# park_average_df = park_average_df[['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + events_list + ['pas_rolling']]\n",
    "# park_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b74003-2b91-4d48-816c-3cc638496986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_average_df = rolling_averages(game_average_df, 243, 'away_name')\n",
    "# team_average_df = team_average_df[['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + events_list + ['pas_rolling']]\n",
    "# team_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aa6c4-b995-464b-9450-0d7581c12282",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24c5e-2448-403b-b822-a8a282cf9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_park_factors(park_avgs, team_avgs):\n",
    "    # Sort by game_date\n",
    "    park_avgs = park_avgs.sort_values('game_date')\n",
    "    team_avgs = team_avgs.sort_values('game_date')\n",
    "\n",
    "    # Create uniform team_name variable equal to name of interest\n",
    "    park_avgs['team_name'] = park_avgs['home_name'].copy()\n",
    "    team_avgs['team_name'] = team_avgs['away_name'].copy()\n",
    "\n",
    "    # Set to datetime\n",
    "    # park_avgs['game_date'] = pd.to_datetime(park_avgs['game_date'], format='%m/%d/%Y')\n",
    "    # team_avgs['game_date'] = pd.to_datetime(team_avgs['game_date'], format='%m/%d/%Y')\n",
    "    park_avgs['game_date'] = pd.to_datetime(park_avgs['game_date'], format='mixed')\n",
    "    team_avgs['game_date'] = pd.to_datetime(team_avgs['game_date'], format='mixed')\n",
    "    \n",
    "    # Perform merge_asof\n",
    "    park_factor_df = pd.merge_asof(park_avgs, team_avgs, left_on='game_date', right_on='game_date', by='team_name', direction='backward', suffixes=('_park', '_team'))\n",
    "\n",
    "    # Calculate park factors\n",
    "    for stat in events_list:\n",
    "        park_factor_df[f'{stat}_pfx'] = park_factor_df[f'{stat}_park'] / park_factor_df[f'{stat}_team'] \n",
    "        \n",
    "    park_factor_df.rename(columns={'gamePk_park': 'gamePk'}, inplace=True)\n",
    "    keep_columns = ['gamePk'] + [col for col in park_factor_df.columns if col.endswith('pfx')]\n",
    "\n",
    "    \n",
    "    return park_factor_df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8cfd-1241-481f-8a33-7788417e22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df = create_park_factors(park_average_df, team_average_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf439c-b77e-4230-aee2-8a4397b7b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0f210-8aa6-4479-8c1b-f6cc194edac0",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd52ba2-27f4-4260-97ad-a2e7a2892d29",
   "metadata": {},
   "source": [
    "Merge together game averages, player averages, and park factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba144343-39bf-499c-b1d9-2a6a68bf46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_df(complete_dataset, league_average_df, park_factor_df):\n",
    "    # Merge on league averages\n",
    "    analysis_df = pd.merge(complete_dataset, league_average_df, on=['game_date'], how='inner')\n",
    "    # Merge on park factors\n",
    "    analysis_df = pd.merge(analysis_df, park_factor_df, on='gamePk', how='inner')\n",
    "   \n",
    "    \n",
    "    # Extract dummies from venues\n",
    "    venue_dummy_df = pd.get_dummies(analysis_df['venue_id'].astype(int), prefix='venue')\n",
    "    # Extract dummy column names\n",
    "    venue_dummies = list(venue_dummy_df.columns)\n",
    "    \n",
    "    # Add in dummies\n",
    "    analysis_df = pd.concat([analysis_df, venue_dummy_df], axis=1)\n",
    "    \n",
    "    # Select variables to keep\n",
    "    mlb_variables = ['x_vect', 'y_vect', 'temperature', 'weather']\n",
    "    meteo_variables = ['meteo_x_vect', 'meteo_y_vect', \n",
    "                       'fieldInfo.leftLine', 'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter', 'fieldInfo.rightCenter', \n",
    "                       'location.elevation', 'fieldInfo.roofType', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'surface_pressure']\n",
    "    variables = mlb_variables + meteo_variables + venue_dummies\n",
    "    \n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    # Select relevant variables and drop missings\n",
    "    analysis_df = analysis_df[[\"eventsModel\", 'gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'batter', 'pitcher', 'batSide', 'pitchHand'] + variables + [col for col in analysis_df if col.endswith(\"_lg\")]]\n",
    "    \n",
    "    # Remove cut\n",
    "    analysis_df = analysis_df[analysis_df['eventsModel'] != \"Cut\"]\n",
    "    \n",
    "    \n",
    "    return analysis_df, venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e6f67-06ac-43c8-9436-0175a0a197ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_df, venue_dummies = create_analysis_df(complete_dataset, league_average_df, park_factor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605e231-bb8d-4f59-b018-63e4f7e80c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0792a0-b2a1-4764-8f7b-b10b6c3244ba",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527bfb6-c394-4d08-9f36-d3ba950ae18e",
   "metadata": {},
   "source": [
    "Dome adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98307d00-947e-4c6e-b8c3-14add6ca1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = complete_dataset['weather'].str.contains('Roof|Dome', case=False, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c58b5-d2a5-49c3-b158-5d4d5b88e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.loc[mask, 'temperature'] = 70\n",
    "complete_dataset.loc[mask, 'x_vect'] = 0\n",
    "complete_dataset.loc[mask, 'y_vect'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a40be-8923-46fa-89b2-5318321403c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.loc[mask, 'temperature_2m'] = 70\n",
    "complete_dataset.loc[mask, 'meteo_x_vect'] = 0\n",
    "complete_dataset.loc[mask, 'meteo_y_vect'] = 0\n",
    "complete_dataset.loc[mask, 'relative_humidity_2m'] = 60\n",
    "complete_dataset.loc[mask, 'dew_point_2m'] = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56023741-a068-45c7-ac75-b7a5351cbfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "392f0dfa-636c-4859-be79-2b0fb5c4450b",
   "metadata": {},
   "source": [
    "Generate or read base rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d4fe7-d269-4232-adee-a744201c2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate base rates (base year = 2014)\n",
    "# Only needs to be run once\n",
    "# Generate:\n",
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)\n",
    "\n",
    "# Read: \n",
    "base_rate_df = pd.read_csv(os.path.join(baseball_path, \"Base Rates.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871e553-13cc-4154-ac4a-3286123271b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes\n",
    "analysis_df_list = []\n",
    "# Loop over batter sides\n",
    "for batSide in ['L', 'R']:\n",
    "    print(batSide)\n",
    "    # Subset complete dataset\n",
    "    complete_dataset_side = complete_dataset[complete_dataset['batSide'] == batSide]\n",
    "    # Calculate game averages (average rates within a particular games)\n",
    "    game_average_df = game_averages(complete_dataset_side)\n",
    "    # # Calculate player averages (average rates of all players coming into the game) (deprecated? - player level is in complete_dataset, so it's unnecessary)\n",
    "    # player_average_df = player_averages(complete_dataset_side)\n",
    "    # Calculate league averages (average rates of all PAs over last n days coming into the day)\n",
    "    league_average_df = league_average(complete_dataset_side, days=30)\n",
    "    # Average rates at park over last n games (both teams)\n",
    "    park_average_df = rolling_averages(game_average_df, 243, 'venue_id')\n",
    "    # Average rates at away games over last n games (both teams)\n",
    "    team_average_df = rolling_averages(game_average_df, 243, 'away_name')\n",
    "    # Park factors\n",
    "    park_factor_df = create_park_factors(park_average_df, team_average_df)\n",
    "    # Create dataframe that can be used to train and analyze data\n",
    "    analysis_df, venue_dummies = create_analysis_df(complete_dataset_side, league_average_df, park_factor_df)\n",
    "    analysis_df_list.append(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc5217-c26b-4dc4-8d03-955ccc7a06ae",
   "metadata": {},
   "source": [
    "Extract Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762f4b5-a727-42f4-bcd9-7647e21bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = analysis_df_list[0].copy()\n",
    "r_analysis_df = analysis_df_list[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2af25-3985-4583-b8bc-a5b4c78f90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del analysis_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fa17f-219c-47e7-bc8e-f2957868b864",
   "metadata": {},
   "source": [
    "### Park Latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4125b4-4db9-41d4-8315-65a140ddbecb",
   "metadata": {},
   "source": [
    "This contains the latest data available at each park, used to create WFX <br>\n",
    "Note: We can't just use multiplier dataset for this because it won't contain data at the end of the last game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42ae99-af38-43e5-9bb3-b0a7b32ee69b",
   "metadata": {},
   "source": [
    "Columns to Keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1195af-3f58-432f-95b4-36bea7147a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_latest_columns = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + venue_dummies + [col for col in l_analysis_df.columns if col.endswith(\"_pfx\")] + [col for col in l_analysis_df.columns if col.endswith(\"_lg\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ef211-a41b-4801-8bf9-12fc610c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df['venue_id'] = l_analysis_df['venue_id'].astype(int)\n",
    "r_analysis_df['venue_id'] = r_analysis_df['venue_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c39efb-3834-48d2-85b2-70f7de44498b",
   "metadata": {},
   "source": [
    "Write Park's Last Values to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147817-6116-4098-8e3f-b46bc39ef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[park_latest_columns].sort_values('game_date').drop_duplicates('venue_id', keep='last').to_csv(os.path.join(baseball_path, \"Park Latest - LHB.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7fdbb-d18c-47e7-a955-d7343aabe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[park_latest_columns].sort_values('game_date').drop_duplicates('venue_id', keep='last').to_csv(os.path.join(baseball_path, \"Park Latest - RHB.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f28a2-7a68-491b-bf45-9299539bf797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171df5e2-2793-4c9a-91a3-e1e011789822",
   "metadata": {},
   "source": [
    "### Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fe782-a3d4-4f2f-b87e-d593313dcb82",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e276c0f-9ee3-48aa-814a-853d948bbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx_list = [col for col in l_analysis_df.columns if col.endswith(\"pfx\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae33f6-ae8a-4f9f-ac72-e1d48d8d2b66",
   "metadata": {},
   "source": [
    "Previous game_date at venue_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94cd0-6d31-4ca3-a0ca-43668a6f76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[pfx_list] = l_analysis_df.groupby(\"venue_id\")[pfx_list].shift(1)\n",
    "l_analysis_df[pfx_list] = l_analysis_df.groupby([\"venue_id\", \"game_date\"])[pfx_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53392d4-d415-4705-9dd4-1ecf68e0468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[pfx_list] = r_analysis_df.groupby(\"venue_id\")[pfx_list].shift(1)\n",
    "r_analysis_df[pfx_list] = r_analysis_df.groupby([\"venue_id\", \"game_date\"])[pfx_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa86df-e891-4615-b351-caed247ded1e",
   "metadata": {},
   "source": [
    "##### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39378c-cff8-4f6e-af9c-4b1b65f03332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_list = [col for col in l_analysis_df.columns if col.endswith(\"lg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bab02-30b5-4a5e-96cc-e3383936f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = l_analysis_df.sort_values('game_date', ascending=True)\n",
    "l_analysis_df[lg_list] = l_analysis_df[lg_list].shift(1)\n",
    "l_analysis_df[lg_list] = l_analysis_df.groupby(\"game_date\")[lg_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4f8da-bb64-4e08-aa13-03519306a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df = r_analysis_df.sort_values('game_date', ascending=True)\n",
    "r_analysis_df[lg_list] = r_analysis_df[lg_list].shift(1)\n",
    "r_analysis_df[lg_list] = r_analysis_df.groupby([\"game_date\"])[lg_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a7286-6a71-4dd5-ac2c-78943c913062",
   "metadata": {},
   "source": [
    "##### Batter Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53543a-f923-4318-a214-3809a06f0610",
   "metadata": {},
   "source": [
    "Note: You need to shift by batter and pitchHand to get the batter's last PA against that hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a3032-0731-42fe-bfcc-58d991592d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_long_list = [col for col in l_analysis_df.columns if col.endswith(\"b_long\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddcdd8-2d8b-4f28-9a5e-a56fbc5cf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[b_long_list] = l_analysis_df.groupby(['batter', 'pitchHand'])[b_long_list].shift(1)\n",
    "r_analysis_df[b_long_list] = r_analysis_df.groupby(['batter', 'pitchHand'])[b_long_list].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef67cd-558d-4049-a7cd-2ce8950580be",
   "metadata": {},
   "source": [
    "##### Pitcher Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fd134-8e9f-4b49-bac8-ab9508ffec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_long_list = [col for col in l_analysis_df.columns if col.endswith(\"p_long\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652851a9-3d52-455b-8e7e-64f318993f44",
   "metadata": {},
   "source": [
    "Note: You don't to shift by batSide to get the pitcher's last PA against that hand because all hands are the same, but why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef64c1a-5f22-40bb-810e-78f442ab8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[p_long_list] = l_analysis_df.groupby(['pitcher', 'batSide'])[p_long_list].shift(1)\n",
    "r_analysis_df[p_long_list] = r_analysis_df.groupby(['pitcher', 'batSide'])[p_long_list].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c553dc3-529b-4cac-8ddf-8554977d5dc0",
   "metadata": {},
   "source": [
    "### Select Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11a93b-f63d-46ec-ac9f-2f981aab4e43",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f4fee-17d4-4c37-bdc2-5a4d873bec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_weather_variables = ['x_vect', 'y_vect', 'temperature'] # drop weather\n",
    "meteo_duplicates_variables = ['meteo_x_vect', 'meteo_y_vect', 'temperature_2m']\n",
    "meteo_weather_variables = ['relative_humidity_2m', 'dew_point_2m', 'surface_pressure']\n",
    "mlb_park_variables = ['fieldInfo.leftLine', 'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter', 'fieldInfo.rightCenter', 'location.elevation'] # drop roof type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c643a5-cf37-4657-9458-24a6e833c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = []\n",
    "# Loop over events\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    training_list += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb5805-7379-4427-8260-8061e58d218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list = []\n",
    "# Loop over events\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    testing_list += [f'{event}_lg', f'{event}_lg', f'{event}_pfx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be644da-a3bd-4d8c-9977-ec70030eaab6",
   "metadata": {},
   "source": [
    "Generic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddedc41-b8b8-414e-9f1a-c5586b74ec75",
   "metadata": {},
   "source": [
    "This uses information about the park and weather to predict, but doesn't train on park-specific dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9e240-f9f4-441d-816c-db61ada04f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_training_input_list = mlb_weather_variables + meteo_weather_variables + mlb_park_variables + training_list\n",
    "generic_testing_input_list = mlb_weather_variables + meteo_weather_variables + mlb_park_variables + testing_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f383e-100e-4cb5-bce2-78f93f75c145",
   "metadata": {},
   "source": [
    "Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfb303-6b61-4cd1-a44c-6cd628b1b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_training_input_list = mlb_weather_variables + meteo_weather_variables + mlb_park_variables + training_list + venue_dummies\n",
    "specific_testing_input_list = mlb_weather_variables + meteo_weather_variables + mlb_park_variables + testing_list + venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1859443-37e2-43c4-9aae-3d6b19419ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_analysis_df['venue_id'] = l_analysis_df['venue_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7de84d-c434-47b0-8109-f0088769228a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779a5290-d1a6-4579-b899-d7eeb70f964b",
   "metadata": {},
   "source": [
    "### Select Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12432be5-8491-4a58-9917-c2875f5dab0e",
   "metadata": {},
   "source": [
    "Remove Infinite Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da2197-c365-4a6d-8d48-1a1d60a0828e",
   "metadata": {},
   "source": [
    "Note: After setting them to missing historically, I'm going to set them to 0 now to ensure new parks aren't excluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb826d-2b92-42b8-b868-7bc823c89545",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[specific_training_input_list] = l_analysis_df[specific_training_input_list].replace([np.inf, -np.inf], 0)\n",
    "r_analysis_df[specific_training_input_list] = r_analysis_df[specific_training_input_list].replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0608d30-a1e1-4963-a1ca-c990d07309ad",
   "metadata": {},
   "source": [
    "Drop if Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0beb1-58cc-4bc6-9202-ff3fe6ddc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = l_analysis_df.dropna()\n",
    "r_analysis_df = r_analysis_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c0368-500b-4417-ae89-49fe7fff7bcc",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ff4ac-8215-4696-82d3-199d2b2ea431",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c428e-b2d4-406f-9541-239a1662f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "l_analysis_df['split'] = np.random.choice([0, 0, 1], size=len(l_analysis_df))\n",
    "r_analysis_df['split'] = np.random.choice([0, 0, 1], size=len(r_analysis_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803d3a0-a00f-4f57-90bc-125654f9e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(specific_training_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbafce-ac12-44c7-9359-fff185d3bfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6816d0-8be4-4c65-918e-034fac32ba83",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cc398-f39c-4a58-b1a9-59fbc31f9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(df, training_list, testing_list, filename, layers, activation, max_iter, alpha, learning_rate, batch_size, random_state):\n",
    "#     # Define file name\n",
    "#     model_filename = f\"{filename}_{random_state}.pkl\"\n",
    "#     model_path_full = os.path.join(model_path, \"M01. Park and Weather Factors\", model_filename)\n",
    "\n",
    "#     print(model_filename)\n",
    "    \n",
    "#     # Model Parameters\n",
    "#     model = MLPClassifier(\n",
    "#         hidden_layer_sizes=layers, \n",
    "#         activation=activation, \n",
    "#         verbose=False, \n",
    "#         alpha=alpha,\n",
    "#         learning_rate_init=learning_rate, \n",
    "#         early_stopping=True, \n",
    "#         random_state=random_state,  # Modify random_state for each model\n",
    "#         max_iter=max_iter, \n",
    "#         batch_size=batch_size\n",
    "#     )\n",
    "\n",
    "#     # Train model\n",
    "#     model.fit(df[training_list], df[['eventsModel']].values.ravel())\n",
    "\n",
    "#     # Save model\n",
    "#     pickle.dump(model, open(model_path_full, 'wb'))\n",
    "\n",
    "#     # Predict using the trained model (overwriting same columns each iteration)\n",
    "#     output_list = list(model.classes_)\n",
    "#     output_list_pred = [f\"{x}_pred\" for x in output_list]  \n",
    "\n",
    "#     df[output_list_pred] = model.predict_proba(df[testing_list].values)\n",
    "\n",
    "#     # Evaluate\n",
    "#     # Get dummies\n",
    "#     for event in events_list:\n",
    "#         df[event] = (df['eventsModel'] == event).astype(int)\n",
    "\n",
    "#     num_quantiles = 20  # Adjust if needed\n",
    "\n",
    "#     # Create a figure with 3x4 subplots\n",
    "#     fig, axes = plt.subplots(3, 4, figsize=(12, 9))  \n",
    "#     axes = axes.flat  # Flatten to easily iterate\n",
    "\n",
    "#     for j, event in enumerate(events_list):  \n",
    "#         if j >= len(axes):  # Safety check if more events than subplots\n",
    "#             break  \n",
    "\n",
    "#         pred_col = f\"{event}_pred\"  \n",
    "#         quantile_col = f\"{event}_quantile\"\n",
    "\n",
    "#         # Create quantiles\n",
    "#         df[quantile_col] = pd.qcut(\n",
    "#             df[pred_col], num_quantiles, labels=False, duplicates='drop'\n",
    "#         )\n",
    "\n",
    "#         # Group by quantiles\n",
    "#         plot_data = df.groupby(quantile_col).agg(\n",
    "#             avg_pred=(pred_col, \"mean\"),  # X-axis\n",
    "#             avg_event=(event, \"mean\")     # Y-axis\n",
    "#         ).reset_index()\n",
    "\n",
    "#         ax = axes[j]\n",
    "\n",
    "#         # Plot data\n",
    "#         sns.lineplot(x=plot_data[\"avg_pred\"], y=plot_data[\"avg_event\"], marker=\"o\", ax=ax)\n",
    "\n",
    "#         # 45-degree reference line\n",
    "#         min_val, max_val = plot_data[\"avg_pred\"].min(), plot_data[\"avg_pred\"].max()\n",
    "#         ax.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "#         ax.set_xlabel(\"Avg Pred\")\n",
    "#         ax.set_ylabel(\"Avg Event\")\n",
    "#         ax.set_title(event)  \n",
    "\n",
    "#     # Adjust layout for better spacing\n",
    "#     plt.tight_layout() \n",
    "#     plt.show()\n",
    "\n",
    "#     # Calculate WFX\n",
    "#     for event in events_list:\n",
    "#         df[f'{event}_wfx_l'] = df[f'{event}_pred'] / base_rate_df[event][0]\n",
    "    \n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eb5cc-e72a-45d3-b336-a8ba85b7fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, training_list, testing_list, filename, layers, activation, max_iter, alpha, learning_rate, batch_size, random_state):\n",
    "    model_filename = f\"{filename}_{random_state}.pkl\"\n",
    "    model_path_full = os.path.join(model_path, \"M01. Park and Weather Factors\", model_filename)\n",
    "\n",
    "    print(model_filename)\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=layers, \n",
    "        activation=activation, \n",
    "        verbose=False, \n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate, \n",
    "        early_stopping=True, \n",
    "        random_state=random_state,  \n",
    "        max_iter=max_iter, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model.fit(df[training_list], df[['eventsModel']].values.ravel())\n",
    "    pickle.dump(model, open(model_path_full, 'wb'))\n",
    "\n",
    "    output_list = list(model.classes_)\n",
    "    output_list_pred = [f\"{x}_pred\" for x in output_list]  \n",
    "    df[output_list_pred] = model.predict_proba(df[testing_list].values)\n",
    "\n",
    "    for event in events_list:\n",
    "        df[event] = (df['eventsModel'] == event).astype(int)\n",
    "\n",
    "    num_quantiles = 20\n",
    "    quantile_results = {}\n",
    "\n",
    "    for event in events_list:  \n",
    "        pred_col = f\"{event}_pred\"  \n",
    "        quantile_col = f\"{event}_quantile\"\n",
    "\n",
    "        df[quantile_col] = pd.qcut(\n",
    "            df[pred_col], num_quantiles, labels=False, duplicates='drop'\n",
    "        )\n",
    "\n",
    "        plot_data = df.groupby(quantile_col).agg(\n",
    "            avg_pred=(pred_col, \"mean\"),\n",
    "            avg_event=(event, \"mean\")\n",
    "        ).reset_index()\n",
    "\n",
    "        quantile_results[event] = plot_data\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "    axes = axes.flat\n",
    "\n",
    "    for j, event in enumerate(events_list):\n",
    "        if j >= len(axes):\n",
    "            break  \n",
    "\n",
    "        ax = axes[j]\n",
    "        plot_data = quantile_results[event]\n",
    "\n",
    "        sns.lineplot(x=plot_data[\"avg_pred\"], y=plot_data[\"avg_event\"], marker=\"o\", ax=ax)\n",
    "\n",
    "        min_val = min(plot_data[\"avg_pred\"].min(), plot_data[\"avg_event\"].min())\n",
    "        max_val = max(plot_data[\"avg_pred\"].max(), plot_data[\"avg_event\"].max())\n",
    "\n",
    "        # Add small padding to prevent dots on edges\n",
    "        padding = (max_val - min_val) * 0.05\n",
    "        min_val -= padding\n",
    "        max_val += padding\n",
    "\n",
    "        ax.set_xlim(min_val, max_val)\n",
    "        ax.set_ylim(min_val, max_val)\n",
    "\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "        # Force square aspect ratio\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        # Let matplotlib auto-handle tick locations (nice round numbers)\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "        ax.set_xlabel(\"Avg Pred\")\n",
    "        ax.set_ylabel(\"Avg Event\")\n",
    "        ax.set_title(event)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for event in events_list:\n",
    "        df[f'{event}_wfx_l'] = df[f'{event}_pred'] / base_rate_df[event][0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb3f72-645a-40b1-b7e8-1e4bcce9af45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f221196-9425-464b-8c59-f5204d4d6bf3",
   "metadata": {},
   "source": [
    "### Specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acec17a-7d0b-4082-a34c-a0572f6cf759",
   "metadata": {},
   "source": [
    "### WFX - L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef5450-90b8-4cbf-b6fb-e52dcc03b442",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898c9d5-0383-43c8-bb57-77daad7c5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (89,89,89,89,89,89,89)\n",
    "# layers = (10,)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "# batch_size=8\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62dee65-e184-4aa4-b632-f8b5153ae850",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef880a-c02f-4da7-bac5-57ddc2dd172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    for i in range(num_models):\n",
    "        l_analysis_df = train_model(l_analysis_df, specific_training_input_list, specific_testing_input_list, \"predict_wfx_l\", layers, activation, max_iter, alpha, learning_rate, batch_size, random_state=random_state+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3c2c6-9925-4cb8-b91e-717c0af3a7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67517943-fabb-4220-98ab-189a96bb7f0c",
   "metadata": {},
   "source": [
    "### WFX - R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10d35b-f53b-40de-bc42-d8e90e10a33b",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a19eb9-d247-43cc-8a10-7ae1e6612641",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (89,89,89,89,89,89,89)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe30b2-585e-40dd-b105-af2a39ad856b",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9261a-c7cd-4f5d-9d77-f1faf2c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    for i in range(num_models):\n",
    "        r_analysis_df = train_model(r_analysis_df, specific_training_input_list, specific_testing_input_list, \"predict_wfx_r\", layers, activation, max_iter, alpha, learning_rate, batch_size, random_state=random_state+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b753ff5-b5f0-4f69-b9cb-ff775a8abb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55251ecc-cdc2-443c-8bea-0c2972cf8fa0",
   "metadata": {},
   "source": [
    "### Generic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dc4c8-bec2-48f3-bc92-2a932cd0e8ae",
   "metadata": {},
   "source": [
    "### WFX - L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb023ae5-6170-45f7-9df0-71a184c3c25b",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81648b35-3c30-479a-9b6f-223d33690846",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (46,46,46)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "batch_size = 8\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa0b64-f7ce-43a3-9253-fc012cb19207",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1a1c6-7c23-4acd-9f63-52b995ea39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    for i in range(num_models):\n",
    "        l_analysis_df = train_model(l_analysis_df, generic_training_input_list, generic_testing_input_list, \"predict_generic_wfx_l\", layers, activation, max_iter, alpha, learning_rate, batch_size, random_state=random_state+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff34a09-13c5-48ca-b488-f8f2cdfdc79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e89f226-bbf5-4779-9c3f-cf25e987720d",
   "metadata": {},
   "source": [
    "### WFX - R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165aa38-69e0-4fd8-a411-eae4c7f3cb7f",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e0d7a-4181-4b7b-95dc-cc70ce5d84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (46,46,46,)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d25cf-6fe0-410e-9cc8-c4c8cfc1607c",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11367181-5954-4177-bcea-e1ff7d724dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    for i in range(num_models):\n",
    "        r_analysis_df = train_model(r_analysis_df, generic_training_input_list, generic_testing_input_list, \"predict_generic_wfx_r\", layers, activation, max_iter, alpha, learning_rate, batch_size, random_state=random_state+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c824ea-a0ac-4b6a-965e-c44a8e8f9fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7be99245-52e8-47d2-b93a-c7fef82def5a",
   "metadata": {},
   "source": [
    "### Model Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b568608-f0ba-423a-996e-36f17085d297",
   "metadata": {},
   "source": [
    "##### 1. Select Models in U5. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986f139-a608-428c-baf7-90ffa073b31a",
   "metadata": {},
   "source": [
    "Update notebook, if WFX models have changed, and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180f261-4d33-4691-b364-5a766eff8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d86d77-25bc-4d42-99dd-62147c769967",
   "metadata": {},
   "source": [
    "##### 2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ffb8e-65b8-4d31-bbba-7ec18149a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = list(predict_wfx_l.classes_)\n",
    "output_list_pred = [f\"{x}_pred\" for x in output_list]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c123d5-2953-499f-86f5-5cc57ce885a0",
   "metadata": {},
   "source": [
    "Specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f31f54-e066-43bb-9ae5-3d571102cd5a",
   "metadata": {},
   "source": [
    "WFX - L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84236c-4ffc-46db-8064-66cf2761ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[output_list_pred] = predict_wfx_l.predict_proba(l_analysis_df[specific_testing_input_list].values)\n",
    "# Calculate WFX\n",
    "for event in events_list:\n",
    "    l_analysis_df[f'{event}_wfx_l'] = l_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8941b-43f5-40e4-b740-e4d614198c59",
   "metadata": {},
   "source": [
    "WFX - R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3caca5-8751-4f86-b3d8-b812730230bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[output_list_pred] = predict_wfx_r.predict_proba(r_analysis_df[specific_testing_input_list].values)\n",
    "# Calculate WFX\n",
    "for event in events_list:\n",
    "    r_analysis_df[f'{event}_wfx_r'] = r_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ba844-c9bf-4cc4-899c-f1b216558edc",
   "metadata": {},
   "source": [
    "Generic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63baa357-60ae-44b1-9ca4-bc7fda4dbfbc",
   "metadata": {},
   "source": [
    "WFX - L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eaf2f7-6b82-44a5-86ac-d0a20ad14358",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[output_list_pred] = predict_generic_wfx_l.predict_proba(l_analysis_df[generic_testing_input_list].values)\n",
    "# Calculate WFX\n",
    "for event in events_list:\n",
    "    l_analysis_df[f'{event}_generic_wfx_l'] = l_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03113af-cc6b-4986-a482-855398dd5ed7",
   "metadata": {},
   "source": [
    "WFX - R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7c71f-4b62-4379-a0c7-2c201e947c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[output_list_pred] = predict_generic_wfx_r.predict_proba(r_analysis_df[generic_testing_input_list].values)\n",
    "# Calculate WFX\n",
    "for event in events_list:\n",
    "    r_analysis_df[f'{event}_generic_wfx_r'] = r_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb576a1-71e5-4281-a4e3-78c7d0ba004c",
   "metadata": {},
   "source": [
    "Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5e999-22e3-40f9-b01a-493e7badae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and select relevant columns\n",
    "scatter_df = l_analysis_df.drop_duplicates('gamePk')[['hr_wfx_l', 'hr_generic_wfx_l']]\n",
    "\n",
    "# Determine axis limits\n",
    "min_val = min(scatter_df.min())\n",
    "max_val = max(scatter_df.max())\n",
    "\n",
    "# Set figure size to be square\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Create scatter plot with regression line\n",
    "sns.regplot(data=scatter_df, x='hr_wfx_l', y='hr_generic_wfx_l', scatter=True, line_kws={\"color\": \"red\"})\n",
    "\n",
    "# Set equal axis limits\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Force aspect ratio to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('HR WFX')\n",
    "plt.ylabel('HR Generic WFX')\n",
    "plt.title('Regression of HR WFX vs HR Generic WFX')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182cbe5-0851-4dfd-bca9-e56fdeb225fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and select relevant columns\n",
    "scatter_df = r_analysis_df.drop_duplicates('gamePk')[['hr_wfx_r', 'hr_generic_wfx_r']]\n",
    "\n",
    "# Determine axis limits\n",
    "min_val = min(scatter_df.min())\n",
    "max_val = max(scatter_df.max())\n",
    "\n",
    "# Set figure size to be square\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Create scatter plot with regression line\n",
    "sns.regplot(data=scatter_df, x='hr_wfx_r', y='hr_generic_wfx_r', scatter=True, line_kws={\"color\": \"red\"})\n",
    "\n",
    "# Set equal axis limits\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Force aspect ratio to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('HR WFX')\n",
    "plt.ylabel('HR Generic WFX')\n",
    "plt.title('Regression of HR WFX vs HR Generic WFX')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98795b2b-1d64-4dc9-9f4b-50c29ce066ec",
   "metadata": {},
   "source": [
    "##### 3. Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f580b-291f-470b-a385-02c18d4445a0",
   "metadata": {},
   "source": [
    "Convert datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5a07a-e22c-474c-bd59-ef8e0fb5a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df['venue_id'] = l_analysis_df['venue_id'].astype(str)\n",
    "r_analysis_df['venue_id'] = r_analysis_df['venue_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d65ae9-6ae0-48a3-8797-56a8e4439276",
   "metadata": {},
   "source": [
    "Select columns to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b17e2c-a811-44e6-9a4d-0d16d12b894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_columns = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'meteo_x_vect', 'meteo_y_vect', 'temperature_2m', 'x_vect', 'y_vect', 'temperature', 'weather']\n",
    "wfx_l_columns = [col for col in l_analysis_df.columns if col.endswith(\"_wfx_l\")]\n",
    "wfx_r_columns = [col for col in r_analysis_df.columns if col.endswith(\"_wfx_r\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7673f-21ff-43d4-b31f-074854f532a9",
   "metadata": {},
   "source": [
    "Merge on common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c06ae9-860d-47fb-91d5-b538501f0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset = pd.merge(l_analysis_df.drop_duplicates('gamePk', keep='last')[descriptive_columns + wfx_l_columns], \n",
    "                              r_analysis_df.drop_duplicates('gamePk', keep='last')[descriptive_columns + wfx_r_columns], on=descriptive_columns, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d786f7-8386-4e13-9cf1-3d54b42fad7f",
   "metadata": {},
   "source": [
    "Create date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c4db5-06f3-45aa-934e-a2cf7a40c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset['date'] = multiplier_dataset['game_date'].str.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50912570-c513-4451-b66e-bbcb197f15c2",
   "metadata": {},
   "source": [
    "##### 4. Write Multiplier Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe9daa-ddb5-4699-8117-79f63e6bf332",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317b1c1-fd77-4155-ac82-11b70dc86d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset.to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16623d6f-b732-4b9d-a483-44ded2b09403",
   "metadata": {},
   "source": [
    "##### 5. Generate Historic Park and Weather Factors Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bab883-1802-4b31-9be1-95d10678b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to keep\n",
    "keep_columns = ['gamePk', 'game_date', 'date', 'venue_id', 'away_name', 'home_name', 'meteo_x_vect', 'meteo_y_vect', 'temperature_2m', 'x_vect', 'y_vect', 'temperature', 'weather'] + [col for col in multiplier_dataset.columns if \"_wfx\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4175ca8-9421-4ca8-94b9-fb45bcbac3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset.sort_values('date', inplace=True)\n",
    "for date in multiplier_dataset[pd.to_datetime(multiplier_dataset['game_date'], format='mixed').dt.year >= 2022]['date'].unique():\n",
    "    # print(date)\n",
    "    if date > \"20220101\":\n",
    "        # Subset by date\n",
    "        daily_weather_df = multiplier_dataset[multiplier_dataset['date'] == date][keep_columns]\n",
    "\n",
    "        # Write to CSV\n",
    "        daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"3. Park and Weather Factors\", f\"Park and Weather Factors {date}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d6d53-a4f4-4a82-bffb-47f86927bdf1",
   "metadata": {},
   "source": [
    "Note: new Park x Weather Factor models will change the historic Multiplier Dataset data. This requires rerunning:\n",
    "- B01. Matchups\n",
    "- M02. Stat Imputations\n",
    "- M03. Plate Appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369898b-b294-4b60-ab88-802030eb9363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3394e2-300e-42e4-a2c2-90b4676d4840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-base)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
