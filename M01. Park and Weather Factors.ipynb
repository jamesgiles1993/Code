{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# M01. Park and Weather Factors\n",
    "- This calculated Park x Weather Factors\n",
    "- Type: Model\n",
    "- Run Frequency: Daily\n",
    "- Sources:\n",
    "    - MLB API\n",
    "    - Steamer\n",
    "- Created: 12/10/2024\n",
    "- Updated: 12/17/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    print(\"Running imports...\")\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\"\n",
    "    print(\"Imports in.\")\n",
    "else:\n",
    "    print(\"Imports already in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce0f6-2eb6-4ecd-841b-2e99fc13c281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02405859-9235-419d-bb06-52eb83ede249",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b979a91-7a59-4d4b-b126-73771a2536ca",
   "metadata": {},
   "source": [
    "Create Latest PA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208611b0-2531-45b0-b7e7-ae2db5db9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "complete_dataset_unadjusted_latest = create_pa_inputs(None, start_year=2022, end_year=2025, short=50, long=300, adjust=False)\n",
    "complete_dataset_unadjusted_latest = complete_dataset_unadjusted_latest[complete_dataset_unadjusted_latest['year'].astype(int) >= 2025].reset_index()\n",
    "complete_dataset_unadjusted_latest.to_csv(os.path.join(baseball_path, \"Complete Dataset - Unadjusted Latest.csv\"), index=False)\n",
    "# complete_dataset_unadjusted_latest = pd.read_csv(os.path.join(baseball_path, \"Complete Dataset - Unadjusted Latest.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd07d4-85a6-429d-8f9b-99d3e4f056de",
   "metadata": {},
   "source": [
    "# Read in Earlier Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb9297-1697-4572-b7f9-fa3730ea596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_unadjusted_earlier = pd.read_csv(os.path.join(baseball_path, \"Complete Dataset - Unadjusted through 2024.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c3481-118b-483d-a04e-a482e3336bbf",
   "metadata": {},
   "source": [
    "Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28199060-a760-4bc1-8721-0ffe09a554b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.concat([complete_dataset_unadjusted_earlier, complete_dataset_unadjusted_latest], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa893181-454e-4e95-93b4-02aa74ee77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del complete_dataset_unadjusted_latest, complete_dataset_unadjusted_earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799477f-8575-4874-8049-7c49fb663613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1b42d3-21c5-4cda-92ea-748580d3884b",
   "metadata": {},
   "source": [
    "### Base Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13121744-f9d8-4d98-8797-267f410948e6",
   "metadata": {},
   "source": [
    "Calculate average stats in a given base year <br>\n",
    "Note: This only has to be run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8511a-581b-47b4-8345-36b219d3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rates(df, base_year=2014):\n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Select period of interest\n",
    "    df = df[df['game_date'].dt.year == base_year]\n",
    "\n",
    "    # Calculate averages over period of interest\n",
    "    base_rate_df = pd.DataFrame(df[events_list].mean()).T\n",
    "\n",
    "    \n",
    "    return base_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a930b6-360b-42d0-99c2-8000eae19653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b183ab-f43b-4c9c-a209-b2ee24c3f190",
   "metadata": {},
   "source": [
    "### Game Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed47d7-2bce-4d17-ac4e-eb608eda8884",
   "metadata": {},
   "source": [
    "Average rates within the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf7da9-72c3-4c3c-985e-c57db1a405d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(df):    \n",
    "    # Calculate averages by game\n",
    "    game_avgs = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'])[events_list].mean().reset_index()\n",
    "\n",
    "    # Add the 'pas' column to count the number of observations in each group\n",
    "    game_avgs['pas'] = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']).size().values\n",
    "\n",
    "    # Sort by date\n",
    "    game_avgs.sort_values(['game_date'], ascending=True, inplace=True)\n",
    "\n",
    "    \n",
    "    return game_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabeca8c-8017-45b2-8942-5d90250eea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_average_df = game_averages(complete_dataset)\n",
    "# game_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a92c3e-8bbf-4687-9079-68d7346bede0",
   "metadata": {},
   "source": [
    "### Player Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82edc50-a380-4caf-87ab-650d932a1e4a",
   "metadata": {},
   "source": [
    "Average stats of all the players in the game, coming into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc16b3-2c4b-4f89-b1e5-32f0356f818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_averages(df):\n",
    "    # Stats to average\n",
    "    batter_inputs_short = [f\"{event}_b_long\" for event in events_list]\n",
    "    pitcher_inputs_short = [f\"{event}_p_long\" for event in events_list]\n",
    "\n",
    "    # Apply stats from last at bat to entire game\n",
    "    df[batter_inputs_short] = df.groupby(['gamePk', 'batter'])[batter_inputs_short].transform('last')\n",
    "    df[pitcher_inputs_short] = df.groupby(['gamePk', 'pitcher'])[pitcher_inputs_short].transform('last')\n",
    "    \n",
    "    # Calculate player averages by game\n",
    "    batter_avgs = df.groupby(['gamePk'])[batter_inputs_short].mean().reset_index()\n",
    "    pitcher_avgs = df.groupby(['gamePk'])[pitcher_inputs_short].mean().reset_index()\n",
    "\n",
    "    # Concatenate together\n",
    "    player_avgs = pd.concat([batter_avgs, pitcher_avgs.drop(columns=['gamePk'])], axis=1)\n",
    "    \n",
    "    \n",
    "    return player_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f1d09-dc1a-4d72-80a1-2fc6f5c0e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_average_df = player_averages(complete_dataset)\n",
    "# player_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9a52f-86cd-4886-ac94-23bb9b111af7",
   "metadata": {},
   "source": [
    "### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617b005-f5ae-4267-bf08-551051f5cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def league_average(complete_dataset, days=30):\n",
    "    # Calculate daily sum of events\n",
    "    league_avg = complete_dataset.groupby('game_date')[events_list].sum().reset_index()\n",
    "    # Calculate total events\n",
    "    league_avg['pas'] = league_avg[events_list].sum(axis=1)\n",
    "    \n",
    "    # Use rolling sum including the current row\n",
    "    for event in events_list + ['pas']:\n",
    "        league_avg[f'{event}_sum'] = league_avg[event].rolling(window=days, min_periods=1).sum()\n",
    "\n",
    "    # Calculate average\n",
    "    for event in events_list:\n",
    "        league_avg[f'{event}_lg'] = league_avg[f'{event}_sum'] / league_avg['pas_sum']\n",
    "\n",
    "        \n",
    "    return league_avg[[\"game_date\"] + [col for col in league_avg if \"_lg\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbf862-3c02-4922-9b6f-0b22d6faf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# league_average_df = league_average(complete_dataset, 30)\n",
    "# league_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb155b-c601-4d64-a88e-c75dce2b7c7f",
   "metadata": {},
   "source": [
    "### Park Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8674102-da42-42e1-9610-d8cf2c36cb21",
   "metadata": {},
   "source": [
    "##### Rolling Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344bd9-99ba-4b5e-82bd-ff0a4f568eb1",
   "metadata": {},
   "source": [
    "Average of stats over last rolling_window games - excluding game of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31d871-22da-4911-a98b-c3ec81d8d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_averages(game_avgs, rolling_window, column):\n",
    "    # Sort by group column and date\n",
    "    rolling_avgs = game_avgs.sort_values([column, 'game_date']).copy()\n",
    "\n",
    "    # Compute rolling sum for `pas`\n",
    "    rolling_avgs['pas_rolling'] = rolling_avgs.groupby(column)['pas'].transform(\n",
    "        lambda x: x.rolling(window=rolling_window, min_periods=1, closed=\"right\").sum()\n",
    "    )\n",
    "\n",
    "    # Define function for rolling weighted average\n",
    "    def weighted_avg(group):\n",
    "        return (\n",
    "            group[events_list]\n",
    "            .rolling(window=rolling_window, min_periods=1, closed=\"right\")\n",
    "            .apply(lambda x: (x * group.loc[x.index, 'pas']).sum() / group.loc[x.index, 'pas'].sum(), raw=False)\n",
    "        )\n",
    "\n",
    "    # Apply rolling weighted average by the given column\n",
    "    rolling_avgs[events_list] = rolling_avgs.groupby(column, group_keys=False).apply(weighted_avg)\n",
    "\n",
    "    \n",
    "    return rolling_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55a7ca-d0e4-4a46-9a46-69395cd4f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to account for small sample parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3941156-6fff-405a-ae97-fdb0d6605d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_average_df = rolling_averages(game_average_df, 243, 'venue_id')\n",
    "# park_average_df = park_average_df[['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + events_list + ['pas_rolling']]\n",
    "# park_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b74003-2b91-4d48-816c-3cc638496986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_average_df = rolling_averages(game_average_df, 243, 'away_name')\n",
    "# team_average_df = team_average_df[['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + events_list + ['pas_rolling']]\n",
    "# team_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aa6c4-b995-464b-9450-0d7581c12282",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24c5e-2448-403b-b822-a8a282cf9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_park_factors(park_avgs, team_avgs):\n",
    "    # Sort by game_date\n",
    "    park_avgs = park_avgs.sort_values('game_date')\n",
    "    team_avgs = team_avgs.sort_values('game_date')\n",
    "\n",
    "    # Create uniform team_name variable equal to name of interest\n",
    "    park_avgs['team_name'] = park_avgs['home_name'].copy()\n",
    "    team_avgs['team_name'] = team_avgs['away_name'].copy()\n",
    "\n",
    "    # Set to datetime\n",
    "    park_avgs['game_date'] = pd.to_datetime(park_avgs['game_date'])\n",
    "    team_avgs['game_date'] = pd.to_datetime(team_avgs['game_date'])\n",
    "    \n",
    "    # Perform merge_asof\n",
    "    park_factor_df = pd.merge_asof(park_avgs, team_avgs, left_on='game_date', right_on='game_date', by='team_name', direction='backward', suffixes=('_park', '_team'))\n",
    "\n",
    "    # Calculate park factors\n",
    "    for stat in events_list:\n",
    "        park_factor_df[f'{stat}_pfx'] = park_factor_df[f'{stat}_park'] / park_factor_df[f'{stat}_team'] \n",
    "        \n",
    "    park_factor_df.rename(columns={'gamePk_park': 'gamePk'}, inplace=True)\n",
    "    keep_columns = ['gamePk'] + [col for col in park_factor_df.columns if col.endswith('pfx')]\n",
    "\n",
    "    \n",
    "    return park_factor_df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8cfd-1241-481f-8a33-7788417e22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df = create_park_factors(park_average_df, team_average_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf439c-b77e-4230-aee2-8a4397b7b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0f210-8aa6-4479-8c1b-f6cc194edac0",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd52ba2-27f4-4260-97ad-a2e7a2892d29",
   "metadata": {},
   "source": [
    "Merge together game averages, player averages, and park factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba144343-39bf-499c-b1d9-2a6a68bf46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_df(complete_dataset, league_average_df, park_factor_df):\n",
    "    # Merge on league averages\n",
    "    analysis_df = pd.merge(complete_dataset, league_average_df, on=['game_date'], how='inner')\n",
    "    # Merge on park factors\n",
    "    analysis_df = pd.merge(analysis_df, park_factor_df, on='gamePk', how='inner')\n",
    "   \n",
    "    \n",
    "    # Extract dummies from venues\n",
    "    venue_dummy_df = pd.get_dummies(analysis_df['venue_id'].astype(int), prefix='venue')\n",
    "    # Extract dummy column names\n",
    "    venue_dummies = list(venue_dummy_df.columns)\n",
    "    \n",
    "    # Add in dummies\n",
    "    analysis_df = pd.concat([analysis_df, venue_dummy_df], axis=1)\n",
    "    \n",
    "    # Select variables to keep\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    # Select relevant variables and drop missings\n",
    "    analysis_df = analysis_df[[\"eventsModel\", 'gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'batter', 'pitcher', 'batSide', 'pitchHand'] + variables + [col for col in analysis_df if col.endswith(\"_lg\")]].dropna()\n",
    "    \n",
    "    # Remove cut\n",
    "    analysis_df = analysis_df[analysis_df['eventsModel'] != \"Cut\"]\n",
    "    \n",
    "    \n",
    "    return analysis_df, venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e6f67-06ac-43c8-9436-0175a0a197ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_df, venue_dummies = create_analysis_df(complete_dataset, league_average_df, park_factor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74231-a561-4350-8dfa-44defbaf432a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0792a0-b2a1-4764-8f7b-b10b6c3244ba",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c58b5-d2a5-49c3-b158-5d4d5b88e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset['temperature'] = complete_dataset.apply(lambda row: 70 if 'Roof' in row['weather'] or 'Dome' in row['weather'] else row['temperature'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f0dfa-636c-4859-be79-2b0fb5c4450b",
   "metadata": {},
   "source": [
    "Generate or read base rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d4fe7-d269-4232-adee-a744201c2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate base rates (base year = 2014)\n",
    "# Only needs to be run once\n",
    "# Generate:\n",
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)\n",
    "\n",
    "# Read: \n",
    "base_rate_df = pd.read_csv(os.path.join(baseball_path, \"Base Rates.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad269a4c-4851-4477-8973-32f4267926da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_dataset['venue_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871e553-13cc-4154-ac4a-3286123271b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes\n",
    "analysis_df_list = []\n",
    "# Loop over batter sides\n",
    "for batSide in ['L', 'R']:\n",
    "    print(batSide)\n",
    "    # Subset complete dataset\n",
    "    complete_dataset_side = complete_dataset[complete_dataset['batSide'] == batSide]\n",
    "    # Calculate game averages (average rates within a particular games)\n",
    "    game_average_df = game_averages(complete_dataset_side)\n",
    "    # # Calculate player averages (average rates of all players coming into the game) (deprecated? - player level is in complete_dataset, so it's unnecessary)\n",
    "    # player_average_df = player_averages(complete_dataset_side)\n",
    "    # Calculate league averages (average rates of all PAs over last n days coming into the day)\n",
    "    league_average_df = league_average(complete_dataset_side, days=30)\n",
    "    # Average rates at park over last n games (both teams)\n",
    "    park_average_df = rolling_averages(game_average_df, 243, 'venue_id')\n",
    "    # Average rates at away games over last n games (both teams)\n",
    "    team_average_df = rolling_averages(game_average_df, 243, 'away_name')\n",
    "    # Park factors\n",
    "    park_factor_df = create_park_factors(park_average_df, team_average_df)\n",
    "    # Create dataframe that can be used to train and analyze data\n",
    "    analysis_df, venue_dummies = create_analysis_df(complete_dataset, league_average_df, park_factor_df)\n",
    "    analysis_df_list.append(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc5217-c26b-4dc4-8d03-955ccc7a06ae",
   "metadata": {},
   "source": [
    "Extract Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762f4b5-a727-42f4-bcd9-7647e21bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = analysis_df_list[0].copy()\n",
    "r_analysis_df = analysis_df_list[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2af25-3985-4583-b8bc-a5b4c78f90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del analysis_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c75831-67b2-4321-8060-f23b1d6f3776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44fa17f-219c-47e7-bc8e-f2957868b864",
   "metadata": {},
   "source": [
    "### Park Latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4125b4-4db9-41d4-8315-65a140ddbecb",
   "metadata": {},
   "source": [
    "This contains the latest data available at each park, used to create WFX <br>\n",
    "Note: We can't just use multiplier dataset for this because it won't contain data at the end of the last game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42ae99-af38-43e5-9bb3-b0a7b32ee69b",
   "metadata": {},
   "source": [
    "Columns to Keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1195af-3f58-432f-95b4-36bea7147a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_latest_columns = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + venue_dummies + [col for col in l_analysis_df.columns if col.endswith(\"_pfx\")] + [col for col in l_analysis_df.columns if col.endswith(\"_lg\")] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c39efb-3834-48d2-85b2-70f7de44498b",
   "metadata": {},
   "source": [
    "Write Park's Last Values to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147817-6116-4098-8e3f-b46bc39ef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[park_latest_columns].sort_values('game_date').drop_duplicates('venue_id', keep='last').to_csv(os.path.join(baseball_path, \"Park Latest - LHB.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7fdbb-d18c-47e7-a955-d7343aabe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[park_latest_columns].sort_values('game_date').drop_duplicates('venue_id', keep='last').to_csv(os.path.join(baseball_path, \"Park Latest - RHB.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f28a2-7a68-491b-bf45-9299539bf797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171df5e2-2793-4c9a-91a3-e1e011789822",
   "metadata": {},
   "source": [
    "### Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fe782-a3d4-4f2f-b87e-d593313dcb82",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e276c0f-9ee3-48aa-814a-853d948bbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx_list = [col for col in l_analysis_df.columns if col.endswith(\"pfx\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae33f6-ae8a-4f9f-ac72-e1d48d8d2b66",
   "metadata": {},
   "source": [
    "Previous game_date at venue_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94cd0-6d31-4ca3-a0ca-43668a6f76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[pfx_list] = l_analysis_df.groupby(\"venue_id\")[pfx_list].shift(1)\n",
    "l_analysis_df[pfx_list] = l_analysis_df.groupby([\"venue_id\", \"game_date\"])[pfx_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53392d4-d415-4705-9dd4-1ecf68e0468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[pfx_list] = r_analysis_df.groupby(\"venue_id\")[pfx_list].shift(1)\n",
    "r_analysis_df[pfx_list] = r_analysis_df.groupby([\"venue_id\", \"game_date\"])[pfx_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa86df-e891-4615-b351-caed247ded1e",
   "metadata": {},
   "source": [
    "##### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39378c-cff8-4f6e-af9c-4b1b65f03332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_list = [col for col in l_analysis_df.columns if col.endswith(\"lg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bab02-30b5-4a5e-96cc-e3383936f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = l_analysis_df.sort_values('game_date', ascending=True)\n",
    "l_analysis_df[lg_list] = l_analysis_df[lg_list].shift(1)\n",
    "l_analysis_df[lg_list] = l_analysis_df.groupby(\"game_date\")[lg_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4f8da-bb64-4e08-aa13-03519306a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df = r_analysis_df.sort_values('game_date', ascending=True)\n",
    "r_analysis_df[lg_list] = r_analysis_df[lg_list].shift(1)\n",
    "r_analysis_df[lg_list] = r_analysis_df.groupby([\"game_date\"])[lg_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a7286-6a71-4dd5-ac2c-78943c913062",
   "metadata": {},
   "source": [
    "##### Batter Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53543a-f923-4318-a214-3809a06f0610",
   "metadata": {},
   "source": [
    "Note: You need to shift by batter and pitchHand to get the batter's last PA against that hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a3032-0731-42fe-bfcc-58d991592d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_long_list = [col for col in l_analysis_df.columns if col.endswith(\"b_long\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddcdd8-2d8b-4f28-9a5e-a56fbc5cf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[b_long_list] = l_analysis_df.groupby(['batter', 'pitchHand'])[b_long_list].shift(1)\n",
    "r_analysis_df[b_long_list] = r_analysis_df.groupby(['batter', 'pitchHand'])[b_long_list].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef67cd-558d-4049-a7cd-2ce8950580be",
   "metadata": {},
   "source": [
    "##### Pitcher Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fd134-8e9f-4b49-bac8-ab9508ffec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_long_list = [col for col in l_analysis_df.columns if col.endswith(\"p_long\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652851a9-3d52-455b-8e7e-64f318993f44",
   "metadata": {},
   "source": [
    "Note: You don't to shift by batSide to get the pitcher's last PA against that hand because all hands are the same, but why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef64c1a-5f22-40bb-810e-78f442ab8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[p_long_list] = l_analysis_df.groupby(['pitcher', 'batSide'])[p_long_list].shift(1)\n",
    "r_analysis_df[p_long_list] = r_analysis_df.groupby(['pitcher', 'batSide'])[p_long_list].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c553dc3-529b-4cac-8ddf-8554977d5dc0",
   "metadata": {},
   "source": [
    "### Select Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11a93b-f63d-46ec-ac9f-2f981aab4e43",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23a588-25eb-44e0-b0c4-733efcb12b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify inputs\n",
    "training_input_list = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "# Loop over events\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    training_input_list += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7a8cb-609e-44bc-be22-caaaa6d3c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify inputs\n",
    "testing_input_list = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "# Loop over events\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    testing_input_list += [f'{event}_lg', f'{event}_lg', f'{event}_pfx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a5290-d1a6-4579-b899-d7eeb70f964b",
   "metadata": {},
   "source": [
    "### Select Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12432be5-8491-4a58-9917-c2875f5dab0e",
   "metadata": {},
   "source": [
    "Remove Infinite Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb826d-2b92-42b8-b868-7bc823c89545",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[training_input_list] = l_analysis_df[training_input_list].replace([np.inf, -np.inf], np.nan)\n",
    "r_analysis_df[training_input_list] = r_analysis_df[training_input_list].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0608d30-a1e1-4963-a1ca-c990d07309ad",
   "metadata": {},
   "source": [
    "Drop if Missing Data (Maybe after shift?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0beb1-58cc-4bc6-9202-ff3fe6ddc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = l_analysis_df.dropna()\n",
    "r_analysis_df = r_analysis_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f709e68-8ea6-482e-ae5f-7f1f2d5c0794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af1c0368-500b-4417-ae89-49fe7fff7bcc",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ff4ac-8215-4696-82d3-199d2b2ea431",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c428e-b2d4-406f-9541-239a1662f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "l_analysis_df['split'] = np.random.choice([0, 0, 1], size=len(l_analysis_df))\n",
    "r_analysis_df['split'] = np.random.choice([0, 0, 1], size=len(r_analysis_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5452e-daef-497a-922d-d0acb15412eb",
   "metadata": {},
   "source": [
    "Create masks to identify training and testing datasets (Might not use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86515a7e-44fe-49c4-ad80-31e00a1037d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training_mask = (l_analysis_df['split'] == 0)\n",
    "r_training_mask = (r_analysis_df['split'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7d473-660a-46a3-9bf7-a01273a37ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803d3a0-a00f-4f57-90bc-125654f9e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acec17a-7d0b-4082-a34c-a0572f6cf759",
   "metadata": {},
   "source": [
    "### WFX - L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef5450-90b8-4cbf-b6fb-e52dcc03b442",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898c9d5-0383-43c8-bb57-77daad7c5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (83,22)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 1\n",
    "\n",
    "quantiles = 10\n",
    "\n",
    "wfx_l_filename = f\"predict_wfx_l.pkl\"\n",
    "print(wfx_l_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf4fdd-3e76-42f3-b72f-b2aca6824da7",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee93bcb-54a1-40e8-b6eb-6bde2e9df283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Create folder\n",
    "    os.makedirs(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate), exist_ok=True)\n",
    "    \n",
    "    # Create Model\n",
    "    predict_wfx_l = MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=False, alpha=alpha, \n",
    "                                  learning_rate_init=learning_rate, early_stopping=True, random_state=random_state, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "    # Fit\n",
    "    predict_wfx_l.fit(l_analysis_df[training_input_list], l_analysis_df[['eventsModel']].values.ravel())\n",
    "\n",
    "    # Save model\n",
    "    pickle.dump(predict_wfx_l, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, wfx_l_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b033748-710c-4d48-9c18-f2d4a8965794",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5604038-b47b-4555-a7f5-954757fc6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_l_outputs = list(predict_wfx_l.classes_)\n",
    "wfx_l_outputs_pred = [x + \"_pred\" for x in list(predict_wfx_l.classes_)]\n",
    "\n",
    "l_analysis_df[wfx_l_outputs_pred] = predict_wfx_l.predict_proba(l_analysis_df[testing_input_list].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61749d-e74e-4527-a50b-c0b3d72557ed",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931c20f-82b8-4d86-b87e-4d0a292dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies\n",
    "for event in events_list:\n",
    "    l_analysis_df[event] = (l_analysis_df['eventsModel'] == event).astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e5f4d-7996-45a7-b613-29e02a18c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "num_quantiles = 20  # Adjust if needed\n",
    "\n",
    "# Create a figure with 3x4 subplots\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))  \n",
    "axes = axes.flat  # Flatten to easily iterate\n",
    "\n",
    "for i, event in enumerate(events_list):\n",
    "    if i >= len(axes):  # Safety check if more events than subplots\n",
    "        break  \n",
    "\n",
    "    pred_col = f\"{event}_pred\"\n",
    "    quantile_col = f\"{event}_quantile\"\n",
    "\n",
    "    # Create quantiles\n",
    "    l_analysis_df[quantile_col] = pd.qcut(\n",
    "        l_analysis_df[pred_col], num_quantiles, labels=False, duplicates='drop'\n",
    "    )\n",
    "\n",
    "    # Group by quantiles\n",
    "    plot_data = l_analysis_df.groupby(quantile_col).agg(\n",
    "        avg_pred=(pred_col, \"mean\"),  # X-axis\n",
    "        avg_event=(event, \"mean\")     # Y-axis\n",
    "    ).reset_index()\n",
    "\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot data\n",
    "    sns.lineplot(x=plot_data[\"avg_pred\"], y=plot_data[\"avg_event\"], marker=\"o\", ax=ax)\n",
    "    \n",
    "    # 45-degree reference line\n",
    "    min_val, max_val = plot_data[\"avg_pred\"].min(), plot_data[\"avg_pred\"].max()\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "    ax.set_xlabel(\"Avg Pred\")\n",
    "    ax.set_ylabel(\"Avg Event\")\n",
    "    ax.set_title(event)\n",
    "    ax.set_aspect(\"equal\")  # Keep the plot square\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b49829-2b27-47ee-9701-5798579b4da3",
   "metadata": {},
   "source": [
    "##### Calculate WFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2b1b-3ebd-4d92-9b3b-484f017dca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    l_analysis_df[f'{event}_wfx_l'] = l_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d6e3a-7f90-44ae-8225-a32b84f46722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fcec6c2-e7ee-4f7c-9a82-1a291446fd93",
   "metadata": {},
   "source": [
    "### WFX - R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c99cc-4d54-4fce-ad81-5977d6058e9f",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808fb9f-cea4-4240-a15b-a6ab9370a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (83,22)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 1\n",
    "\n",
    "quantiles = 20\n",
    "\n",
    "wfx_r_filename = f\"predict_wfx_r.pkl\"\n",
    "print(wfx_r_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce681fc-6e96-49f5-873a-89b62097f059",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e3e36-90b8-45ac-9b4d-759bb5f773de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Create Model\n",
    "    predict_wfx_r = MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=False, alpha=alpha, \n",
    "                                  learning_rate_init=learning_rate, early_stopping=True, random_state=random_state, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "    # Fit\n",
    "    predict_wfx_r.fit(r_analysis_df[training_input_list], r_analysis_df[['eventsModel']].values.ravel())\n",
    "\n",
    "    # Save model\n",
    "    pickle.dump(predict_wfx_r, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, wfx_r_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72166d7-166d-47f9-a12f-2c00111c3b67",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768ad92-30df-448d-8a86-e974de259ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_r_outputs = list(predict_wfx_r.classes_)\n",
    "wfx_r_outputs_pred = [x + \"_pred\" for x in wfx_r_outputs]\n",
    "\n",
    "r_analysis_df[wfx_r_outputs_pred] = predict_wfx_r.predict_proba(r_analysis_df[testing_input_list].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740374cf-17e7-414b-84c9-f0acc5f23595",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9622e8-4aa8-4bd4-b9a6-8c73dadecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies\n",
    "for event in events_list:\n",
    "    r_analysis_df[event] = (r_analysis_df['eventsModel'] == event).astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8bc6-328e-493e-98b3-4c118fc55bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "num_quantiles = 20  # Adjust if needed\n",
    "\n",
    "# Create a figure with 3x4 subplots\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))  \n",
    "axes = axes.flat  # Flatten to easily iterate\n",
    "\n",
    "for i, event in enumerate(events_list):\n",
    "    if i >= len(axes):  # Safety check if more events than subplots\n",
    "        break  \n",
    "\n",
    "    pred_col = f\"{event}_pred\"\n",
    "    quantile_col = f\"{event}_quantile\"\n",
    "\n",
    "    # Create quantiles\n",
    "    r_analysis_df[quantile_col] = pd.qcut(\n",
    "        r_analysis_df[pred_col], num_quantiles, labels=False, duplicates='drop'\n",
    "    )\n",
    "\n",
    "    # Group by quantiles\n",
    "    plot_data = r_analysis_df.groupby(quantile_col).agg(\n",
    "        avg_pred=(pred_col, \"mean\"),  # X-axis\n",
    "        avg_event=(event, \"mean\")     # Y-axis\n",
    "    ).reset_index()\n",
    "\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot data\n",
    "    sns.lineplot(x=plot_data[\"avg_pred\"], y=plot_data[\"avg_event\"], marker=\"o\", ax=ax)\n",
    "    \n",
    "    # 45-degree reference line\n",
    "    min_val, max_val = plot_data[\"avg_pred\"].min(), plot_data[\"avg_pred\"].max()\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "    ax.set_xlabel(\"Avg Pred\")\n",
    "    ax.set_ylabel(\"Avg Event\")\n",
    "    ax.set_title(event)\n",
    "    ax.set_aspect(\"equal\")  # Keep the plot square\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7f028-de4e-426e-b02b-d4d254429682",
   "metadata": {},
   "source": [
    "##### Calculate WFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af494f02-d0e0-4ea8-abb8-deaa0d9a5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    r_analysis_df[f'{event}_wfx_r'] = r_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bf838-f9d9-4ba9-80fe-5eedf4c232d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d62eceb-e366-44ca-9d34-668a3957971a",
   "metadata": {},
   "source": [
    "### Multiplier Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b17e2c-a811-44e6-9a4d-0d16d12b894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_columns = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']\n",
    "wfx_l_columns = [col for col in l_analysis_df.columns if col.endswith(\"_wfx_l\")]\n",
    "wfx_r_columns = [col for col in r_analysis_df.columns if col.endswith(\"_wfx_r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c06ae9-860d-47fb-91d5-b538501f0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset = pd.merge(l_analysis_df.drop_duplicates('gamePk', keep='last')[descriptive_columns + wfx_l_columns], \n",
    "                              r_analysis_df.drop_duplicates('gamePk', keep='last')[descriptive_columns + wfx_r_columns], on=descriptive_columns, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059d1e2-c750-48cb-9cbb-cc9b07ca68e1",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c4db5-06f3-45aa-934e-a2cf7a40c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset['date'] = multiplier_dataset['game_date'].str.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317b1c1-fd77-4155-ac82-11b70dc86d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset.to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d80922-4b5c-4da8-bd9b-cfe5e27495ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511398-2b77-4594-9155-2c5aaf04465d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2a6a58-f18b-48da-8b63-7da94a3ee58f",
   "metadata": {},
   "source": [
    "### Generate Historic Park and Weather Factors files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddb0af-72f3-43f8-9ae0-093e9fd902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplier_dataset = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4175ca8-9421-4ca8-94b9-fb45bcbac3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select columns to keep\n",
    "# keep_columns = ['gamePk', 'game_date', 'date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'] + [col for col in multiplier_dataset.columns if \"_wfx\" in col]\n",
    "    \n",
    "# multiplier_dataset.sort_values('date', inplace=True)\n",
    "# for date in multiplier_dataset[pd.to_datetime(multiplier_dataset['game_date']).dt.year >= 2022]['date'].unique():\n",
    "#     print(date)\n",
    "#     if date > \"20220101\":\n",
    "#         # Subset by date\n",
    "#         daily_weather_df = multiplier_dataset[multiplier_dataset['date'] == date][keep_columns]\n",
    "\n",
    "#         # Write to CSV\n",
    "#         daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"3. Park and Weather Factors\", f\"{date} Park and Weather Factors.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54965ad8-c7cc-4a0c-9184-acf9132abeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0627957-32c5-40e1-a759-d9b92f28ff3b",
   "metadata": {},
   "source": [
    "### Note: Rerun B01. Matchups.ipynb if new historic Park x Weather Effects are generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
