{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9606b7e",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfaba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import concurrent.futures\n",
    "import csv\n",
    "import cv2\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import distutils.dir_util\n",
    "import gc\n",
    "import glob\n",
    "import IPython.display\n",
    "import ipywidgets as widgets\n",
    "import joblib\n",
    "import json\n",
    "import keras\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openmeteo_requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import pyautogui\n",
    "import pyperclip\n",
    "import pytz\n",
    "import re\n",
    "import requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import selenium\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import statsapi\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import traceback\n",
    "import unidecode\n",
    "import warnings\n",
    "import webbrowser\n",
    "import xlrd\n",
    "import random\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from copy import deepcopy\n",
    "from dateutil import parser\n",
    "\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "from functools import partial\n",
    "from io import StringIO\n",
    "from IPython.display import display, Javascript, clear_output\n",
    "from joblib import Parallel, delayed\n",
    "from lxml import html\n",
    "from openpyxl import load_workbook\n",
    "from paretoset import paretoset\n",
    "from pathlib import Path\n",
    "from pulp import GLPK_CMD  \n",
    "from pybaseball import statcast\n",
    "from pydfs_lineup_optimizer import get_optimizer, Site, Sport, Player, TeamStack, PlayerFilter, RandomFantasyPointsStrategy, ProgressiveFantasyPointsStrategy, AfterEachExposureStrategy, LineupOptimizer\n",
    "# from pydfs_lineup_optimizer.solvers.mip_solver import MIPSolver\n",
    "from pydfs_lineup_optimizer.solvers import PuLPSolver\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import smtplib\n",
    "import ssl\n",
    "from statsapi import get\n",
    "from sqlalchemy import create_engine\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Ensure the warning is ignored only once\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "# Display the DataFrame\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Set paths\n",
    "model_path = r\"C:\\Users\\james\\Documents\\MLB\\Models\"\n",
    "baseball_path = r\"C:\\Users\\james\\Documents\\MLB\\Database\"\n",
    "download_path = r\"C:\\Users\\james\\Downloads\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d57c3",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today's Date\n",
    "# YYYY-MM-DD (datetime)\n",
    "todaysdate_dt = datetime.date.today()\n",
    "\n",
    "# YYYY-MM-DD (string)\n",
    "todaysdate_dash = str(todaysdate_dt)\n",
    "\n",
    "# MM/DD/YYYY\n",
    "todaysdate_slash = todaysdate_dash.split(\"-\")\n",
    "todaysdate_slash = todaysdate_slash[1] + \"/\" + todaysdate_slash[2] + \"/\" + todaysdate_slash[0]\n",
    "\n",
    "# YYYYMMDD\n",
    "todaysdate = todaysdate_dash.replace(\"-\", \"\")\n",
    "\n",
    "## MM-DD-YYYY\n",
    "todaysdate_dash = todaysdate[:4] + \"-\" + todaysdate[4:6] + \"-\" + todaysdate[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "current_date = datetime.datetime.now()\n",
    "\n",
    "# Subtract one day from the current date to get yesterday's date\n",
    "yesterday_dt = current_date - datetime.timedelta(days=1)\n",
    "\n",
    "# Format yesterday's date as \"YYYYMMDD\"\n",
    "yesterdaysdate = yesterday_dt.strftime(\"%Y%m%d\")\n",
    "\n",
    "# MM/DD/YYYY\n",
    "yesterdaysdate_slash = yesterdaysdate[4:6] + \"/\" + yesterdaysdate[6:8] + \"/\" + yesterdaysdate[0:4] \n",
    "\n",
    "## MM-DD-YYYY\n",
    "yesterdaysdate_dash = yesterdaysdate[:4] + \"-\" + yesterdaysdate[4:6] + \"-\" + yesterdaysdate[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f378a83",
   "metadata": {},
   "source": [
    "### Stat Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97d0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b1_adj', 'b2_adj', 'b3_adj', 'hr_adj', 'bb_adj', 'hbp_adj', 'so_adj', 'fo_adj', 'go_adj', 'lo_adj', 'po_adj']\n"
     ]
    }
   ],
   "source": [
    "# Plate apperance events \n",
    "events_list = ['b1','b2','b3','hr','bb','hbp','so','fo','go','lo','po']\n",
    "# Adjusted for park\n",
    "events_list_adj = [f'{event}_adj' for event in events_list]\n",
    "# List of statcast stats\n",
    "statcast_list = ['estimated_woba_using_speedangle','to_left','to_middle','to_right','hard_hit','barrel']\n",
    "# Stats for which we need to calculate rates\n",
    "calc_list = ['iso','slg','obp','woba']\n",
    "# Stats for which we want maximums over time\n",
    "max_list = ['totalDistance','maxSpeed','maxSpin','launchSpeed']\n",
    "\n",
    "# Stats for which we want averages over time\n",
    "avg_list = events_list + statcast_list \n",
    "\n",
    "# All stats\n",
    "stat_list = avg_list + calc_list + max_list + ['ab','pa']\n",
    "\n",
    "### Batter\n",
    "# Create position/length-specific inputs from stats\n",
    "batter_avg_short = [f\"{stat}_b\" for stat in avg_list]\n",
    "batter_avg_long =  [f\"{stat}_b_long\" for stat in avg_list]\n",
    "\n",
    "batter_calc_short = [f\"{stat}_b\" for stat in calc_list]\n",
    "batter_calc_long = [f\"{stat}_b_long\" for stat in calc_list]\n",
    "\n",
    "batter_max_short = [f\"{stat}_b\" for stat in max_list]\n",
    "batter_max_long =  [f\"{stat}_b_long\" for stat in max_list]\n",
    "\n",
    "batter_stats_short = [f\"{stat}_b\" for stat in stat_list]\n",
    "batter_stats_long  = [f\"{stat}_b_long\" for stat in stat_list]\n",
    "\n",
    "# Steamer stats\n",
    "batter_stats_fg =     ['b1_rate','b2_rate','b3_rate','hr_rate','bb_rate','hbp_rate','so_rate','woba','slg','obp']\n",
    "# For use in imputations\n",
    "batter_stats_fg_imp = batter_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "\n",
    "### Pitcher\n",
    "# Create position/length-specific inputs from stats\n",
    "pitcher_avg_short = [f\"{stat}_p\" for stat in avg_list]\n",
    "pitcher_avg_long =  [f\"{stat}_p_long\" for stat in avg_list]\n",
    "\n",
    "pitcher_calc_short = [f\"{stat}_p\" for stat in calc_list]\n",
    "pitcher_calc_long = [f\"{stat}_p_long\" for stat in calc_list]\n",
    "\n",
    "pitcher_max_short = [f\"{stat}_p\" for stat in max_list]\n",
    "pitcher_max_long =  [f\"{stat}_p_long\" for stat in max_list]\n",
    "\n",
    "pitcher_stats_short = [f\"{stat}_p\" for stat in stat_list]\n",
    "pitcher_stats_long  = [f\"{stat}_p_long\" for stat in stat_list]\n",
    "\n",
    "# Steamer stats (for imputing)\n",
    "pitcher_stats_fg =    ['H9','HR9','K9','BB9','GBrate','FBrate','LDrate','SIERA']\n",
    "# Add stats for projecting pulls\n",
    "pitcher_stats_fg2 = pitcher_stats_fg + ['reliability','IP_start','IP','relief_IP']\n",
    "# For use in imputations\n",
    "pitcher_stats_fg_imp = pitcher_stats_fg + ['b_L', 'p_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b202b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game state\n",
    "game_info_list = ['date', 'gamePk', 'atBatIndex', 'inning', 'halfInning', 'outs', 'awayscore', 'homescore']\n",
    "\n",
    "# Player info\n",
    "batter_info_list = ['batterName', 'batter', 'batSide']\n",
    "pitcher_info_list = ['pitcherName', 'pitcher', 'pitchHand']\n",
    "           \n",
    "# PAs and ABs\n",
    "pa_list = ['pa_b', 'ab_b', 'pa_p', 'ab_p', 'pa_b_long', 'ab_b_long', 'pa_p_long', 'ab_p_long']\n",
    "    \n",
    "# Base running\n",
    "baserunning_stats = [\"sba_2b\", \"sb_2b\", \"sba_3b\", \"sb_3b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd329da",
   "metadata": {},
   "source": [
    "### PA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exclusions\n",
    "# Stats that do not apply to the position or we just don't want\n",
    "pa_exclude = [\"maxSpeed_b\", \"maxSpin_b\", \"maxSpeed_b_long\", \"maxSpin_b_long\", \n",
    "              \"totalDistance_p\", \"totalDistance_p_long\", \"launchSpeed_p\", \"launchSpeed_p_long\",\n",
    "              \"ab_b\", \"pa_b\", \"ab_b_long\", \"pa_b_long\", \n",
    "              \"ab_p\", \"pa_p\", \"ab_p_long\", \"pa_p_long\"]\n",
    "\n",
    "### batter_inputs\n",
    "batter_inputs = batter_stats_short + batter_stats_long\n",
    "batter_inputs = [item for item in batter_inputs if item not in pa_exclude]\n",
    "\n",
    "### pitcher_inputs\n",
    "pitcher_inputs = pitcher_stats_short + pitcher_stats_long\n",
    "pitcher_inputs = [item for item in pitcher_inputs if item not in pa_exclude]\n",
    "\n",
    "# Hand-specific stats (not in PA model)\n",
    "batter_stats_l = [f'{stat}_l' for stat in batter_inputs]\n",
    "batter_stats_r = [f'{stat}_r' for stat in batter_inputs]\n",
    "\n",
    "pitcher_stats_l = [f'{stat}_l' for stat in pitcher_inputs]\n",
    "pitcher_stats_r = [f'{stat}_r' for stat in pitcher_inputs]\n",
    "\n",
    "# Hand dummies\n",
    "hand_inputs = ['p_L', 'b_L']\n",
    "\n",
    "# Venue ID numbers\n",
    "venue_nums = ['1', '2', '3', '4', '5', '7', '10', '12', '13', '14', '15', '16', '17', '19', '22', '31', '32', \n",
    "              '680', '2392', '2394', '2395', '2535', '2536', '2602', '2680', '2681', '2701', '2735', '2756', \n",
    "              '2889', '3289', '3309', '3312', '3313', '4169', '4705', '5010', '5325', '5365', '5381', '5445']\n",
    "\n",
    "# Venue dummies\n",
    "venue_inputs = [f\"venue_{num}\" for num in venue_nums]\n",
    "\n",
    "# Year dummies\n",
    "year_inputs = [f\"year_{year}\" for year in range(2015,2025)]\n",
    "\n",
    "# Weather inputs\n",
    "weather_inputs = ['x_vect','y_vect','temperature']\n",
    "\n",
    "# Game state inputs\n",
    "game_state_inputs = ['onFirst','onSecond','onThird','top','outs_pre','score_diff']\n",
    "\n",
    "# Imputation flag dummies\n",
    "imp_inputs = ['imp_b', 'imp_p']\n",
    "\n",
    "# Starter dummy\n",
    "starter_inputs = ['starter']\n",
    "\n",
    "# All inputs into final PA model\n",
    "pa_inputs = batter_inputs + pitcher_inputs + hand_inputs + venue_inputs + year_inputs + weather_inputs + game_state_inputs + imp_inputs + starter_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a962202",
   "metadata": {},
   "source": [
    "### Pull Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of rolling stats in pull inputs (these are derived from other status and therefore shouldn't be included in models_inputs_plus)\n",
    "rolled_sums_list = [f'{stat}_sum' for stat in events_list] + ['rbi_sum', 'faced_sum']\n",
    "\n",
    "# Pull inputs related to the game state\n",
    "# OUT may be redundant, but it still might help\n",
    "pull_inputs_game = ['OUT','pitcherScore','batterScore','inning_adj','outs_adj','faced_inning','br_inning','onFirst','onSecond','onThird']\n",
    "pull_inputs_other = ['IP_start'] # + year_inputs\n",
    "\n",
    "# All inputs into final pulls model (specifically, from the API/Statcast, not Steamer)\n",
    "pull_inputs_pre = rolled_sums_list + pull_inputs_game + pull_inputs_other\n",
    "# Elements to remove\n",
    "pull_exclude = [\"fo_sum\", \"go_sum\", \"lo_sum\", \"po_sum\", \"rbi_sum\"] # Definitely do not want RBI sum as it's redundant with batterScore (we have no unearned runs)\n",
    "\n",
    "# Create a new list with elements not in the elements_to_remove list\n",
    "pull_inputs = [element for element in pull_inputs_pre if element not in pull_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d250c316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb170b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55eee41-c3b4-47ec-80dd-5dd491c28b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom MLPDropout classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network._stochastic_optimizers import AdamOptimizer\n",
    "from sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS\n",
    "from sklearn.utils import shuffle, gen_batches, check_random_state, _safe_indexing\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.base import is_classifier\n",
    "\n",
    "class MLPDropout(MLPClassifier):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation=\"relu\",\n",
    "        *,\n",
    "        solver=\"adam\",\n",
    "        alpha=0.0001,\n",
    "        batch_size=\"auto\",\n",
    "        learning_rate=\"constant\",\n",
    "        learning_rate_init=0.001,\n",
    "        power_t=0.5,\n",
    "        max_iter=200,\n",
    "        shuffle=True,\n",
    "        random_state=None,\n",
    "        tol=1e-4,\n",
    "        verbose=False,\n",
    "        warm_start=False,\n",
    "        momentum=0.9,\n",
    "        nesterovs_momentum=True,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.1,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        n_iter_no_change=10,\n",
    "        max_fun=15000,\n",
    "        dropout = None,\n",
    "    ):\n",
    "        '''\n",
    "        Additional Parameters:\n",
    "        ----------\n",
    "        dropout : float in range (0, 1), default=None\n",
    "            Dropout parameter for the model, defines the percentage of nodes\n",
    "            to remove at each layer.\n",
    "            \n",
    "        '''\n",
    "        self.dropout = dropout\n",
    "        super().__init__(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            power_t=power_t,\n",
    "            max_iter=max_iter,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "            tol=tol,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            momentum=momentum,\n",
    "            nesterovs_momentum=nesterovs_momentum,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "            epsilon=epsilon,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            max_fun=max_fun,\n",
    "        )\n",
    "    \n",
    "    def _fit_stochastic(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        activations,\n",
    "        deltas,\n",
    "        coef_grads,\n",
    "        intercept_grads,\n",
    "        layer_units,\n",
    "        incremental,\n",
    "    ):\n",
    "        params = self.coefs_ + self.intercepts_\n",
    "        if not incremental or not hasattr(self, \"_optimizer\"):\n",
    "            if self.solver == \"sgd\":\n",
    "                self._optimizer = SGDOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.learning_rate,\n",
    "                    self.momentum,\n",
    "                    self.nesterovs_momentum,\n",
    "                    self.power_t,\n",
    "                )\n",
    "            elif self.solver == \"adam\":\n",
    "                self._optimizer = AdamOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.beta_1,\n",
    "                    self.beta_2,\n",
    "                    self.epsilon,\n",
    "                )\n",
    "\n",
    "        # early_stopping in partial_fit doesn't make sense\n",
    "        early_stopping = self.early_stopping and not incremental\n",
    "        if early_stopping:\n",
    "            # don't stratify in multilabel classification\n",
    "            should_stratify = is_classifier(self) and self.n_outputs_ == 1\n",
    "            stratify = y if should_stratify else None\n",
    "            X, X_val, y, y_val = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                random_state=self._random_state,\n",
    "                test_size=self.validation_fraction,\n",
    "                stratify=stratify,\n",
    "            )\n",
    "            if is_classifier(self):\n",
    "                y_val = self._label_binarizer.inverse_transform(y_val)\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        sample_idx = np.arange(n_samples, dtype=int)\n",
    "\n",
    "        if self.batch_size == \"auto\":\n",
    "            batch_size = min(200, n_samples)\n",
    "        else:\n",
    "            if self.batch_size < 1 or self.batch_size > n_samples:\n",
    "                warnings.warn(\n",
    "                    \"Got `batch_size` less than 1 or larger than \"\n",
    "                    \"sample size. It is going to be clipped\"\n",
    "                )\n",
    "            batch_size = np.clip(self.batch_size, 1, n_samples)\n",
    "\n",
    "        try:\n",
    "            for it in range(self.max_iter):\n",
    "                if self.shuffle:\n",
    "                    # Only shuffle the sample indices instead of X and y to\n",
    "                    # reduce the memory footprint. These indices will be used\n",
    "                    # to slice the X and y.\n",
    "                    sample_idx = shuffle(sample_idx, random_state=self._random_state)\n",
    "\n",
    "                accumulated_loss = 0.0\n",
    "                for batch_slice in gen_batches(n_samples, batch_size):\n",
    "                    if self.shuffle:\n",
    "                        X_batch = _safe_indexing(X, sample_idx[batch_slice])\n",
    "                        y_batch = y[sample_idx[batch_slice]]\n",
    "                    else:\n",
    "                        X_batch = X[batch_slice]\n",
    "                        y_batch = y[batch_slice]\n",
    "                    \n",
    "                    activations[0] = X_batch\n",
    "                    # (DROPOUT ADDITION) layer_units passed forward to help build dropout mask.\n",
    "                    batch_loss, coef_grads, intercept_grads = self._backprop(\n",
    "                        X_batch,\n",
    "                        y_batch,\n",
    "                        activations,\n",
    "                        layer_units,\n",
    "                        deltas,\n",
    "                        coef_grads,\n",
    "                        intercept_grads,\n",
    "                    )\n",
    "                    accumulated_loss += batch_loss * (\n",
    "                        batch_slice.stop - batch_slice.start\n",
    "                    )\n",
    "\n",
    "                    # update weights\n",
    "                    grads = coef_grads + intercept_grads\n",
    "                    self._optimizer.update_params(params, grads)\n",
    "\n",
    "                self.n_iter_ += 1\n",
    "                self.loss_ = accumulated_loss / X.shape[0]\n",
    "\n",
    "                self.t_ += n_samples\n",
    "                self.loss_curve_.append(self.loss_)\n",
    "                if self.verbose:\n",
    "                    print(\"Iteration %d, loss = %.8f\" % (self.n_iter_, self.loss_))\n",
    "\n",
    "                # update no_improvement_count based on training loss or\n",
    "                # validation score according to early_stopping\n",
    "                self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
    "\n",
    "                # for learning rate that needs to be updated at iteration end\n",
    "                self._optimizer.iteration_ends(self.t_)\n",
    "\n",
    "                if self._no_improvement_count > self.n_iter_no_change:\n",
    "                    # not better than last `n_iter_no_change` iterations by tol\n",
    "                    # stop or decrease learning rate\n",
    "                    if early_stopping:\n",
    "                        msg = (\n",
    "                            \"Validation score did not improve more than \"\n",
    "                            \"tol=%f for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "                    else:\n",
    "                        msg = (\n",
    "                            \"Training loss did not improve more than tol=%f\"\n",
    "                            \" for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "\n",
    "                    is_stopping = self._optimizer.trigger_stopping(msg, self.verbose)\n",
    "                    if is_stopping:\n",
    "                        break\n",
    "                    else:\n",
    "                        self._no_improvement_count = 0\n",
    "\n",
    "                if incremental:\n",
    "                    break\n",
    "\n",
    "                if self.n_iter_ == self.max_iter:\n",
    "                    warnings.warn(\n",
    "                        \"Stochastic Optimizer: Maximum iterations (%d) \"\n",
    "                        \"reached and the optimization hasn't converged yet.\"\n",
    "                        % self.max_iter,\n",
    "                        ConvergenceWarning,\n",
    "                    )\n",
    "        except KeyboardInterrupt:\n",
    "            warnings.warn(\"Training interrupted by user.\")\n",
    "\n",
    "        if early_stopping:\n",
    "            # restore best weights\n",
    "            self.coefs_ = self._best_coefs\n",
    "            self.intercepts_ = self._best_intercepts\n",
    "    \n",
    "    def _backprop(self, X, y, activations, layer_units, deltas, coef_grads, intercept_grads):\n",
    "        \"\"\"Compute the MLP loss function and its corresponding derivatives\n",
    "        with respect to each parameter: weights and bias vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "             \n",
    "        layer_units (DROPOUT ADDITION) : list, length = n_layers\n",
    "            The layer units of the neural net, this is the shape of the\n",
    "            Neural Net model. This is used to build the dropout mask.\n",
    "\n",
    "        deltas : list, length = n_layers - 1\n",
    "            The ith element of the list holds the difference between the\n",
    "            activations of the i + 1 layer and the backpropagated error.\n",
    "            More specifically, deltas are gradients of loss with respect to z\n",
    "            in each layer, where z = wx + b is the value of a particular layer\n",
    "            before passing through the activation function\n",
    "\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            coefficient parameters of the ith layer in an iteration.\n",
    "\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            intercept parameters of the ith layer in an iteration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        dropout_masks = None\n",
    "        \n",
    "        # Create the Dropout Mask (DROPOUT ADDITION)\n",
    "        if self.dropout != None:\n",
    "            if 0 < self.dropout < 1:\n",
    "                keep_probability = 1 - self.dropout\n",
    "                dropout_masks = [np.ones(layer_units[0])]\n",
    "                \n",
    "                # Create hidden Layer Dropout Masks\n",
    "                for units in layer_units[1:-1]:\n",
    "                    # Create inverted Dropout Mask, check for random_state\n",
    "                    if self.random_state != None:\n",
    "                        layer_mask = (self._random_state.random(units) < keep_probability).astype(int) / keep_probability\n",
    "                    else:\n",
    "                        layer_mask = (np.random.rand(units) < keep_probability).astype(int) / keep_probability\n",
    "                    dropout_masks.append(layer_mask)\n",
    "            else:\n",
    "                raise ValueError('Dropout must be between zero and one. If Dropout=X then, 0 < X < 1.')\n",
    "        \n",
    "        # Forward propagate\n",
    "        # Added dropout_makss to _forward_pass call (DROPOUT ADDITION)\n",
    "        activations = self._forward_pass(activations, dropout_masks)\n",
    "        \n",
    "        # Get loss\n",
    "        loss_func_name = self.loss\n",
    "        if loss_func_name == \"log_loss\" and self.out_activation_ == \"logistic\":\n",
    "            loss_func_name = \"binary_log_loss\"\n",
    "        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])\n",
    "        # Add L2 regularization term to loss\n",
    "        values = 0\n",
    "        for s in self.coefs_:\n",
    "            s = s.ravel()\n",
    "            values += np.dot(s, s)\n",
    "        loss += (0.5 * self.alpha) * values / n_samples\n",
    "\n",
    "        # Backward propagate\n",
    "        last = self.n_layers_ - 2\n",
    "\n",
    "        # The calculation of delta[last] here works with following\n",
    "        # combinations of output activation and loss function:\n",
    "        # sigmoid and binary cross entropy, softmax and categorical cross\n",
    "        # entropy, and identity with squared loss\n",
    "        deltas[last] = activations[-1] - y\n",
    "        \n",
    "        # Compute gradient for the last layer\n",
    "        self._compute_loss_grad(\n",
    "            last, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "        )\n",
    "\n",
    "        inplace_derivative = DERIVATIVES[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 2, 0, -1):\n",
    "            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)\n",
    "            inplace_derivative(activations[i], deltas[i - 1])\n",
    "            \n",
    "            self._compute_loss_grad(\n",
    "                i - 1, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "            )\n",
    "        \n",
    "        # Apply Dropout Masks to the Parameter Gradients (DROPOUT ADDITION)\n",
    "        if dropout_masks != None:\n",
    "            for layer in range(len(coef_grads)-1):\n",
    "                mask = (~(dropout_masks[layer+1] == 0)).astype(int)\n",
    "                coef_grads[layer] = coef_grads[layer] * mask[None, :]\n",
    "                coef_grads[layer+1] = (coef_grads[layer+1] * mask.reshape(-1, 1))\n",
    "                intercept_grads[layer] = intercept_grads[layer] * mask\n",
    "        \n",
    "        return loss, coef_grads, intercept_grads\n",
    "    \n",
    "    def _forward_pass(self, activations, dropout_masks=None):\n",
    "        \"\"\"Perform a forward pass on the network by computing the values\n",
    "        of the neurons in the hidden layers and the output layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "        dropout_mask : list, length = n_layers - 1\n",
    "            The ith element of the list holds the dropout mask for the ith\n",
    "            layer.\n",
    "        \"\"\"\n",
    "        hidden_activation = ACTIVATIONS[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 1):\n",
    "            activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i])\n",
    "            activations[i + 1] += self.intercepts_[i]\n",
    "            \n",
    "            # For the hidden layers\n",
    "            if (i + 1) != (self.n_layers_ - 1):\n",
    "                hidden_activation(activations[i + 1])\n",
    "            \n",
    "            # Apply Dropout Mask (DROPOUT ADDITION)\n",
    "            if (i + 1) != (self.n_layers_ - 1) and dropout_masks != None:\n",
    "                check1 = activations[i].copy()\n",
    "                activations[i+1] = activations[i+1] * dropout_masks[i+1][None, :]\n",
    "\n",
    "        # For the last layer\n",
    "        output_activation = ACTIVATIONS[self.out_activation_]\n",
    "        output_activation(activations[i + 1])\n",
    "        return activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-base)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
