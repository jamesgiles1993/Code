{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdd389f-17f6-489f-b015-f012060f521a",
   "metadata": {},
   "source": [
    "# U2. Functions\n",
    "- This imports commonly-used functions and maps\n",
    "- Type: Utility\n",
    "- Run Frequency: Frequent\n",
    "- Created: 11/1/2023\n",
    "- Updated: 8/20/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfd222-ad2e-4306-b1ba-8fd358d7e592",
   "metadata": {},
   "source": [
    "##### Remove Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502d9b4-e3f2-485d-9ed5-1ad7b3cba55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(old):\n",
    "    new = re.sub(r'[àáâãäå]', 'a', old)\n",
    "    new = re.sub(r'[èéêë]', 'e', new)\n",
    "    new = re.sub(r'[ìíîï]', 'i', new)\n",
    "    new = re.sub(r'[òóôõö]', 'o', new)\n",
    "    new = re.sub(r'[ùúûü]', 'u', new)\n",
    "    new = re.sub(r'[ñ]', 'n', new)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b476d71-d9b7-436d-97d1-653dff3152f7",
   "metadata": {},
   "source": [
    "##### Pause Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a832ee-1a24-41a7-8bca-e7582a7b1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_code(start_time='2023-08-09T07:24:30', timezone='EST'):\n",
    "    est_timezone = pytz.timezone('America/New_York')  # Eastern Standard Time (EST)\n",
    "    \n",
    "    # Convert start_time to datetime object in EST timezone\n",
    "    naive_datetime = datetime.datetime.fromisoformat(start_time)\n",
    "    est_start_time = est_timezone.localize(naive_datetime)\n",
    "\n",
    "    # Convert EST time to UTC\n",
    "    utc_start_time = est_start_time.astimezone(pytz.utc)\n",
    "\n",
    "    time_difference = utc_start_time - datetime.datetime.now(pytz.utc)\n",
    "    total_seconds = time_difference.total_seconds()\n",
    "    \n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    \n",
    "    est_time_str = est_start_time.strftime(\"%I:%M%p\")\n",
    "    time_until_str = f\"{est_time_str}. {hours} hours, {minutes} minutes, and {seconds} seconds.\"\n",
    "    \n",
    "    print(\"Time until\", time_until_str)\n",
    "\n",
    "    # Loop with a small sleep interval, checking for interruption\n",
    "    try:\n",
    "        while total_seconds > 0:\n",
    "            time.sleep(1)  # Sleep for 1 second\n",
    "            total_seconds -= 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    ### Set date (may be different in morning)\n",
    "    # Today's Date\n",
    "    # YYYY-MM-DD (datetime)\n",
    "    todaysdate_dt = datetime.date.today()\n",
    "    \n",
    "    # YYYY-MM-DD (string)\n",
    "    todaysdate_dash = str(todaysdate_dt)\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    todaysdate_slash = todaysdate_dash.split(\"-\")\n",
    "    todaysdate_slash = todaysdate_slash[1] + \"/\" + todaysdate_slash[2] + \"/\" + todaysdate_slash[0]\n",
    "    \n",
    "    # YYYYMMDD\n",
    "    todaysdate = todaysdate_dash.replace(\"-\", \"\")\n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    todaysdate_dash = todaysdate[:4] + \"-\" + todaysdate[4:6] + \"-\" + todaysdate[6:]\n",
    "\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now()\n",
    "    \n",
    "    # Subtract one day from the current date to get yesterday's date\n",
    "    yesterday_dt = current_date - datetime.timedelta(days=1)\n",
    "    \n",
    "    # Format yesterday's date as \"YYYYMMDD\"\n",
    "    yesterdaysdate = yesterday_dt.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # MM/DD/YYYY\n",
    "    yesterdaysdate_slash = yesterdaysdate[4:6] + \"/\" + yesterdaysdate[6:8] + \"/\" + yesterdaysdate[0:4] \n",
    "    \n",
    "    ## MM-DD-YYYY\n",
    "    yesterdaysdate_dash = yesterdaysdate[:4] + \"-\" + yesterdaysdate[4:6] + \"-\" + yesterdaysdate[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7c23d-7760-4eb7-a4f5-958cf1fe3413",
   "metadata": {},
   "source": [
    "##### Identify Pareto-Optimal Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291910d3-199d-4157-8f00-08f74178f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimal(df, objectives, directions):\n",
    "    data = df[objectives].values\n",
    "    num_points = data.shape[0]\n",
    "\n",
    "    # Convert objectives based on direction\n",
    "    for i, direction in enumerate(directions):\n",
    "        if direction == \"Maximize\":\n",
    "            data[:, i] *= -1\n",
    "\n",
    "    # Pareto front mask\n",
    "    pareto_mask = np.ones(num_points, dtype=bool)\n",
    "\n",
    "    # Check for dominance\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            if i != j:\n",
    "                # Row j dominates row i if it's better in at least one objective and not worse in others\n",
    "                if np.all(data[j] <= data[i]) and np.any(data[j] < data[i]):\n",
    "                    pareto_mask[i] = False\n",
    "                    break\n",
    "\n",
    "    # Return the Pareto-optimal rows\n",
    "    return df[pareto_mask].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e55232-7856-4de0-881e-65227fc790d7",
   "metadata": {},
   "source": [
    "##### Create Game DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673590db-9dbb-4e5f-9c2f-7be60560686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_games(start_date, end_date, team_dict):\n",
    "    \"\"\"\n",
    "    Fetch game schedules for a given date range.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date (str): Start date in \"YYYYMMDD\" format.\n",
    "    - end_date (str): End date in \"YYYYMMDD\" format.\n",
    "    \n",
    "    Returns:\n",
    "    - Data Frame: Combined schedule for the specified date range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reformat dates\n",
    "    start_date = start_date[4:6] + \"/\" + start_date[6:8] + \"/\" + start_date[:4] \n",
    "    end_date = end_date[4:6] + \"/\" + end_date[6:8] + \"/\" + end_date[:4] \n",
    "\n",
    "    # Extract year    \n",
    "    start_year = int(start_date.split(\"/\")[-1])\n",
    "    end_year = int(end_date.split(\"/\")[-1])\n",
    "    \n",
    "    # Initialize an empty list to hold game schedules\n",
    "    games = []\n",
    "    \n",
    "    # Iterate through each year in the range and fetch schedules\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Determine the bounds for statsapi.schedule\n",
    "        year_start = start_date if year == start_year else f\"01/01/{year}\"\n",
    "        year_end = end_date if year == end_year else f\"12/31/{year}\"\n",
    "        \n",
    "        # Fetch and append the schedules\n",
    "        games.extend(statsapi.schedule(start_date=year_start, end_date=year_end))\n",
    "    \n",
    "    # Create dataframe\n",
    "    game_df = pd.DataFrame(games)\n",
    "    # Create date variable\n",
    "    game_df['date'] = game_df['game_date'].str.replace(\"-\",\"\")\n",
    "    # Create year variable\n",
    "    game_df['year'] = game_df['game_date'].str[0:4]\n",
    "    # Select subsample of games to run (exclude spring training, all-star games, exhibitions, and cancelled games\n",
    "    game_df = game_df.query('game_type != \"S\" and game_type != \"A\" and game_type != \"E\" and status != \"Cancelled\" and status != \"Postponed\"').reset_index(drop=True)\n",
    "\n",
    "    # Map in team names\n",
    "    game_df['away_team'] = game_df['away_name'].map(team_dict)\n",
    "    game_df['home_team'] = game_df['home_name'].map(team_dict)\n",
    "\n",
    "    # Convert to numeric\n",
    "    game_df['away_score'] = game_df['away_score'].astype('int')\n",
    "    game_df['home_score'] = game_df['home_score'].astype('int')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    game_df.drop_duplicates('game_id', inplace=True, keep='last')\n",
    "    game_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    game_df.drop(columns=['home_pitcher_note', 'away_pitcher_note', 'national_broadcasts', 'series_status', 'summary'], inplace=True)\n",
    "    \n",
    "    \n",
    "    return game_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaf000-e550-4cc7-964f-e10d5bab23e1",
   "metadata": {},
   "source": [
    "##### Create Contest Guide DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd06fc5-4b09-476e-b85e-c5b8db55213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contests(start_date=None, end_date=None, name=None, entryFee=None, exclusions=['vs', 'Turbo', '@']):\n",
    "    # Get all file paths\n",
    "    all_files = glob.glob(os.path.join(baseball_path, \"A09. Contest Guides\", \"*.csv\"))\n",
    "\n",
    "    # Parallel read\n",
    "    df_list = Parallel(n_jobs=-1)(\n",
    "        delayed(pd.read_csv)(file, dtype='str') for file in all_files\n",
    "    )\n",
    "\n",
    "    # Concatenate\n",
    "    contest_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Convert data types\n",
    "    contest_df['game_id'] = contest_df['game_id'].astype(int)\n",
    "    contest_df['date'] = pd.to_datetime(contest_df['date'].astype(str), format='%Y%m%d')\n",
    "\n",
    "    # Apply filters\n",
    "    if start_date is not None:\n",
    "        contest_df = contest_df[contest_df['date'] >= pd.to_datetime(start_date, format='%Y%m%d')]\n",
    "    if end_date is not None:\n",
    "        contest_df = contest_df[contest_df['date'] <= pd.to_datetime(end_date, format='%Y%m%d')]\n",
    "    if name is not None:\n",
    "        contest_df = contest_df[contest_df['name'].str.contains(name)]\n",
    "    if exclusions != []:\n",
    "        for exclusion in exclusions:\n",
    "            contest_df = contest_df[~contest_df['name'].str.contains(exclusion)]\n",
    "\n",
    "    # Convert date back to string\n",
    "    contest_df['date'] = contest_df['date'].dt.strftime('%Y%m%d')\n",
    "\n",
    "    # Calculate slate_size \n",
    "    contest_df['slate_size'] = contest_df.groupby('contestKey')['contestKey'].transform('count')\n",
    "\n",
    "    return contest_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dd708-4264-4149-9805-187c1b31c2cb",
   "metadata": {},
   "source": [
    "##### MLP with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2009a-1312-4b87-a0fc-c491e138761e",
   "metadata": {},
   "source": [
    "Source: https://datascience.stackexchange.com/questions/117082/how-can-i-implement-dropout-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6dcda6-730f-4baa-a42d-00abe3dfa041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom MLPDropout classifier\n",
    "from sklearn.neural_network._stochastic_optimizers import AdamOptimizer\n",
    "from sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS\n",
    "from sklearn.utils import shuffle, gen_batches, check_random_state, _safe_indexing\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.base import is_classifier\n",
    "\n",
    "class MLPDropout(MLPClassifier):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation=\"relu\",\n",
    "        *,\n",
    "        solver=\"adam\",\n",
    "        alpha=0.0001,\n",
    "        batch_size=\"auto\",\n",
    "        learning_rate=\"constant\",\n",
    "        learning_rate_init=0.001,\n",
    "        power_t=0.5,\n",
    "        max_iter=200,\n",
    "        shuffle=True,\n",
    "        random_state=None,\n",
    "        tol=1e-4,\n",
    "        verbose=False,\n",
    "        warm_start=False,\n",
    "        momentum=0.9,\n",
    "        nesterovs_momentum=True,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.1,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        n_iter_no_change=10,\n",
    "        max_fun=15000,\n",
    "        dropout = None,\n",
    "    ):\n",
    "        '''\n",
    "        Additional Parameters:\n",
    "        ----------\n",
    "        dropout : float in range (0, 1), default=None\n",
    "            Dropout parameter for the model, defines the percentage of nodes\n",
    "            to remove at each layer.\n",
    "            \n",
    "        '''\n",
    "        self.dropout = dropout\n",
    "        super().__init__(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            power_t=power_t,\n",
    "            max_iter=max_iter,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "            tol=tol,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            momentum=momentum,\n",
    "            nesterovs_momentum=nesterovs_momentum,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "            epsilon=epsilon,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            max_fun=max_fun,\n",
    "        )\n",
    "    \n",
    "    def _fit_stochastic(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        activations,\n",
    "        deltas,\n",
    "        coef_grads,\n",
    "        intercept_grads,\n",
    "        layer_units,\n",
    "        incremental,\n",
    "    ):\n",
    "        params = self.coefs_ + self.intercepts_\n",
    "        if not incremental or not hasattr(self, \"_optimizer\"):\n",
    "            if self.solver == \"sgd\":\n",
    "                self._optimizer = SGDOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.learning_rate,\n",
    "                    self.momentum,\n",
    "                    self.nesterovs_momentum,\n",
    "                    self.power_t,\n",
    "                )\n",
    "            elif self.solver == \"adam\":\n",
    "                self._optimizer = AdamOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.beta_1,\n",
    "                    self.beta_2,\n",
    "                    self.epsilon,\n",
    "                )\n",
    "\n",
    "        # early_stopping in partial_fit doesn't make sense\n",
    "        early_stopping = self.early_stopping and not incremental\n",
    "        if early_stopping:\n",
    "            # don't stratify in multilabel classification\n",
    "            should_stratify = is_classifier(self) and self.n_outputs_ == 1\n",
    "            stratify = y if should_stratify else None\n",
    "            X, X_val, y, y_val = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                random_state=self._random_state,\n",
    "                test_size=self.validation_fraction,\n",
    "                stratify=stratify,\n",
    "            )\n",
    "            if is_classifier(self):\n",
    "                y_val = self._label_binarizer.inverse_transform(y_val)\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        sample_idx = np.arange(n_samples, dtype=int)\n",
    "\n",
    "        if self.batch_size == \"auto\":\n",
    "            batch_size = min(200, n_samples)\n",
    "        else:\n",
    "            if self.batch_size < 1 or self.batch_size > n_samples:\n",
    "                warnings.warn(\n",
    "                    \"Got `batch_size` less than 1 or larger than \"\n",
    "                    \"sample size. It is going to be clipped\"\n",
    "                )\n",
    "            batch_size = np.clip(self.batch_size, 1, n_samples)\n",
    "\n",
    "        try:\n",
    "            for it in range(self.max_iter):\n",
    "                if self.shuffle:\n",
    "                    # Only shuffle the sample indices instead of X and y to\n",
    "                    # reduce the memory footprint. These indices will be used\n",
    "                    # to slice the X and y.\n",
    "                    sample_idx = shuffle(sample_idx, random_state=self._random_state)\n",
    "\n",
    "                accumulated_loss = 0.0\n",
    "                for batch_slice in gen_batches(n_samples, batch_size):\n",
    "                    if self.shuffle:\n",
    "                        X_batch = _safe_indexing(X, sample_idx[batch_slice])\n",
    "                        y_batch = y[sample_idx[batch_slice]]\n",
    "                    else:\n",
    "                        X_batch = X[batch_slice]\n",
    "                        y_batch = y[batch_slice]\n",
    "                    \n",
    "                    activations[0] = X_batch\n",
    "                    # (DROPOUT ADDITION) layer_units passed forward to help build dropout mask.\n",
    "                    batch_loss, coef_grads, intercept_grads = self._backprop(\n",
    "                        X_batch,\n",
    "                        y_batch,\n",
    "                        activations,\n",
    "                        layer_units,\n",
    "                        deltas,\n",
    "                        coef_grads,\n",
    "                        intercept_grads,\n",
    "                    )\n",
    "                    accumulated_loss += batch_loss * (\n",
    "                        batch_slice.stop - batch_slice.start\n",
    "                    )\n",
    "\n",
    "                    # update weights\n",
    "                    grads = coef_grads + intercept_grads\n",
    "                    self._optimizer.update_params(params, grads)\n",
    "\n",
    "                self.n_iter_ += 1\n",
    "                self.loss_ = accumulated_loss / X.shape[0]\n",
    "\n",
    "                self.t_ += n_samples\n",
    "                self.loss_curve_.append(self.loss_)\n",
    "                if self.verbose:\n",
    "                    print(\"Iteration %d, loss = %.8f\" % (self.n_iter_, self.loss_))\n",
    "\n",
    "                # update no_improvement_count based on training loss or\n",
    "                # validation score according to early_stopping\n",
    "                self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
    "\n",
    "                # for learning rate that needs to be updated at iteration end\n",
    "                self._optimizer.iteration_ends(self.t_)\n",
    "\n",
    "                if self._no_improvement_count > self.n_iter_no_change:\n",
    "                    # not better than last `n_iter_no_change` iterations by tol\n",
    "                    # stop or decrease learning rate\n",
    "                    if early_stopping:\n",
    "                        msg = (\n",
    "                            \"Validation score did not improve more than \"\n",
    "                            \"tol=%f for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "                    else:\n",
    "                        msg = (\n",
    "                            \"Training loss did not improve more than tol=%f\"\n",
    "                            \" for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "\n",
    "                    is_stopping = self._optimizer.trigger_stopping(msg, self.verbose)\n",
    "                    if is_stopping:\n",
    "                        break\n",
    "                    else:\n",
    "                        self._no_improvement_count = 0\n",
    "\n",
    "                if incremental:\n",
    "                    break\n",
    "\n",
    "                if self.n_iter_ == self.max_iter:\n",
    "                    warnings.warn(\n",
    "                        \"Stochastic Optimizer: Maximum iterations (%d) \"\n",
    "                        \"reached and the optimization hasn't converged yet.\"\n",
    "                        % self.max_iter,\n",
    "                        ConvergenceWarning,\n",
    "                    )\n",
    "        except KeyboardInterrupt:\n",
    "            warnings.warn(\"Training interrupted by user.\")\n",
    "\n",
    "        if early_stopping:\n",
    "            # restore best weights\n",
    "            self.coefs_ = self._best_coefs\n",
    "            self.intercepts_ = self._best_intercepts\n",
    "    \n",
    "    def _backprop(self, X, y, activations, layer_units, deltas, coef_grads, intercept_grads):\n",
    "        \"\"\"Compute the MLP loss function and its corresponding derivatives\n",
    "        with respect to each parameter: weights and bias vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "             \n",
    "        layer_units (DROPOUT ADDITION) : list, length = n_layers\n",
    "            The layer units of the neural net, this is the shape of the\n",
    "            Neural Net model. This is used to build the dropout mask.\n",
    "\n",
    "        deltas : list, length = n_layers - 1\n",
    "            The ith element of the list holds the difference between the\n",
    "            activations of the i + 1 layer and the backpropagated error.\n",
    "            More specifically, deltas are gradients of loss with respect to z\n",
    "            in each layer, where z = wx + b is the value of a particular layer\n",
    "            before passing through the activation function\n",
    "\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            coefficient parameters of the ith layer in an iteration.\n",
    "\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            intercept parameters of the ith layer in an iteration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        dropout_masks = None\n",
    "        \n",
    "        # Create the Dropout Mask (DROPOUT ADDITION)\n",
    "        if self.dropout != None:\n",
    "            if 0 < self.dropout < 1:\n",
    "                keep_probability = 1 - self.dropout\n",
    "                dropout_masks = [np.ones(layer_units[0])]\n",
    "                \n",
    "                # Create hidden Layer Dropout Masks\n",
    "                for units in layer_units[1:-1]:\n",
    "                    # Create inverted Dropout Mask, check for random_state\n",
    "                    if self.random_state != None:\n",
    "                        layer_mask = (self._random_state.random(units) < keep_probability).astype(int) / keep_probability\n",
    "                    else:\n",
    "                        layer_mask = (np.random.rand(units) < keep_probability).astype(int) / keep_probability\n",
    "                    dropout_masks.append(layer_mask)\n",
    "            else:\n",
    "                raise ValueError('Dropout must be between zero and one. If Dropout=X then, 0 < X < 1.')\n",
    "        \n",
    "        # Forward propagate\n",
    "        # Added dropout_makss to _forward_pass call (DROPOUT ADDITION)\n",
    "        activations = self._forward_pass(activations, dropout_masks)\n",
    "        \n",
    "        # Get loss\n",
    "        loss_func_name = self.loss\n",
    "        if loss_func_name == \"log_loss\" and self.out_activation_ == \"logistic\":\n",
    "            loss_func_name = \"binary_log_loss\"\n",
    "        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])\n",
    "        # Add L2 regularization term to loss\n",
    "        values = 0\n",
    "        for s in self.coefs_:\n",
    "            s = s.ravel()\n",
    "            values += np.dot(s, s)\n",
    "        loss += (0.5 * self.alpha) * values / n_samples\n",
    "\n",
    "        # Backward propagate\n",
    "        last = self.n_layers_ - 2\n",
    "\n",
    "        # The calculation of delta[last] here works with following\n",
    "        # combinations of output activation and loss function:\n",
    "        # sigmoid and binary cross entropy, softmax and categorical cross\n",
    "        # entropy, and identity with squared loss\n",
    "        deltas[last] = activations[-1] - y\n",
    "        \n",
    "        # Compute gradient for the last layer\n",
    "        self._compute_loss_grad(\n",
    "            last, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "        )\n",
    "\n",
    "        inplace_derivative = DERIVATIVES[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 2, 0, -1):\n",
    "            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)\n",
    "            inplace_derivative(activations[i], deltas[i - 1])\n",
    "            \n",
    "            self._compute_loss_grad(\n",
    "                i - 1, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "            )\n",
    "        \n",
    "        # Apply Dropout Masks to the Parameter Gradients (DROPOUT ADDITION)\n",
    "        if dropout_masks != None:\n",
    "            for layer in range(len(coef_grads)-1):\n",
    "                mask = (~(dropout_masks[layer+1] == 0)).astype(int)\n",
    "                coef_grads[layer] = coef_grads[layer] * mask[None, :]\n",
    "                coef_grads[layer+1] = (coef_grads[layer+1] * mask.reshape(-1, 1))\n",
    "                intercept_grads[layer] = intercept_grads[layer] * mask\n",
    "        \n",
    "        return loss, coef_grads, intercept_grads\n",
    "    \n",
    "    def _forward_pass(self, activations, dropout_masks=None):\n",
    "        \"\"\"Perform a forward pass on the network by computing the values\n",
    "        of the neurons in the hidden layers and the output layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "        dropout_mask : list, length = n_layers - 1\n",
    "            The ith element of the list holds the dropout mask for the ith\n",
    "            layer.\n",
    "        \"\"\"\n",
    "        hidden_activation = ACTIVATIONS[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 1):\n",
    "            activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i])\n",
    "            activations[i + 1] += self.intercepts_[i]\n",
    "            \n",
    "            # For the hidden layers\n",
    "            if (i + 1) != (self.n_layers_ - 1):\n",
    "                hidden_activation(activations[i + 1])\n",
    "            \n",
    "            # Apply Dropout Mask (DROPOUT ADDITION)\n",
    "            if (i + 1) != (self.n_layers_ - 1) and dropout_masks != None:\n",
    "                check1 = activations[i].copy()\n",
    "                activations[i+1] = activations[i+1] * dropout_masks[i+1][None, :]\n",
    "\n",
    "        # For the last layer\n",
    "        output_activation = ACTIVATIONS[self.out_activation_]\n",
    "        output_activation(activations[i + 1])\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be55f4-919e-4c93-b2c4-1e1727c90d6b",
   "metadata": {},
   "source": [
    "##### MLPClassifier with string outcome support (compatible with recent sklearn versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f30fe-9fb7-42aa-aecf-2b4caa886bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeMLPClassifier(MLPClassifier):\n",
    "    def _score(self, X, y):\n",
    "        \"\"\"Override sklearn's _score to avoid np.isnan on string predictions.\"\"\"\n",
    "        y_pred = self._predict(X, check_input=False)\n",
    "        # If predictions are numeric, check for NaN/inf normally\n",
    "        if np.issubdtype(np.array(y_pred).dtype, np.number):\n",
    "            if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
    "                return np.nan\n",
    "        # Otherwise just compute accuracy directly\n",
    "        return (y_pred == y).mean()\n",
    "\n",
    "    def _score_with_function(self, X, y, score_function):\n",
    "        \"\"\"Same fix for newer sklearn versions using _score_with_function.\"\"\"\n",
    "        y_pred = self._predict(X, check_input=False)\n",
    "        # Handle numeric vs string safely\n",
    "        if np.issubdtype(np.array(y_pred).dtype, np.number):\n",
    "            if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
    "                return np.nan\n",
    "        return score_function(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eac81-0adf-4785-9162-db06ee2a0828",
   "metadata": {},
   "source": [
    "##### Create Universal Team Map Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad76028-82f8-409f-8dda-d99b3e842c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_map = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Team Map.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc3711-8642-429a-aaec-1fc27aaae7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "team_dict = {}\n",
    "\n",
    "# Filter columns that end with \"TEAM\"\n",
    "team_columns = [col for col in team_map.columns if col.endswith(\"TEAM\") or col.endswith(\"NAME\") or col.endswith(\"Id\")]\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for _, row in team_map.iterrows():\n",
    "    bbref_team = row['BBREFTEAM']  # Get the BBREFTEAM value\n",
    "    # Iterate over filtered columns in the row\n",
    "    for column in team_columns:\n",
    "        value = row[column]\n",
    "        if pd.notna(value):  # Skip NaN values\n",
    "            team_dict[value] = bbref_team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593df449-248d-4fad-a220-cbee3a081003",
   "metadata": {},
   "source": [
    "##### Create Venue Map DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f38761-9163-4724-9a76-c7054f60614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_venue_map(write=False):\n",
    "    # Fetch JSON data from the URL\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract venue details \n",
    "    venues = data.get(\"venues\", data)  \n",
    "    \n",
    "    # Normalize the JSON into a DataFrame\n",
    "    df = pd.json_normalize(venues)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if write == True:\n",
    "        df.sort_values('id').to_csv(os.path.join(baseball_path, \"Utilities\", \"Venue Map.csv\"), index=False)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8e957-42ec-4cb1-9d19-249631fd8e85",
   "metadata": {},
   "source": [
    "##### Add Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1d101-312c-440b-a9db-24be17f1aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Venue Map\n",
    "venue_map_df = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Venue Map.csv\"))\n",
    "\n",
    "# George M. Steinbrenner\n",
    "venue_map_df.loc[venue_map_df['id'] == 2523, ['fieldInfo.leftCenter', 'fieldInfo.rightCenter']] = [399.0, 385.0] # Yankee Stadium dimensions\n",
    "# Sutter Health Park\n",
    "venue_map_df.loc[venue_map_df['id'] == 2529, ['fieldInfo.leftCenter', 'fieldInfo.rightCenter']] = [375.0, 368.0] # https://x.com/JonPgh/status/1875224135573594599"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f55fe2-4dd0-437c-b956-aca24a7d3b8b",
   "metadata": {},
   "source": [
    "##### Log Print Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e31316-7469-45b6-83f8-3f66ee323af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_print(text, sep, end, file, flush, write=False):\n",
    "    if write == True:\n",
    "        with open(os.path.join(baseball_path, f\"{todaysdate} Sim Log.txt\"), \"w\") as f:\n",
    "            print(text, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b643a1-3197-4947-bbd1-f71a7db24ab2",
   "metadata": {},
   "source": [
    "##### Median Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502c9d0-214d-46b4-ab6d-983f41cbe976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MedianCenterer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # store column-wise medians\n",
    "        self.medians_ = np.median(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X - self.medians_\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return X + self.medians_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-base)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
