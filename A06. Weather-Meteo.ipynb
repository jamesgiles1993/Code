{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# A06. Weather "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66c3d1-2a3f-4bc4-a77a-728a13dbbe93",
   "metadata": {},
   "source": [
    "Note: All historic Park and Weather Factors files are created from M01. Park and Weatehr Factors.ipynb upon the training of new models. A06. Weather is for daily files only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports executed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\"\n",
    "    print(\"Imports executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f93710-16ec-421a-a2f5-b31fec6633de",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c011b295-fa27-4240-a02d-536a5a9114d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Set date range \n",
    "    start_date = todaysdate\n",
    "    end_date = todaysdate\n",
    "    all_game_df = pd.read_csv(os.path.join(baseball_path, \"game_df.csv\"))\n",
    "    game_df = all_game_df[(all_game_df['date'].astype(str) >= start_date) & (all_game_df['date'].astype(str) <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f8e18-6292-4ac4-848a-5da53a9ef2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb1b8a7a-e097-4905-9618-822f2417730c",
   "metadata": {},
   "source": [
    "### Venue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e57d2-1c5e-4271-8810-b8e353b881cb",
   "metadata": {},
   "source": [
    "Merge in venue-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff19019-9e4e-4bba-a294-47e88425caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = pd.merge(game_df, venue_map_df[['id', 'location.defaultCoordinates.latitude', 'location.defaultCoordinates.longitude',\n",
    "                                          'fieldInfo.leftLine', 'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter', 'fieldInfo.rightCenter', \n",
    "                                          'location.elevation', 'location.azimuthAngle', 'fieldInfo.roofType', 'active']], \n",
    "                                           left_on=['venue_id'], right_on=['id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba6392-d79b-4502-a8d0-67bca7753d64",
   "metadata": {},
   "source": [
    "Convert to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc5915f7-a077-4e08-be6f-2d3ed73e3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_df[\"game_datetime\"] = pd.to_datetime(game_df[\"game_datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5d88f-6c22-4ef1-94cd-a0ebdb753ed5",
   "metadata": {},
   "source": [
    "Drop if missing coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea13794-cc82-4412-af64-0c80c469fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df.dropna(subset=['location.defaultCoordinates.latitude', 'location.defaultCoordinates.longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442729a4-42a8-4d6e-b8e5-085a869c7abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c503c7-9876-416a-8fda-3c7014283079",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc3cc7-2984-46e4-b4dc-30d4c0344b59",
   "metadata": {},
   "source": [
    "##### 1. Open Meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d33a7-e6f7-4059-a63f-ee682f62ffcb",
   "metadata": {},
   "source": [
    "Historic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a87683-d40d-40f8-a871-22aa5729155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "def fetch_historical_weather_data(latitude, longitude, game_datetime):\n",
    "    \"\"\"Fetch historical weather data for a given game datetime and location.\"\"\"\n",
    "\n",
    "    # Convert game_datetime to date for API request\n",
    "    game_date = game_datetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Define the parameters for the weather request\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": game_date,\n",
    "        \"end_date\": game_date,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "            \"weather_code\", \"surface_pressure\", \"wind_speed_10m\", \"wind_direction_10m\"\n",
    "        ],\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "        \"wind_speed_unit\": \"mph\",\n",
    "        \"precipitation_unit\": \"inch\"\n",
    "    }\n",
    "\n",
    "    # Fetch data from Open-Meteo API\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Process hourly data\n",
    "    hourly = response.Hourly()\n",
    "    hourly_data = {\n",
    "        \"datetime\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"temperature_2m\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "        \"relative_humidity_2m\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "        \"dew_point_2m\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "        \"weather_code\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "        \"surface_pressure\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "        \"wind_speed_10m\": hourly.Variables(5).ValuesAsNumpy(),\n",
    "        \"wind_direction_10m\": hourly.Variables(6).ValuesAsNumpy()\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(hourly_data)\n",
    "\n",
    "def create_historic_weather_df(game_df):\n",
    "    \"\"\"Append weather data to each game in game_df based on game_datetime.\"\"\"\n",
    "\n",
    "    # Convert game_datetime to UTC\n",
    "    game_df[\"game_datetime\"] = pd.to_datetime(game_df[\"game_datetime\"], utc=True)\n",
    "\n",
    "    # Lists to store the matched weather data\n",
    "    weather_columns = [\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\",\n",
    "        \"weather_code\", \"surface_pressure\", \"wind_speed_10m\", \"wind_direction_10m\"\n",
    "    ]\n",
    "    weather_data_lists = {col: [] for col in weather_columns}\n",
    "\n",
    "    # Loop through each game in the DataFrame\n",
    "    for _, row in game_df.iterrows():\n",
    "        latitude = row[\"location.defaultCoordinates.latitude\"]\n",
    "        longitude = row[\"location.defaultCoordinates.longitude\"]\n",
    "        game_datetime = row[\"game_datetime\"]\n",
    "\n",
    "        # Fetch historical weather data for that day\n",
    "        weather_data = fetch_historical_weather_data(latitude, longitude, game_datetime)\n",
    "\n",
    "        # Find the closest weather timestamp to game_datetime (typically, first top of the hour after game starts)\n",
    "        closest_weather_row = weather_data.iloc[\n",
    "            (weather_data[\"datetime\"] - game_datetime).abs().argsort()[0]\n",
    "        ]\n",
    "\n",
    "        # Append the closest weather data to lists\n",
    "        for col in weather_columns:\n",
    "            weather_data_lists[col].append(closest_weather_row[col])\n",
    "\n",
    "    # Add the weather data as new columns in game_df\n",
    "    for col in weather_columns:\n",
    "        game_df[col] = weather_data_lists[col]\n",
    "\n",
    "\n",
    "    return game_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7784841-f895-4724-8859-127d04f3cedc",
   "metadata": {},
   "source": [
    "Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8924fb-697f-4a13-9a42-a9bb0d4ecd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it doesn't have issues with double headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d8a934-2b64-499a-b02c-d7b582be9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(latitude, longitude, start, end):\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "                   \"precipitation_probability\", \"surface_pressure\", \n",
    "                   \"wind_speed_10m\", \"wind_direction_10m\", \"weather_code\"],\n",
    "        \"start\": start,  # Start time of forecast\n",
    "        \"end\": end,  # End time of forecast\n",
    "        \"wind_speed_unit\": \"mph\",\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "        \"precipitation_unit\": \"inch\",\n",
    "        \"past_days\": 2\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "    # Process the weather data\n",
    "    response = responses[0]\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_precipitation_probability = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_surface_pressure = hourly.Variables(4).ValuesAsNumpy()\n",
    "    hourly_wind_speed_10m = hourly.Variables(5).ValuesAsNumpy()\n",
    "    hourly_wind_direction_10m = hourly.Variables(6).ValuesAsNumpy()\n",
    "    hourly_weather_code = hourly.Variables(7).ValuesAsNumpy()\n",
    "\n",
    "    # Create the DataFrame with the weather data\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "    hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "    hourly_data[\"dew_point_2m\"] = hourly_dew_point_2m\n",
    "    hourly_data[\"precipitation_probability\"] = hourly_precipitation_probability\n",
    "    hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
    "    hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "    hourly_data[\"wind_direction_10m\"] = hourly_wind_direction_10m\n",
    "    hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "\n",
    "    hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "\n",
    "    # Filter the data to only include rows within the requested time range\n",
    "    hourly_dataframe[\"date\"] = pd.to_datetime(hourly_dataframe[\"date\"], utc=True)\n",
    "    hourly_dataframe = hourly_dataframe[\n",
    "        (hourly_dataframe[\"date\"] >= pd.to_datetime(start, utc=True)) & \n",
    "        (hourly_dataframe[\"date\"] <= pd.to_datetime(end, utc=True))\n",
    "    ]\n",
    "\n",
    "    return hourly_dataframe\n",
    "\n",
    "\n",
    "# Now iterate over each game in game_df to fetch the weather data\n",
    "def create_daily_weather_df(game_df):\n",
    "    # Lists to hold the weather data columns\n",
    "    temperature_2m_list = []\n",
    "    relative_humidity_2m_list = []\n",
    "    dew_point_2m_list = []\n",
    "    precipitation_probability_list = []\n",
    "    surface_pressure_list = []\n",
    "    wind_speed_10m_list = []\n",
    "    wind_direction_10m_list = []\n",
    "    weather_code_list = []\n",
    "\n",
    "    # Iterate over the rows of game_df\n",
    "    for index, row in game_df.iterrows():\n",
    "        latitude = row[\"location.defaultCoordinates.latitude\"]\n",
    "        longitude = row[\"location.defaultCoordinates.longitude\"]\n",
    "        game_datetime = pd.to_datetime(row[\"game_datetime\"])\n",
    "\n",
    "        # Set start and end time for the forecast\n",
    "        start = game_datetime.isoformat()  # Ensure ISO 8601 format without 'Z'\n",
    "        end = (game_datetime + pd.Timedelta(hours=1)).isoformat()  # Add 1 hour\n",
    "\n",
    "        # Fetch the weather data for the game\n",
    "        weather_data = fetch_weather_data(latitude, longitude, start, end)\n",
    "        \n",
    "        # Get the first row of the weather data (since we're getting 1 hour of forecast)\n",
    "        first_row = weather_data.iloc[0]\n",
    "\n",
    "        # Append the weather data to the lists\n",
    "        temperature_2m_list.append(first_row[\"temperature_2m\"])\n",
    "        relative_humidity_2m_list.append(first_row[\"relative_humidity_2m\"])\n",
    "        dew_point_2m_list.append(first_row[\"dew_point_2m\"])\n",
    "        precipitation_probability_list.append(first_row[\"precipitation_probability\"])\n",
    "        surface_pressure_list.append(first_row[\"surface_pressure\"])\n",
    "        wind_speed_10m_list.append(first_row[\"wind_speed_10m\"])\n",
    "        wind_direction_10m_list.append(first_row[\"wind_direction_10m\"])\n",
    "        weather_code_list.append(first_row[\"weather_code\"])\n",
    "\n",
    "    # Append the new weather columns to game_df\n",
    "    game_df[\"temperature_2m\"] = temperature_2m_list\n",
    "    game_df[\"relative_humidity_2m\"] = relative_humidity_2m_list\n",
    "    game_df[\"dew_point_2m\"] = dew_point_2m_list\n",
    "    game_df[\"precipitation_probability\"] = precipitation_probability_list\n",
    "    game_df[\"surface_pressure\"] = surface_pressure_list\n",
    "    game_df[\"wind_speed_10m\"] = wind_speed_10m_list\n",
    "    game_df[\"wind_direction_10m\"] = wind_direction_10m_list\n",
    "    game_df[\"weather_code\"] = weather_code_list\n",
    "\n",
    "\n",
    "    return game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f23a2-d7eb-4541-a9cb-ae70b688e767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a1ed6df-103d-49bb-9194-8e83acb0e1b1",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cee7f9-76d2-4639-a001-66c8f0603dc4",
   "metadata": {},
   "source": [
    "Swish Analytics contains weather projections to be used before MLB Stats API updates theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911a879-6dac-429f-8e42-28f8b17c1977",
   "metadata": {},
   "source": [
    "Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9cdb2-c7c2-4356-a7d9-36bcd2299748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Swish Analytics for weather data\n",
    "def swishanalytics(date):\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    # Swish Analytics URL \n",
    "    url = \"https://swishanalytics.com/mlb/weather?date=\" + date_dash\n",
    "\n",
    "     # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class 'weather-card'\n",
    "        weather_cards = soup.find_all('div', class_='weather-card')\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dfs = []\n",
    "        \n",
    "        # Iterate over each weather card\n",
    "        for weather_card in weather_cards:\n",
    "            # Extract relevant information from the weather card\n",
    "            time_info = weather_card.find('small', class_='text-muted')\n",
    "            location_info = weather_card.find('h4', class_='lato inline vert-mid bold')\n",
    "            \n",
    "            # Extract time and location information\n",
    "            time = time_info.text.strip() if time_info else None\n",
    "            location = location_info.text.strip() if location_info else None\n",
    "            \n",
    "            # Find the table within the weather card\n",
    "            table = weather_card.find('table', class_='table-bordered')\n",
    "            \n",
    "            # If table exists, extract data from it\n",
    "            if table:\n",
    "                # Extract table data into a list of lists\n",
    "                rows = table.find_all('tr')\n",
    "                data = []\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    row_data = [cell.text.strip() for cell in cells]\n",
    "                    data.append(row_data)\n",
    "                \n",
    "                # Convert data into a pandas DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Set the first row as the column headers\n",
    "                df.columns = df.iloc[0]\n",
    "                df = df[1:]  # Remove the first row since it's the header row\n",
    "                \n",
    "                # Add time and location as additional columns\n",
    "                df['Time'] = time\n",
    "                df['Location'] = location\n",
    "\n",
    "                # Create dataframem from the second time period scraped\n",
    "                daily_weather_df = pd.DataFrame(df.iloc[:, 2]).T\n",
    "                # Extract home team name \n",
    "                daily_weather_df['Matchup'] = df['Location'][1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['Matchup'].str.split(\"@\", expand=True).iloc[:, 1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['FANGRAPHSTEAM'].str.replace(\"\\xa0\\xa0\", \"\")\n",
    "\n",
    "                dfs.append(daily_weather_df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    # Append together dataframes\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={1:'Weather', 2:'Temperature', 3:'Feels Like', 4:'Humidity', 5:'Speed', 6:'Direction', 'BBREFTEAM': 'home_team'}, inplace=True)\n",
    "\n",
    "    # Clean\n",
    "    df['Speed'] = df['Speed'].str.replace(\" mph\", \"\").astype(float)\n",
    "    df['Temperature'] = df['Temperature'].str.replace('°', '')\n",
    "    df['Feels Like'] = df['Feels Like'].str.replace('°', '')\n",
    "    df.reset_index(drop=False, inplace=True, names='Time')\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1a5d5-af47-463b-8708-911b012cf77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0feb060b-9677-4f9e-9d2e-de88f9874c7b",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5cd41-3e3d-472a-9c6b-721ea3d1db24",
   "metadata": {},
   "source": [
    "RotoGrinders hosts weather warnings used to identify matchups to avoid based on weather risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e3674-87ac-4227-9a07-7e552894006b",
   "metadata": {},
   "source": [
    "Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbe8bc-5e6f-4c13-8453-4f4caf44bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotogrinders(date, team_map):\n",
    "    # URL of the web page containing the table\n",
    "    url = \"https://rotogrinders.com/weather/mlb\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Get the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = soup.find_all(\"li\", class_=\"weather-blurb\")\n",
    "\n",
    "        # Create an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        for li_element in li_elements:\n",
    "            # Extract the tag colors from the <span> elements\n",
    "            tag_elements = li_element.find_all(\"span\", class_=[\"green\", \"yellow\", \"orange\", \"red\"])\n",
    "        \n",
    "            # Extract the first tag color\n",
    "            tag = tag_elements[0].text.strip() if tag_elements else None\n",
    "        \n",
    "            # Extract the second tag color if it exists\n",
    "            tag2 = tag_elements[1].text.strip() if len(tag_elements) > 1 else None\n",
    "        \n",
    "            # Extract the matchup from the <span> element with class \"bold\"\n",
    "            matchup_span = li_element.find(\"span\", class_=\"bold\")\n",
    "            matchup = matchup_span.text.strip() if matchup_span else None\n",
    "        \n",
    "            # Extract the description if it exists\n",
    "            if matchup_span:\n",
    "                description_span = matchup_span.find_next_sibling(\"span\")\n",
    "                description = description_span.text.strip() if description_span else None\n",
    "            else:\n",
    "                description = None\n",
    "        \n",
    "            # Append the data to the list\n",
    "            data.append({\"Tag\": tag, \"Tag2\": tag2, \"Matchup\": matchup, \"Description\": description})\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        df[['away', 'home']] = df['Matchup'].str.split(\" @ \", expand=True)\n",
    "\n",
    "        # Add in DK team abbreviations \n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['away'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_away\"))\n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['home'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_home\"))\n",
    "        df = df[['Tag', 'Tag2', 'Matchup', 'DKTEAM', 'DKTEAM_home', 'Description']]\n",
    "        df.rename(columns={'DKTEAM':'Away', 'DKTEAM_home': 'Home'}, inplace=True)\n",
    "        \n",
    "        # Add the date column to the DataFrame\n",
    "        df['date'] = date\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        # Return an error message if the response is not successful\n",
    "        return \"Failed to retrieve data. Response status code: {}\".format(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d82a29-46f9-4318-b170-46ae15462ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3b4d98-8ef6-4622-ab4b-1d492d48c65f",
   "metadata": {},
   "source": [
    "##### 3. Park x Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e9aea-1d4c-40d6-93a3-96a05f159ad3",
   "metadata": {},
   "source": [
    "Calculate wind x and y vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab4f83-7852-4009-8b8c-6887bb2d5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors(row, azimuth_column, wind_column, speed_column):\n",
    "    angle = row[wind_column] - row[azimuth_column]\n",
    "    \n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row[speed_column] * -1\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row[speed_column] * -1\n",
    "\n",
    "    return pd.Series([x_vect, y_vect], index=['x_vect', 'y_vect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0aec-75ff-4191-8979-07eafb32b0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8a3b07-f812-4a86-87e6-c79adcbb62f1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752c8b6-6f18-43c3-96fc-ff954227f4cc",
   "metadata": {},
   "source": [
    "##### 1. Open Meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29756881-3e43-4093-867e-8652cf5f0a6f",
   "metadata": {},
   "source": [
    "Columns to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e717a2e-e383-4c4a-a87a-a233a4b3e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from game_df\n",
    "game_columns = ['game_id', 'game_datetime', 'game_date', 'date', 'year', 'game_type', 'status', 'away_team', 'home_team', 'doubleheader', 'game_num', 'venue_id', 'venue_name']\n",
    "# Columns Venue Map\n",
    "venue_columns = ['location.defaultCoordinates.latitude', 'location.defaultCoordinates.longitude', 'fieldInfo.leftLine', 'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter',\n",
    "                 'fieldInfo.rightCenter', 'location.elevation', 'location.azimuthAngle', 'fieldInfo.roofType', 'active']\n",
    "# Columns from Open Mateo \n",
    "weather_columns = ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'surface_pressure', 'wind_speed_10m', 'wind_direction_10m', 'weather_code']\n",
    "# Forecast-only columns from Open Meteo\n",
    "forecast_only_columns = ['precipitation_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b7c14f-6ca7-4db1-83b4-82ae218deeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250412\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 233 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop over dates\n",
    "for date in game_df['date'].unique():\n",
    "    print(date)\n",
    "    if int(date) == int(todaysdate):\n",
    "        # Create daily weather dataframe (forecast)\n",
    "        create_daily_weather_df(game_df[game_df['date'] == date])[game_columns + venue_columns + weather_columns + forecast_only_columns].to_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Open Meteo\", f\"Open Meteo {date}.csv\"), index=False)\n",
    "    else:\n",
    "        # Create historic weather dataframe\n",
    "        create_historic_weather_df(game_df[game_df['date'] == date])[game_columns + venue_columns + weather_columns].to_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Open Meteo\", f\"Open Meteo {date}.csv\"), index=False)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e995618-7cd1-4af1-b01c-19e9ce1060e1",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a707003-941c-4acc-8fd0-596008959fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Scrape Swish Analytics\n",
    "    swishanalytics_df = swishanalytics(todaysdate)\n",
    "    # To CSV\n",
    "    swishanalytics_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Swish Analytics\", f\"Swish Analytics {todaysdate}.csv\"), index=False, encoding='iso-8859-1')\n",
    "except:\n",
    "    print(\"Could not scrape Swish Analytics weather data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b471b-a217-478d-ae6f-6704d87675c0",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7527eda-bb27-46c5-8b7b-7ed0e4a30022",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Scrape RotoGrinders\n",
    "    rotogrinders_df = rotogrinders(todaysdate, team_map)\n",
    "    # To CSV\n",
    "    rotogrinders_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"2. RotoGrinders\", f\"RotoGrinders {todaysdate}.csv\"), index=False)\n",
    "except:\n",
    "    print(\"Could not scrape RotoGrinders weather data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa4ad3-4ddb-467c-80ae-28717461422d",
   "metadata": {},
   "source": [
    "##### 3. Park x Weather Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd268cd4-2f92-4f43-a86c-83ce67264593",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_weather_variables = ['x_vect', 'y_vect', 'temperature'] # drop weather\n",
    "meteo_duplicates_variables = ['meteo_x_vect', 'meteo_y_vect', 'temperature_2m']\n",
    "meteo_weather_variables = ['relative_humidity_2m', 'dew_point_2m', 'surface_pressure']\n",
    "mlb_park_variables = ['fieldInfo.leftLine', 'fieldInfo.center', 'fieldInfo.rightLine', 'fieldInfo.leftCenter', 'fieldInfo.rightCenter', 'location.elevation'] # drop roof type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918b966-8c0e-450d-be13-aa5e13278dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_venue_list = [2523, 2529]\n",
    "\n",
    "# Read in Meteo Data\n",
    "meteo_df = pd.read_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Open Meteo\", f\"Open Meteo {todaysdate}.csv\"), encoding='iso-8859-1')\n",
    "\n",
    "# Read in latest park data\n",
    "l_park_latest_df = pd.read_csv(os.path.join(baseball_path, \"Park Latest - LHB.csv\"))\n",
    "r_park_latest_df = pd.read_csv(os.path.join(baseball_path, \"Park Latest - RHB.csv\"))\n",
    "\n",
    "# Venue dummies\n",
    "venue_dummies = [col for col in l_park_latest_df.columns if (col.startswith(\"venue_\") and not col.endswith(\"id\"))]\n",
    "\n",
    "# Read in base rates\n",
    "base_rate_df = pd.read_csv(os.path.join(baseball_path, \"Base Rates.csv\"))\n",
    "\n",
    "# Calculate wind vectors\n",
    "meteo_df[['meteo_x_vect', 'meteo_y_vect']] = meteo_df.apply(lambda row: calculate_vectors(row, 'location.azimuthAngle', 'wind_direction_10m', 'wind_speed_10m'), axis=1)\n",
    "\n",
    "\n",
    "meteo_df[['weather', 'wind', 'venue', 'date', 'missing_weather']] = meteo_df['game_id'].apply(lambda game_id: pd.Series(create_box(game_id)))\n",
    "\n",
    "# Domes\n",
    "mask = meteo_df['weather'].str.contains('Roof|Dome', case=False, na=False)\n",
    "\n",
    "# # Apply the updates using the mask for each column\n",
    "meteo_df.loc[mask, 'temperature_2m'] = 70\n",
    "meteo_df.loc[mask, 'meteo_x_vect'] = 0\n",
    "meteo_df.loc[mask, 'meteo_y_vect'] = 0\n",
    "meteo_df.loc[mask, 'relative_humidity_2m'] = 60\n",
    "meteo_df.loc[mask, 'dew_point_2m'] = 57\n",
    "\n",
    "\n",
    "# Loop over events\n",
    "lg_pfx_list = []\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    lg_pfx_list += [f'{event}_lg', f'{event}_lg', f'{event}_pfx']\n",
    "\n",
    "# Identify inputs\n",
    "# Consider replacing with MLB weather, if available\n",
    "specific_model_input_list = meteo_duplicates_variables + meteo_weather_variables + mlb_park_variables + lg_pfx_list + venue_dummies\n",
    "generic_model_input_list = meteo_duplicates_variables + meteo_weather_variables + mlb_park_variables + lg_pfx_list\n",
    "\n",
    "### LHB    \n",
    "# Specific, where available\n",
    "weather_input_l_df = pd.merge(meteo_df, l_park_latest_df.drop_duplicates('venue_id', keep='last').drop(columns={'gamePk', 'game_date'}), on=['venue_id'], suffixes=(\"\", \"_l\"), how='left')\n",
    "# Predicted outputs\n",
    "wfx_l_columns = [f\"{col}_wfx_l\" for col in list(predict_wfx_l.classes_)]\n",
    "\n",
    "# Create a mask for rows where there are no missing values in the required columns\n",
    "valid_rows = weather_input_l_df[specific_model_input_list].notna().all(axis=1)\n",
    "\n",
    "# Convert infinites to 0\n",
    "for col in specific_model_input_list:\n",
    "    weather_input_l_df[col] = weather_input_l_df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "# Make predictions only for valid rows\n",
    "weather_input_l_df.loc[valid_rows, wfx_l_columns] = predict_wfx_l.predict_proba(weather_input_l_df.loc[valid_rows, specific_model_input_list].values)\n",
    "\n",
    "# Optionally, fill the missing predictions with NaN\n",
    "weather_input_l_df[wfx_l_columns] = weather_input_l_df[wfx_l_columns].astype(float)\n",
    "\n",
    "# Convert to PFX\n",
    "for event in events_list:\n",
    "    weather_input_l_df[f\"{event}_wfx_l\"] = weather_input_l_df[f\"{event}_wfx_l\"] / base_rate_df[event][0]\n",
    "\n",
    "# Generic. Always.\n",
    "# Predicted outputs\n",
    "generic_wfx_l_columns = [f\"{col}_generic_wfx_l\" for col in list(predict_wfx_l.classes_)]\n",
    "\n",
    "# Fill missings with column averages\n",
    "for col in weather_input_l_df[generic_model_input_list]:\n",
    "    average = weather_input_l_df[col].mean()\n",
    "    weather_input_l_df[col].fillna(average, inplace=True)\n",
    "\n",
    "# Make predictions\n",
    "weather_input_l_df[generic_wfx_l_columns] = predict_generic_wfx_l.predict_proba(weather_input_l_df[generic_model_input_list].values)\n",
    "\n",
    "# Convert to PFX\n",
    "for event in events_list:\n",
    "    weather_input_l_df[f\"{event}_generic_wfx_l\"] = weather_input_l_df[f\"{event}_generic_wfx_l\"] / base_rate_df[event][0]\n",
    "\n",
    "# Replace park-specific wfx estimates with generic ones if in generic_venue_list\n",
    "for event in events_list:\n",
    "    weather_input_l_df[f\"{event}_wfx_l\"] = np.where(weather_input_l_df['venue_id'].isin(generic_venue_list), weather_input_l_df[f\"{event}_generic_wfx_l\"], weather_input_l_df[f\"{event}_wfx_l\"])\n",
    "\n",
    "\n",
    "### RHB    \n",
    "# Specific, where available\n",
    "weather_input_r_df = pd.merge(meteo_df, r_park_latest_df.drop_duplicates('venue_id', keep='last').drop(columns={'gamePk', 'game_date'}), on=['venue_id'], suffixes=(\"\", \"_r\"), how='left')\n",
    "# Predicted outputs\n",
    "wfx_r_columns = [f\"{col}_wfx_r\" for col in list(predict_wfx_r.classes_)]\n",
    "\n",
    "# Create a mask for rows where there are no missing values in the required columns\n",
    "valid_rows = weather_input_r_df[specific_model_input_list].notna().all(axis=1)\n",
    "\n",
    "# Convert infinites to 0\n",
    "for col in specific_model_input_list:\n",
    "    weather_input_r_df[col] = weather_input_r_df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "# Make predictions only for valid rows\n",
    "weather_input_r_df.loc[valid_rows, wfx_r_columns] = predict_wfx_r.predict_proba(weather_input_r_df.loc[valid_rows, specific_model_input_list].values)\n",
    "\n",
    "# Optionally, fill the missing predictions with NaN\n",
    "weather_input_r_df[wfx_r_columns] = weather_input_r_df[wfx_r_columns].astype(float)\n",
    "\n",
    "# Convert to PFX\n",
    "for event in events_list:\n",
    "    weather_input_r_df[f\"{event}_wfx_r\"] = weather_input_r_df[f\"{event}_wfx_r\"] / base_rate_df[event][0]\n",
    "\n",
    "# Generic. Always.\n",
    "# Predicted outputs\n",
    "generic_wfx_r_columns = [f\"{col}_generic_wfx_r\" for col in list(predict_wfx_r.classes_)]\n",
    "\n",
    "# Fill missings with column averages\n",
    "for col in weather_input_r_df[generic_model_input_list]:\n",
    "    average = weather_input_r_df[col].mean()\n",
    "    weather_input_r_df[col].fillna(average, inplace=True)\n",
    "\n",
    "# Make predictions\n",
    "weather_input_r_df[generic_wfx_r_columns] = predict_generic_wfx_r.predict_proba(weather_input_r_df[generic_model_input_list].values)\n",
    "\n",
    "# Convert to PFX\n",
    "for event in events_list:\n",
    "    weather_input_r_df[f\"{event}_generic_wfx_r\"] = weather_input_r_df[f\"{event}_generic_wfx_r\"] / base_rate_df[event][0]\n",
    "\n",
    "# Replace park-specific wfx estimates with generic ones if in generic_venue_list\n",
    "for event in events_list:\n",
    "    weather_input_r_df[f\"{event}_wfx_r\"] = np.where(weather_input_r_df['venue_id'].isin(generic_venue_list), weather_input_r_df[f\"{event}_generic_wfx_r\"], weather_input_r_df[f\"{event}_wfx_r\"])\n",
    "\n",
    "\n",
    "\n",
    "# Combine LHB and RHB weather effects\n",
    "wfx_df = pd.concat([weather_input_l_df, weather_input_r_df[wfx_r_columns]], axis=1)\n",
    "\n",
    "# Clean date\n",
    "wfx_df['date'] = wfx_df['game_date'].str.replace(\"-\", \"\")\n",
    "# Rename\n",
    "wfx_df.rename(columns={'game_id': 'gamePk'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "wfx_df[['gamePk', 'game_date', 'date', 'game_num', 'away_team', 'home_team', 'venue_id', 'meteo_x_vect', 'meteo_y_vect', 'temperature_2m', 'weather', 'wind'] + wfx_l_columns + wfx_r_columns].to_csv(os.path.join(baseball_path, \"A06. Weather\", \"3. Park and Weather Factors\", f\"Park and Weather Factors {todaysdate}.csv\"), index=False, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342bfea-6955-48b1-ac51-c2aeb014b5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-base)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
