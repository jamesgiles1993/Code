{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8a8409-11cb-4b7c-bd45-02c7f7e4fbcc",
   "metadata": {},
   "source": [
    "# A04. Bullpens\n",
    "This scrapes bullpen depth charts via team websites, using the Wayback Machine for missing historic data\n",
    "- Type: Data\n",
    "- Run Frequency: Pre-contest\n",
    "- Sources:\n",
    "    - Team pages on mlb.com\n",
    "    - Wayback Machine (if necessary)\n",
    "- Dates:\n",
    "    - Created: 9/23/2023\n",
    "    - Updated: 4/21/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c53eef-e3ae-4700-82cc-105fc7d536de",
   "metadata": {},
   "source": [
    "Note: This relies upon the belief that these depth charts are sorted by descending leverage, which appears largely correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d91123-c902-47a8-a493-5cca5e546be2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774f7910-8e01-4637-a5e3-dee2ded92899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports executed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Functions.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    print(\"Imports executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb43ff-6441-431b-8974-b9d5dbea59d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9934418a-be1a-41bd-b271-f5289bd8ba89",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c062b5-71b8-4e8c-8ec2-15af758ee5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_df created.\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Set date range \n",
    "    start_date = \"20250319\"\n",
    "    end_date = \"20250319\"\n",
    "    all_game_df = pd.read_csv(os.path.join(baseball_path, \"game_df.csv\"))\n",
    "    game_df = all_game_df[(all_game_df['date'].astype(str) >= start_date) & (all_game_df['date'].astype(str) <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afb653-2a35-4e83-86a6-55f81b625ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b972131-cdbd-44c6-9904-c53e05ff73fb",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933acbe-c688-493b-9ed7-803e9211e7e3",
   "metadata": {},
   "source": [
    "##### Scrape Bullpen Data from MLB.com Depth Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deecafd-3c09-4450-9c85-a0d809331b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bullpen(mlburl, bbrefteam, historic=False, date=None):\n",
    "    if historic:\n",
    "        url = f\"https://web.archive.org/web/{date}/https://www.mlb.com/{mlburl}/roster/depth-chart\"\n",
    "    else:\n",
    "        url = f\"https://www.mlb.com/{mlburl}/roster/depth-chart\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Read tables with pandas\n",
    "    dfs = pd.read_html(StringIO(r.text), encoding='iso-8859-1')\n",
    "\n",
    "    # Use BeautifulSoup to get links\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    player_links = {}\n",
    "    for tag in soup.select('a[href*=\"/player/\"]'):\n",
    "        name = tag.get_text(strip=True)\n",
    "        href = tag['href']\n",
    "        if name:\n",
    "            # Remove numbers and (CL) just like we do below\n",
    "            cleaned_name = re.sub(r'\\(CL\\)|\\d+', '', name).strip()\n",
    "            cleaned_name = remove_accents(cleaned_name)\n",
    "            player_links[cleaned_name] = f\"https://www.mlb.com{href}\"\n",
    "\n",
    "    # Bullpen can be one of two tables\n",
    "    try:\n",
    "        df = dfs[2]\n",
    "        df = df[df[\"Bullpen.1\"].str.contains(\"IL-\") == False].reset_index(drop=True)\n",
    "        df = df[df[\"Bullpen.1\"].str.contains(\" Minors\") == False].reset_index(drop=True)\n",
    "    except:\n",
    "        df = dfs[1]\n",
    "        df = df[df[\"Bullpen.1\"].str.contains(\"IL-\") == False].reset_index(drop=True)\n",
    "        df = df[df[\"Bullpen.1\"].str.contains(\" Minors\") == False].reset_index(drop=True)\n",
    "\n",
    "    # Assume leverage = 0 by default\n",
    "    df['Leverage'] = 0\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            df.at[i, 'Leverage'] = 4\n",
    "        elif i < 4:\n",
    "            df.at[i, 'Leverage'] = 3\n",
    "        elif i < 11:\n",
    "            df.at[i, 'Leverage'] = 2\n",
    "\n",
    "    df.loc[df.index[-1], 'Leverage'] = 2\n",
    "    if 3 not in list(df['Leverage']):\n",
    "        df.loc[df.index[-2], 'Leverage'] = 3\n",
    "\n",
    "    # Extract name and B/T\n",
    "    df[['Name', 'drop']] = df['Bullpen.1'].str.split(\"B/T\", expand=True)\n",
    "    df['Name'] = df['Name'].str.replace(r'\\d+', '', regex=True)\n",
    "    df['Name'] = df['Name'].str.replace(r\"\\(CL\\)\", '', regex=True)\n",
    "    df['Name'] = df['Name'].apply(remove_accents).str.strip()\n",
    "\n",
    "    # Rebuild B/T column\n",
    "    df['B/T'] = df['drop'].str.extract(r'([LR]+/[LR]+)', expand=False)\n",
    "\n",
    "    # Add player URLs\n",
    "    df['URL'] = df['Name'].map(player_links)\n",
    "\n",
    "    # Extract player's MLB id from URL\n",
    "    df['id'] = df['URL'].str.split('/').str[-1]\n",
    "    \n",
    "    # Final columns\n",
    "    df = df[['Name', 'B/T', 'Leverage', 'URL', 'id']]\n",
    "    df['date'] = date\n",
    "    df['BBREFTEAM'] = bbrefteam\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd021b7-9f81-4ea7-bbcb-1ba3aad92274",
   "metadata": {},
   "source": [
    "##### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f435c8-dd70-44f6-8893-c1964bed2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bullpen dataframe (and csv)\n",
    "# Note: can write to csv in parallel but not to sql\n",
    "def bullpens(date, team_map, historic):    \n",
    "    # Create folder, if necessary\n",
    "    os.makedirs(os.path.join(baseball_path, 'A04. Bullpens', f\"Bullpens {date}\"), exist_ok=True)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Scrape bullpens\n",
    "    for i in range(len(team_map)):\n",
    "        # Extract team's website URL\n",
    "        mlburl = team_map['MLBURL'][i]\n",
    "        # Extract team's Baseball Reference abbreviation\n",
    "        bbrefteam = team_map['BBREFTEAM'][i]\n",
    "        # Scrape bullpens\n",
    "        bullpen_df = scrape_bullpen(mlburl, bbrefteam, historic, date)\n",
    "        # To CSV\n",
    "        bullpen_df.to_csv(os.path.join(baseball_path, \"A04. Bullpens\", f\"Bullpens {date}\", f\"Bullpen {bbrefteam} {date}.csv\"), index=False, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8141c-f06f-4f4a-b474-8cb3f60dc9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd41cf3-9079-4cdd-abf6-9c29f2615b31",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b896a-345c-44b2-bf34-31e7a7d52672",
   "metadata": {},
   "source": [
    "##### Scrape in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16873daa-d089-4f46-810c-b89730dd547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 172 ms\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "empty_list = Parallel(n_jobs=-1, verbose=0)(delayed(bullpens)(date=date, team_map=team_map, historic=len(game_df['date'].unique()) > 1) for date in list(game_df['date'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b715b-44d2-4209-86bb-6b85819f379a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
