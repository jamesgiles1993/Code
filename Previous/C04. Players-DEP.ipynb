{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f955d08b-7da5-4b5e-b23d-2ea450f0b242",
   "metadata": {},
   "source": [
    "# C04. Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c1bfeb-97ac-47e7-9a97-b71d1763d3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bf4c5-7a97-40d0-83ff-f33e68ad3ae2",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "- Compare my FP projections to actual FP scored\n",
    "    - Overall\n",
    "    - Lefty/Righty\n",
    "    - By Park/Weather\n",
    "    - By imputation status\n",
    "    - By starting pitcher status\n",
    "    - By year\n",
    "    - By projection decile (are low scores too low, are they too high?)\n",
    "- Compare my FP projections to other FP projections\n",
    "- Compare my scoring component projections to actual scoring component scoring (projected singles vs. actual singles, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f3978-c96e-4bbb-b4f0-913636611288",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf4e9c7-dfaa-45a6-b08e-0bdaeca641cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"20240318\"\n",
    "# start_date = yesterdaysdate\n",
    "end_date = yesterdaysdate\n",
    "# end_date = \"20240518\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5017b-4acb-4645-9349-3e9cbd5d2af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aaae491-d555-4264-b7fd-2d66a37aed60",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a32558-b12f-4bd6-994f-0175fe63b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = read_and_save_games(team_map, generate=True)\n",
    "game_df = game_df[(game_df['date'] >= start_date) & (game_df['date'] <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e690fe7-e131-42ac-b0e5-903b77907ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = list(game_df['date'].unique())\n",
    "date_folders = [f\"Matchups {date}\" for date in date_list]\n",
    "\n",
    "game_list = list(game_df['game_id'].unique())\n",
    "player_folders = [f\"Players {game}\" for game in game_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1dc61-4489-4e4a-98aa-1d866c2e8bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5367efe-b695-4c33-9bfe-a83091a4144c",
   "metadata": {},
   "source": [
    "### Create Player Stat Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fcb31-1a20-4634-8f5e-1cf5f411735d",
   "metadata": {},
   "source": [
    "Extract date, teams, and gamePk from folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7c7236-0ecd-4bca-b7e5-c8cf0833ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_folder(date_folder, matchup_folder):\n",
    "    # Extract date from date folder\n",
    "    date = date_folder.split(' ')[1]\n",
    "    \n",
    "    # Extract teams and gamePK from matchup folder\n",
    "    parts = matchup_folder.split(' ')\n",
    "    away_team, home_team = parts[0].split('@')\n",
    "    gamePk = parts[1]\n",
    "    \n",
    "    return date, away_team, home_team, gamePk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a35c55-e6b4-4e67-9437-eb166630fef4",
   "metadata": {},
   "source": [
    "Average player stats for a given position group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc35a2b9-2419-4580-ac3e-390753bc8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(date_folder, matchup_folder, position='pitchers'):\n",
    "    date, away_team, home_team, gamePk = extract_info_from_folder(date_folder, matchup_folder)\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Get a list of all CSV files in the matchup folder\n",
    "    csv_files = [file for file in os.listdir(os.path.join(baseball_path, \"B02. Simulations\", \"2. Player Sims\", date_folder, matchup_folder)) if file.startswith(position) and file.endswith('.csv')]\n",
    "    \n",
    "    # Iterate over each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(baseball_path, \"B02. Simulations\", \"2. Player Sims\", date_folder, matchup_folder, csv_file))\n",
    "        \n",
    "        # Append date, away_team, home_team, and gamePk columns\n",
    "        df['date'] = date\n",
    "        df['away_team'] = away_team\n",
    "        df['home_team'] = home_team\n",
    "        df['gamePk'] = gamePk\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list\n",
    "    combined_df = pd.concat(dfs)\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = combined_df.select_dtypes(include='number')\n",
    "    \n",
    "    # Group by fullName and calculate the mean for numeric columns\n",
    "    averaged_numeric_cols = numeric_cols.groupby(combined_df['fullName'], sort=False).mean()\n",
    "    \n",
    "    # Select team and additional columns\n",
    "    additional_cols = combined_df[['fullName', 'team', 'date', 'away_team', 'home_team', 'gamePk']].drop_duplicates().set_index('fullName')\n",
    "    \n",
    "    # Concatenate numeric and additional columns\n",
    "    averaged_df = pd.concat([additional_cols, averaged_numeric_cols], axis=1).reset_index()\n",
    "\n",
    "    averaged_df['team_abbrev'] = np.where(averaged_df['team'] == \"away\", averaged_df['away_team'], averaged_df['home_team'])\n",
    "\n",
    "    averaged_df['starter'] = (~averaged_df['team'].duplicated()).astype(int)\n",
    "\n",
    "    return averaged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac73fb0-c8c7-4b55-bd26-61a04a79f9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3603832-c19f-46b7-989c-8783829261e2",
   "metadata": {},
   "source": [
    "### Player Average Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42dfdd4-4ce5-4c2f-8200-150d5f302a8b",
   "metadata": {},
   "source": [
    "##### Batters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f65139-41dd-42e3-abfb-454bb093d650",
   "metadata": {},
   "source": [
    "Calculate averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d11d1-eae1-4b5d-bb92-81fe94f5d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "position = 'batters'\n",
    "\n",
    "# Parallelize the loop using joblib and directly return df_list\n",
    "batter_df_list = Parallel(n_jobs=-1)(\n",
    "    delayed(game_averages)(date_folder, matchup_folder, position) \n",
    "    for date_folder in date_folders \n",
    "    for matchup_folder in os.listdir(os.path.join(baseball_path, \"B02. Simulations\", \"2. Player Sims\", date_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a41563-ca02-4450-af4f-5e63891bbfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c058968b-bca8-432f-ac0f-e8b76628ff94",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2340be8-e6b1-47d9-874c-d0fcdbe1abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in batter_df_list:\n",
    "    gamePk = df['gamePk'][0]\n",
    "    away_df = df.query('team == \"away\"')\n",
    "    home_df = df.query('team == \"home\"')\n",
    "    \n",
    "    # Create folder\n",
    "    os.makedirs(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\"), exist_ok=True)\n",
    "\n",
    "    # Write to csv\n",
    "    away_df.to_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"away batters projections {gamePk}.csv\"), index=False)\n",
    "    home_df.to_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"home batters projections {gamePk}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84ee4d-70d3-4b9d-bbc5-5d7d223b2920",
   "metadata": {},
   "source": [
    "##### Pitchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9983e1-8db1-4304-8719-93ebb9573495",
   "metadata": {},
   "source": [
    "Calculate averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e3ecc-8e16-41a9-b15f-4d2e4ec9032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "position = 'pitchers'\n",
    "\n",
    "# Parallelize the loop using joblib and directly return df_list\n",
    "pitcher_df_list = Parallel(n_jobs=-1)(\n",
    "    delayed(game_averages)(date_folder, matchup_folder, position) \n",
    "    for date_folder in date_folders \n",
    "    for matchup_folder in os.listdir(os.path.join(baseball_path, \"B02. Simulations\", \"2. Player Sims\", date_folder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627337d-36d0-43d4-8d7a-b0abbb1c71dc",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4cf72-a02c-4b42-8704-7e0e2fc037fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in pitcher_df_list:\n",
    "    gamePk = df['gamePk'][0]\n",
    "    away_df = df.query('team == \"away\"')\n",
    "    home_df = df.query('team == \"home\"')\n",
    "    \n",
    "    # Create folder\n",
    "    os.makedirs(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\"), exist_ok=True)\n",
    "\n",
    "    # Write to csv\n",
    "    away_df.to_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"away pitchers projections {gamePk}.csv\"), index=False)\n",
    "    home_df.to_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"home pitchers projections {gamePk}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518db8a2-5896-494c-93c8-43e7ab2cd224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16b8c6b-6935-40e8-9a7b-86cf8f9f1493",
   "metadata": {},
   "source": [
    "### Evaluate Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42cc0a-9c0e-4c5a-843d-d511fbce1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process each folder\n",
    "def process_batters(folder):\n",
    "    # Extract gamePk\n",
    "    gamePk = folder.split(\" \")[1]\n",
    "\n",
    "    ### Batters\n",
    "    ## Away\n",
    "    # Read in projections\n",
    "    away_batter_projected_results_df = pd.read_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"away batters projections {gamePk}.csv\"))\n",
    "    # Read in results\n",
    "    away_batter_actual_results_df = pd.read_csv(os.path.join(baseball_path, \"A10. Player Results\", f\"Player Results {gamePk}\", f\"away batters {gamePk}.csv\"))\n",
    "\n",
    "    # Merge\n",
    "    away_batters_merged = away_batter_projected_results_df[['fullName', 'id', 'imp_b_l', 'imp_b_r', 'PA', 'HBP', 'BB', 'B1', 'B2', 'B3', 'HR', 'SB', 'R', 'RBI', 'FP', 'gamePk']].merge(away_batter_actual_results_df, left_on=['id', 'gamePk'], right_on=['personId', 'gamePk'], how='outer')\n",
    "\n",
    "    ## Home\n",
    "    # Read in projections\n",
    "    home_batter_projected_results_df = pd.read_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"home batters projections {gamePk}.csv\"))\n",
    "    # Read in results\n",
    "    home_batter_actual_results_df = pd.read_csv(os.path.join(baseball_path, \"A10. Player Results\", f\"Player Results {gamePk}\", f\"home batters {gamePk}.csv\"))\n",
    "\n",
    "    # Merge\n",
    "    home_batters_merged = home_batter_projected_results_df[['fullName', 'id', 'imp_b_l', 'imp_b_r', 'PA', 'HBP', 'BB', 'B1', 'B2', 'B3', 'HR', 'SB', 'R', 'RBI', 'FP', 'gamePk']].merge(home_batter_actual_results_df, left_on=['id', 'gamePk'], right_on=['personId', 'gamePk'], how='outer')\n",
    "\n",
    "    # Append them together\n",
    "    batters_merged = pd.concat([away_batters_merged, home_batters_merged], axis=0)\n",
    "\n",
    "    \n",
    "    return batters_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627e5fb-9bd9-4ec5-87b1-9b417f3b8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batters2(folder):\n",
    "    try:\n",
    "        batters_merged = process_batters(folder)    \n",
    "        return batters_merged   \n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153050e-6415-456d-89d7-7baf32c0e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the loop in parallel\n",
    "batters_merged_list = Parallel(n_jobs=-1)(delayed(process_batters2)(folder) for folder in player_folders)\n",
    "batters_merged_df = pd.concat(batters_merged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8224b-ab5f-408d-aa2b-3d1dea528077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1169c7-3fd3-4387-a887-fd2efb20d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pitchers(folder):\n",
    "    # Extract gamePk\n",
    "    gamePk = folder.split(\" \")[1]\n",
    "    \n",
    "    ### Pitchers\n",
    "    ## Away\n",
    "    # Read in projections\n",
    "    away_pitcher_projected_results_df = pd.read_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"away pitchers projections {gamePk}.csv\"))\n",
    "    away_pitcher_projected_results_df['team'] = \"away\"\n",
    "    # Read in results\n",
    "    away_pitcher_actual_results_df = pd.read_csv(os.path.join(baseball_path, \"A10. Player Results\", f\"Player Results {gamePk}\", f\"away pitchers {gamePk}.csv\"))\n",
    "\n",
    "    # Merge\n",
    "    away_pitchers_merged = away_pitcher_projected_results_df[['fullName', 'id', 'imp_p_l', 'imp_p_r', 'OUT', 'PA', 'SO', 'HBP', 'BB', 'B1', 'B2', 'B3', 'HR', 'H', 'R', 'ER', 'W', 'CG', 'CGSO', 'NH', 'FP', 'team', 'gamePk']].merge(away_pitcher_actual_results_df, left_on=['id', 'gamePk', 'team'], right_on=['personId', 'gamePk', 'team'], how='outer')\n",
    "    \n",
    "    ## Home\n",
    "    # Read in projections\n",
    "    home_pitcher_projected_results_df = pd.read_csv(os.path.join(baseball_path, \"C04. Players\", f\"Players {gamePk}\", f\"home pitchers projections {gamePk}.csv\"))\n",
    "    home_pitcher_projected_results_df['team'] = \"home\"\n",
    "    # Read in results\n",
    "    home_pitcher_actual_results_df = pd.read_csv(os.path.join(baseball_path, \"A10. Player Results\", f\"Player Results {gamePk}\", f\"home pitchers {gamePk}.csv\"))\n",
    "\n",
    "    # Merge\n",
    "    home_pitchers_merged = home_pitcher_projected_results_df[['fullName', 'id', 'imp_p_l', 'imp_p_r', 'OUT', 'PA', 'SO', 'HBP', 'BB', 'B1', 'B2', 'B3', 'HR', 'H', 'R', 'ER', 'W', 'CG', 'CGSO', 'NH', 'FP', 'team', 'gamePk']].merge(home_pitcher_actual_results_df, left_on=['id', 'gamePk', 'team'], right_on=['personId', 'gamePk', 'team'], how='outer')\n",
    "\n",
    "    # Append them together\n",
    "    pitchers_merged = pd.concat([away_pitchers_merged, home_pitchers_merged], axis=0)\n",
    "\n",
    "    \n",
    "    return pitchers_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147efe8-c1fe-459a-ac83-b42abb667305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pitchers2(folder):\n",
    "    try:\n",
    "        pitchers_merged = process_pitchers(folder)    \n",
    "        return pitchers_merged   \n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8caac1-fbde-4f5f-b9a0-50fd4d47b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the loop in parallel\n",
    "pitchers_merged_list = Parallel(n_jobs=-1)(delayed(process_pitchers2)(folder) for folder in player_folders)\n",
    "pitchers_merged_df = pd.concat(pitchers_merged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ac182-cab7-417a-91f9-2960cf2d009f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8acb422a-ff8b-4699-8180-408bafd5ae0d",
   "metadata": {},
   "source": [
    "### Batters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143c348-16cb-4dbd-b465-a2022bd89c90",
   "metadata": {},
   "source": [
    "##### Create New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169daf0-4788-400a-bf17-da3d815e8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual singles\n",
    "batters_merged_df['singles'] = batters_merged_df['h'] - batters_merged_df['doubles'] - batters_merged_df['triples'] - batters_merged_df['hr']\n",
    "# Actual PA\n",
    "batters_merged_df['pa'] = batters_merged_df[['ab', 'bb', 'hbp']].sum(axis=1)\n",
    "# Projected hits\n",
    "batters_merged_df['H'] = batters_merged_df[['B1', 'B2', 'B3', 'HR']].sum(axis=1)\n",
    "# Reached\n",
    "batters_merged_df['ON'] = batters_merged_df[['H', 'BB', 'HBP']].sum(axis=1)\n",
    "batters_merged_df['on'] = batters_merged_df[['h', 'bb', 'hbp']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87923c33-d5a8-4c10-b508-dbbae4c08a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify number of batters that batted in a given spot in the order (we may only want those who were never subbed out or are subs)\n",
    "batters_merged_df['battingSpot'] = batters_merged_df['battingOrder'] // 100\n",
    "batters_merged_df['battersSpot'] = batters_merged_df.groupby(['gamePk', 'team', 'battingSpot'])['battingSpot'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47ac7c-4503-48aa-98b6-e51b478525eb",
   "metadata": {},
   "source": [
    "##### Starters - Never Subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7f62b-1f7c-4933-8375-ecc230cf93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_batters = batters_merged_df.query('substitution == False').query('battersSpot == 1')[['PA', 'pa', 'ON', 'on', 'H', 'h', 'B1', 'singles', 'B2', 'doubles', 'B3', 'triples', 'HR', 'hr', 'BB', 'bb', 'SB', 'sb', 'FP', 'fp']].agg(['mean', 'sum'])\n",
    "\n",
    "# Split the dataframe into projected (even rows) and actual (odd rows)\n",
    "projected = starting_batters.T.iloc[::2].reset_index()\n",
    "actual = starting_batters.T.iloc[1::2].reset_index()\n",
    "\n",
    "# Concatenate the two dataframes side-by-side\n",
    "starting_batters = pd.concat([projected, actual], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "starting_batters.columns = [\"Projected\", \"Projected Mean\", 'Projected Sum', \"Actual\", \"Actual Mean\", 'Actual Sum']\n",
    "\n",
    "starting_batters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f13ef3-68fa-43a3-ac23-9d8cac53f900",
   "metadata": {},
   "source": [
    "##### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10013692-3f80-4bb1-9abf-b6397fa6104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_batters = batters_merged_df.groupby(['gamePk', 'team']).sum(numeric_only=True)[['PA', 'pa', 'ON', 'on', 'H', 'h', 'B1', 'singles', 'B2', 'doubles', 'B3', 'triples', 'HR', 'hr', 'BB', 'bb', 'HBP', 'hbp', 'R', 'r', 'RBI', 'rbi', 'SB', 'sb', 'FP', 'fp']].agg(['mean', 'sum'])\n",
    "\n",
    "# Split the dataframe into projected (even rows) and actual (odd rows)\n",
    "projected = team_batters.T.iloc[::2].reset_index()\n",
    "actual = team_batters.T.iloc[1::2].reset_index()\n",
    "\n",
    "# Concatenate the two dataframes side-by-side\n",
    "team_batters = pd.concat([projected, actual], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "team_batters.columns = [\"Projected\", \"Projected Mean\", 'Projected Sum', \"Actual\", \"Actual Mean\", 'Actual Sum']\n",
    "\n",
    "team_batters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc376b-07d3-4692-aff2-189aa3856e61",
   "metadata": {},
   "source": [
    "##### Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174638d3-0266-4f92-9b31-99db45a5f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_pa_mean = batters_merged_df.groupby(['gamePk', 'team']).sum(numeric_only=True)['pa'].mean()\n",
    "projected_pa_mean = batters_merged_df.groupby(['gamePk', 'team']).sum(numeric_only=True)['PA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445e054-a2be-416d-9c2e-aa14de0d646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fraction\n",
    "fraction = actual_pa_mean/projected_pa_mean\n",
    "\n",
    "# Select the columns you want to multiply and multiply them by the fraction\n",
    "columns_to_multiply = ['PA', 'H', 'B1', 'B2', 'B3', 'HR', 'BB', 'HBP', 'R', 'RBI', 'SB', 'FP']\n",
    "batters_merged_df_scaled = batters_merged_df.copy()\n",
    "batters_merged_df_scaled[columns_to_multiply] = batters_merged_df[columns_to_multiply] * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c85fb2-0fb6-4520-a8f5-b2da17e0f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_batters = batters_merged_df_scaled.groupby(['gamePk', 'team']).sum(numeric_only=True)[['PA', 'pa', 'ON', 'on', 'H', 'h', 'B1', 'singles', 'B2', 'doubles', 'B3', 'triples', 'HR', 'hr', 'BB', 'bb', 'HBP', 'hbp', 'R', 'r', 'RBI', 'rbi', 'SB', 'sb', 'FP', 'fp']]\n",
    "\n",
    "# Convert to DF\n",
    "scaled_batters = pd.DataFrame(scaled_batters.mean().reset_index())\n",
    "\n",
    "# Split the dataframe into projected (even rows) and actual (odd rows)\n",
    "projected = scaled_batters.iloc[::2].reset_index(drop=True)\n",
    "actual = scaled_batters.iloc[1::2].reset_index(drop=True)\n",
    "\n",
    "# Concatenate the two dataframes side-by-side\n",
    "scaled_batters = pd.concat([projected, actual], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "scaled_batters.columns = [\"Projected\", \"Projected Value\", \"Actual\", \"Actual Value\"]\n",
    "\n",
    "scaled_batters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a4553-83fc-4b85-af01-99577e2dd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider tracking if players were never removed from game\n",
    "# Consider merging on innings and only looking at full games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b8abe-0b53-4ae8-a99f-6b417dc96c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032a240e-fca1-408e-8aad-23704fd61882",
   "metadata": {},
   "source": [
    "### Pitchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3fc010-1fb5-4fd1-9aa0-f79c049d11f1",
   "metadata": {},
   "source": [
    "##### Create New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2e99a-955e-468b-8f48-65ed03a852cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_merged_df.sort_values(['gamePk', 'team', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37ed30-fe27-497c-93d8-caa92da21f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_merged_df['personId'].fillna(pitchers_merged_df['id'], inplace=True)\n",
    "pitchers_merged_df['name'].fillna(pitchers_merged_df['fullName'], inplace=True)\n",
    "\n",
    "for col in ['starter', 'ip', 'outs', 'h', 'r', 'er', 'bb', 'k', 'hr', 'hbp', 'w', 'l', 'cg', 'cgso', 'nh', 'fp']:\n",
    "    pitchers_merged_df[col].fillna(0, inplace=True)\n",
    "    \n",
    "for col in ['date', 'year', 'venue_id', 'team', 'teamabbrev']:\n",
    "    # pitchers_merged_df.sort_values(['date', 'year', 'venue_id', 'team', 'teamabbrev'], ascending=False, inplace=True)\n",
    "    pitchers_merged_df[col].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c13e4-6b8d-45b8-a607-427815487f3e",
   "metadata": {},
   "source": [
    "##### Starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3136e4-bbf8-4d6c-9714-c2a0dffc51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_pitchers = pitchers_merged_df.dropna().query('starter == 1')[['OUT', 'outs', 'PA', 'pa', 'ER', 'er', 'R', 'r', 'H', 'h', 'SO', 'k', 'W', 'w', 'FP', 'fp']].agg(['mean', 'sum'])\n",
    "\n",
    "# Split the dataframe into projected (even rows) and actual (odd rows)\n",
    "projected = starting_pitchers.T.iloc[::2].reset_index()\n",
    "actual = starting_pitchers.T.iloc[1::2].reset_index()\n",
    "\n",
    "# Concatenate the two dataframes side-by-side\n",
    "starting_pitchers = pd.concat([projected, actual], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "starting_pitchers.columns = [\"Projected\", \"Projected Mean\", 'Projected Sum', \"Actual\", \"Actual Mean\", 'Actual Sum']\n",
    "\n",
    "starting_pitchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f2746-a929-4c3b-9410-685692e5274d",
   "metadata": {},
   "source": [
    "##### Scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5874f-2344-4e95-ad1a-e2084b329612",
   "metadata": {},
   "source": [
    "This calculates how starting pitchers would do if they went as long as they were supposed to. <br>\n",
    "Note: outs should have a nonlinear relationship with wins, so this won't be exactly right, but close enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4187db7-f81a-4bb2-8660-d196f2187b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_outs_mean = pitchers_merged_df.dropna().query('starter == 1')['outs'].mean()\n",
    "projected_outs_mean = pitchers_merged_df.dropna().query('starter == 1')['OUT'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecf2bc-6f7c-4195-9c09-cd90ca5a7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fraction\n",
    "fraction = actual_outs_mean/projected_outs_mean\n",
    "\n",
    "# Select the columns you want to multiply and multiply them by the fraction\n",
    "columns_to_multiply = ['OUT', 'ER', 'R', 'SO', 'FP']\n",
    "pitchers_merged_df_scaled = pitchers_merged_df.copy()\n",
    "pitchers_merged_df_scaled[columns_to_multiply] = pitchers_merged_df_scaled[columns_to_multiply] * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa8ff6-02c8-4c2a-97eb-1b7519894f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the DataFrame\n",
    "pitchers_scaled = pitchers_merged_df_scaled.dropna().query('starter == 1')[['OUT', 'outs', 'PA', 'pa', 'ER', 'er', 'R', 'r', 'SO', 'k', 'W', 'w', 'FP', 'fp']].agg(['mean', 'sum'])\n",
    "\n",
    "# Split the dataframe into projected (even rows) and actual (odd rows)\n",
    "projected = pitchers_scaled.T.iloc[::2].reset_index()\n",
    "actual = pitchers_scaled.T.iloc[1::2].reset_index()\n",
    "\n",
    "# Concatenate the two dataframes side-by-side\n",
    "pitchers_scaled = pd.concat([projected, actual], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "pitchers_scaled.columns = [\"Projected\", \"Projected Mean\", 'Projected Sum', \"Actual\", \"Actual Mean\", 'Actual Sum']\n",
    "\n",
    "pitchers_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49551f7e-2b11-437d-be15-908eb873260e",
   "metadata": {},
   "source": [
    "### Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd2d12-c4b5-4f0e-af39-2fc15ebc6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_pitchers = pitchers_merged_df.groupby(['gamePk', 'team']).sum(numeric_only=True).query('outs >= 24')[['OUT', 'outs', 'PA', 'pa', 'ER', 'er', 'R', 'r', 'SO', 'k', 'H', 'h', 'BB', 'bb', 'HR', 'hr', 'FP', 'fp']].agg(['mean'])\n",
    "\n",
    "# Convert to DF\n",
    "scaled_pitchers = pd.DataFrame(scaled_pitchers.mean().reset_index())\n",
    "\n",
    "# Split the dataframe into projected (even rows) and actual (odd rows)\n",
    "projected = scaled_pitchers.iloc[::2].reset_index(drop=True)\n",
    "actual = scaled_pitchers.iloc[1::2].reset_index(drop=True)\n",
    "\n",
    "# Concatenate the two dataframes side-by-side\n",
    "scaled_pitchers = pd.concat([projected, actual], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "scaled_pitchers.columns = [\"Projected\", \"Projected Value\", \"Actual\", \"Actual Value\"]\n",
    "\n",
    "scaled_pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bcfca-3b83-4103-81ce-8942e846f4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a68c51f-f70b-4bd5-bf30-db00bcb55e20",
   "metadata": {},
   "source": [
    "### Read in projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cefabe-7bae-41a4-978c-0dd6903126f4",
   "metadata": {},
   "source": [
    "##### DFF - Date-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383aef2-ebda-4263-97f7-97ba21278579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = r'C:\\Users\\james\\Documents\\MLB\\Database\\A07. Projections\\1. DFF\\2. Projections\\Date'\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV file\n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dfs.append(df)  # Append the dataframe to the list\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "dff_date_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop duplicates based on \"First Name\", \"Last Name\", and \"date\"\n",
    "dff_date_df.drop_duplicates(subset=[\"first_name\", \"last_name\", \"game_date\"], inplace=True)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(dff_date_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf2bd7-c88b-43f6-bf38-25b02d91a647",
   "metadata": {},
   "source": [
    "##### DFF - Slate-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935b2d3-7025-4d25-84ff-6008d4f51709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = r'C:\\Users\\james\\Documents\\MLB\\Database\\A07. Projections\\1. DFF\\2. Projections'\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV file\n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dfs.append(df)  # Append the dataframe to the list\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "dff_slate_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop duplicates based on \"First Name\", \"Last Name\", and \"date\"\n",
    "dff_slate_df.drop_duplicates(subset=[\"First Name\", \"Last Name\", \"date\"], inplace=True)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(dff_slate_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536ba6c-76ac-4e15-9b75-fe34e1253280",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_date_df.rename(columns={'first_name':'First Name', 'last_name':'Last Name', 'ppg_projection':'FP', 'team':'Team'}, inplace=True)\n",
    "dff_date_df['date'] = dff_date_df['game_date'].str.replace(\"-\", \"\").astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a4e2d-658a-4a3c-b18c-94a9f888f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_df = pd.concat([dff_date_df[['First Name', 'Last Name', 'Team', 'FP', 'date']], dff_slate_df[['First Name', 'Last Name', 'FP', 'Team', 'date']]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175d04e-ac81-4df4-beee-4764c92abe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_df['fullName'] = dff_df['First Name'] + \" \" + dff_df['Last Name']\n",
    "dff_df.rename(columns={'FP': 'FP_DFF'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db032e99-6240-4efe-a854-2650a587126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_df.drop_duplicates(['fullName', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68911a70-335b-4556-99f6-1d0b9bf9140c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec8cb30e-289b-4b01-8ba7-e3baf1f8b85f",
   "metadata": {},
   "source": [
    "##### Batters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd175e48-be5e-47fe-bb1b-4211452f0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "batters_with_dff = batters_merged_df.drop_duplicates(['fullName', 'date']).merge(dff_df, on=['fullName', 'date'], how='inner', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82cdcb-367b-437a-93a8-0e4c7f4b4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "batters_with_dff['error_me'] = (batters_with_dff['fp'] - batters_with_dff['FP'])\n",
    "batters_with_dff['error_dff'] = (batters_with_dff['fp'] - batters_with_dff['FP_DFF'])\n",
    "\n",
    "batters_with_dff['error_me2'] = batters_with_dff['error_me'] ** 2\n",
    "batters_with_dff['error_dff2'] = batters_with_dff['error_dff'] ** 2\n",
    "\n",
    "batters_with_dff['beat_dff'] = (batters_with_dff['error_me2'] < batters_with_dff['error_dff2']).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e9cf4-b0d9-45c1-896f-aa2074881c64",
   "metadata": {},
   "source": [
    "##### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97ff26-d464-4031-807d-e36871096e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batters_with_dff.query('FP > 5 and FP_DFF > 5')[['FP', 'FP_DFF', 'fp', 'error_me', 'error_dff', 'error_me2', 'error_dff2', 'beat_dff']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e606dc-af02-4c47-b2dd-8482dba9a63c",
   "metadata": {},
   "source": [
    "##### Winsorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a67fe3-30fe-451d-999d-627e195ceffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize the DataFrame\n",
    "batters_winsorized = batters_with_dff.query('FP > 5 and FP_DFF > 5')[['FP', 'FP_DFF', 'fp', 'error_me', 'error_dff', 'error_me2', 'error_dff2', 'beat_dff']].apply(lambda x: winsorize(x, limits=[0.05, 0.05]))\n",
    "\n",
    "# Describe the winsorized DataFrame\n",
    "batters_winsorized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006db2-e84a-4753-a40d-710c7b14fa5a",
   "metadata": {},
   "source": [
    "##### Pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2474a2-fa31-4956-81b0-cae99a96cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_with_dff = pitchers_merged_df.drop_duplicates(['fullName', 'date']).merge(dff_df, on=['fullName', 'date'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203631e3-9e4a-426b-92ab-e5f33a084c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_with_dff['error_me'] = (pitchers_with_dff['fp'] - pitchers_with_dff['FP'])\n",
    "pitchers_with_dff['error_dff'] = (pitchers_with_dff['fp'] - pitchers_with_dff['FP_DFF'])\n",
    "\n",
    "pitchers_with_dff['error_me2'] = pitchers_with_dff['error_me'] ** 2\n",
    "pitchers_with_dff['error_dff2'] = pitchers_with_dff['error_dff'] ** 2\n",
    "\n",
    "pitchers_with_dff['beat_dff'] = (pitchers_with_dff['error_me2'] < pitchers_with_dff['error_dff2']).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b1166-e292-4870-a743-1bab050657b8",
   "metadata": {},
   "source": [
    "##### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8eb7cc-a230-435b-b41d-73f0b0e45aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_with_dff.dropna().query('starter == 1')[['FP', 'FP_DFF', 'fp', 'error_me', 'error_dff', 'error_me2', 'error_dff2', 'beat_dff']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489f228-aaab-4e5f-ada2-63011e1646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_with_dff.query('FP > 10').dropna().query('starter == 1')[['FP', 'FP_DFF', 'fp', 'error_me', 'error_dff', 'error_me2', 'error_dff2', 'beat_dff']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83882015-af01-4040-bc26-5e78bb55ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pitchers_with_dff.query('starter == 1').dropna()\n",
    "\n",
    "# Bucketing the data based on intervals of 0.5 for FP\n",
    "bucket_size = 2\n",
    "FP_bucketed = np.floor(df['FP'] / bucket_size) * bucket_size\n",
    "\n",
    "# Calculating the average fp for each bucket\n",
    "grouped_data = df.groupby(FP_bucketed)['fp'].mean()\n",
    "\n",
    "# Getting the center of each bucket\n",
    "bucket_centers = (grouped_data.index + bucket_size / 2)\n",
    "\n",
    "# Creating scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(bucket_centers, grouped_data, color='blue')\n",
    "plt.title('Average fp vs FP (Bucketed)')\n",
    "plt.xlabel('FP')\n",
    "plt.ylabel('Average fp')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the same intervals on each side from -10 to 60\n",
    "plt.xlim(-10, 60)\n",
    "plt.ylim(-10, 60)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014335e0-103f-4177-a934-a780370a8b8d",
   "metadata": {},
   "source": [
    "##### Winsorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881e370-97c7-43b7-a251-dabeaf9faf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize the DataFrame\n",
    "pitchers_winsorized = pitchers_with_dff.query('starter == 1')[['FP', 'FP_DFF', 'fp', 'error_me', 'error_dff', 'error_me2', 'error_dff2', 'beat_dff']].apply(lambda x: winsorize(x, limits=[0.05, 0.05]))\n",
    "\n",
    "# Describe the winsorized DataFrame\n",
    "pitchers_winsorized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb39bbc-784d-49cc-a7c3-1706be032b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821c528-ad1c-4657-9640-d576be528d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab9d49-a21b-4050-9a83-a798a2af4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRERASLK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81116ed5-1067-4062-8129-da3d6b77cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebeb38-9f11-42fd-94b4-e2b4efb99a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = batters_merged_df[~batters_merged_df['FP'].isna()][['fullName', 'imp_b_l', 'imp_b_r', 'FP', 'fp']]\n",
    "test['overproject'] = (test['FP'] > test['fp']).astype(int)\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d555f-f4e7-41ec-8cb8-c15563690e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('imp_b_l')['overproject'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac4acb-6ded-4337-bec9-ccb50e9b85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('imp_b_r')['overproject'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22883a9-34b3-4647-b424-1cc815410124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "# test = pd.DataFrame(...)\n",
    "\n",
    "# Create buckets of 0.1 for FP\n",
    "test['FP_bucket'] = np.floor(test['FP'] / 0.2) * 0.2\n",
    "\n",
    "# Group by the buckets and calculate the mean of fp for each bucket\n",
    "grouped = test.groupby('FP_bucket')['fp'].mean().reset_index()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 8))  # Make the plot square\n",
    "plt.scatter(grouped['FP_bucket'], grouped['fp'], label='Data points')\n",
    "plt.xlabel('FP (bucketed)')\n",
    "plt.ylabel('Average fp')\n",
    "plt.title('Average fp for each 0.1 FP bucket')\n",
    "\n",
    "# Set the aspect ratio to be equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Ensure the limits of the axes are the same\n",
    "min_val = min(grouped['FP_bucket'].min(), grouped['fp'].min())\n",
    "max_val = max(grouped['FP_bucket'].max(), grouped['fp'].max())\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Plot the 45-degree line\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='45-degree line')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555ef1a-823c-4d21-89de-be02a108a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Sample DataFrame\n",
    "# test = pd.DataFrame(...)\n",
    "\n",
    "# Perform OLS regression with intercept set to 0\n",
    "slope, intercept, r_value, p_value, std_err = linregress(test['FP'], test['fp'])\n",
    "\n",
    "# Since we want the intercept to be 0, we calculate the slope directly without using linregress\n",
    "slope = np.sum(test['FP'] * test['fp']) / np.sum(test['FP']**2)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Print the summary\n",
    "print(\"OLS Regression Summary (with intercept = 0):\")\n",
    "print(f\"Slope: {slope:.4f}\")\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Standard Error: {std_err:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfa3e3-bf82-45bf-8163-ab730048f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "# Assuming you have a DataFrame named 'df' with columns 'FP' and 'overproject'\n",
    "\n",
    "# Create buckets of 10 for FP\n",
    "test['FP_bucket'] = np.floor(test['FP'] / 1) * 1\n",
    "\n",
    "# Group by the buckets and calculate the mean of overproject for each bucket\n",
    "grouped = test.groupby('FP_bucket')['overproject'].mean().reset_index()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(grouped['FP_bucket'], grouped['overproject'])\n",
    "plt.xlabel('FP (bucketed)')\n",
    "plt.ylabel('Average overproject')\n",
    "plt.title('Average overproject as FP increases')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0b88b-64ca-4bb5-ad8f-329724e9aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_test = pitchers_merged_df[~pitchers_merged_df['FP'].isna()].query('FP > 8')[['fullName', 'imp_p_l', 'imp_p_r', 'FP', 'fp']]\n",
    "pitcher_test['overproject'] = (pitcher_test['FP'] > pitcher_test['fp']).astype(int)\n",
    "pitcher_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de22fa-a9a8-48ae-b227-0c47f0aebfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "# Assuming you have a DataFrame named 'df' with columns 'FP' and 'overproject'\n",
    "\n",
    "# Create buckets of 10 for FP\n",
    "pitcher_test['FP_bucket'] = np.floor(pitcher_test['FP'] / 2) * 2\n",
    "\n",
    "# Group by the buckets and calculate the mean of overproject for each bucket\n",
    "grouped = pitcher_test.groupby('FP_bucket')['overproject'].mean().reset_index()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(grouped['FP_bucket'], grouped['overproject'])\n",
    "plt.xlabel('FP (bucketed)')\n",
    "plt.ylabel('Average overproject')\n",
    "plt.title('Average overproject as FP increases')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71417382-da6b-4785-be13-e35ae73665b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
