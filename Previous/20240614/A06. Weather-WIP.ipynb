{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f14ae0-6bcd-495d-92cb-2eda9f54d663",
   "metadata": {},
   "source": [
    "# A06. Weather\n",
    "This scrapes extracts batting orders and rosters\n",
    "- Type: Data\n",
    "- Run Frequency: Pre-contest\n",
    "- Sources:\n",
    "    - Swish Analytics\n",
    "    - RotoGrinders\n",
    "    - Ballpark Pal (deprecated) \n",
    "- Dates:\n",
    "    - Created: 9/23/2023\n",
    "    - Updated: 4/21/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974f09f3-ee38-4de3-953b-e54f73583f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports-WIP.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fe878-dd80-4ce2-8308-8798454520b2",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6817ec-6b9d-46d5-8127-674f2698842f",
   "metadata": {},
   "source": [
    "##### Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b89d212-d742-473b-8a85-8fb095599d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reverses winds so that they're named for where they're going, not where they're from. This is so vectors make more sense logically.\n",
    "def wind_reverser(direction):\n",
    "    direction = direction.replace(\"N\", \"s\")\n",
    "    direction = direction.replace(\"S\", \"n\")\n",
    "    direction = direction.replace(\"E\", \"w\")\n",
    "    direction = direction.replace(\"W\", \"e\")\n",
    "    \n",
    "    return direction.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebf5e88-0e82-4b1e-9bf6-4c28664ce2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates number of degrees for each direction\n",
    "def find_degree(direction):\n",
    "    if direction == \"N\":\n",
    "        degree = 0\n",
    "    elif direction == \"NNE\":\n",
    "        degree = 1\n",
    "    elif direction == \"NE\":\n",
    "        degree = 2\n",
    "    elif direction == \"ENE\":\n",
    "        degree = 3\n",
    "    elif direction == \"E\":\n",
    "        degree = 4\n",
    "    elif direction == \"ESE\":\n",
    "        degree = 5\n",
    "    elif direction == \"SE\":\n",
    "        degree = 6\n",
    "    elif direction == \"SSE\":\n",
    "        degree = 7\n",
    "    elif direction == \"S\":\n",
    "        degree = 8\n",
    "    elif direction == \"SSW\":\n",
    "        degree = 9\n",
    "    elif direction == \"SW\":\n",
    "        degree = 10\n",
    "    elif direction == \"WSW\":\n",
    "        degree = 11\n",
    "    elif direction == \"W\":\n",
    "        degree = 12\n",
    "    elif direction == \"WNW\":\n",
    "        degree = 13\n",
    "    elif direction == \"NW\":\n",
    "        degree = 14\n",
    "    elif direction == \"NNW\":\n",
    "        degree = 15\n",
    "        \n",
    "    degree = degree * 22.5 \n",
    "\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da62f051-0720-4938-89a0-18f3b905b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the x and y vectors given the park's orientation and the wind's direction\n",
    "def calculate_vectors(row):\n",
    "    # Determines degree of centerfield\n",
    "    park_angle = find_degree(row['CF'])\n",
    "    # Determine degree of wind\n",
    "    row['Direction'] = wind_reverser(row['Direction'])\n",
    "    wind_angle = find_degree(row['Direction']) \n",
    "    \n",
    "    # Determine angle between them\n",
    "    angle = wind_angle - park_angle \n",
    "\n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row['Speed']\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row['Speed']\n",
    "\n",
    "    return x_vect, y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff49e30-fa37-4633-b8a0-697049387004",
   "metadata": {},
   "source": [
    "##### Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26c37ef-173a-427d-8096-6a528150420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Swish Analytics for weather date\n",
    "def swishanalytics(date):\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    # Swish Analytics URL \n",
    "    url = \"https://swishanalytics.com/mlb/weather?date=\" + date_dash\n",
    "\n",
    "     # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class 'weather-card'\n",
    "        weather_cards = soup.find_all('div', class_='weather-card')\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dfs = []\n",
    "        \n",
    "        # Iterate over each weather card\n",
    "        for weather_card in weather_cards:\n",
    "            # Extract relevant information from the weather card\n",
    "            time_info = weather_card.find('small', class_='text-muted')\n",
    "            location_info = weather_card.find('h4', class_='lato inline vert-mid bold')\n",
    "            \n",
    "            # Extract time and location information\n",
    "            time = time_info.text.strip() if time_info else None\n",
    "            location = location_info.text.strip() if location_info else None\n",
    "            \n",
    "            # Find the table within the weather card\n",
    "            table = weather_card.find('table', class_='table-bordered')\n",
    "            \n",
    "            # If table exists, extract data from it\n",
    "            if table:\n",
    "                # Extract table data into a list of lists\n",
    "                rows = table.find_all('tr')\n",
    "                data = []\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    row_data = [cell.text.strip() for cell in cells]\n",
    "                    data.append(row_data)\n",
    "                \n",
    "                # Convert data into a pandas DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Set the first row as the column headers\n",
    "                df.columns = df.iloc[0]\n",
    "                df = df[1:]  # Remove the first row since it's the header row\n",
    "                \n",
    "                # Add time and location as additional columns\n",
    "                df['Time'] = time\n",
    "                df['Location'] = location\n",
    "\n",
    "                # Create dataframem from the second time period scraped\n",
    "                daily_weather_df = pd.DataFrame(df.iloc[:, 2]).T\n",
    "                # Extract home team name \n",
    "                daily_weather_df['Matchup'] = df['Location'][1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['Matchup'].str.split(\"@\", expand=True).iloc[:, 1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['FANGRAPHSTEAM'].str.replace(\"\\xa0\\xa0\", \"\")\n",
    "\n",
    "                dfs.append(daily_weather_df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    # Append together dataframes\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "\n",
    "    # Identify CF\n",
    "    df = df.merge(team_map[['FANGRAPHSTEAM', 'BBREFTEAM', 'CF']], on='FANGRAPHSTEAM', how='left')\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={1:'Weather', 2:'temperature', 3:'Feels Like', 4:'Humidity', 5:'Speed', 6:'Direction', 'BBREFTEAM': 'home_team'}, inplace=True)\n",
    "\n",
    "    # Remove mph\n",
    "    df['Speed'] = df['Speed'].str.replace(\" mph\", \"\").astype(float)\n",
    "    df['temperature'] = df['temperature'].str.replace('°', '')\n",
    "    df['Feels Like'] = df['Feels Like'].str.replace('°', '')\n",
    "    \n",
    "    # Apply the calculate_vectors function row-wise and assign results to new columns\n",
    "    df[['x_vect', 'y_vect']] = df.apply(calculate_vectors, axis=1, result_type='expand')\n",
    "    \n",
    "    \n",
    "    return df[['Matchup', 'home_team', 'Weather', 'Feels Like', 'Humidity', 'Speed', 'Direction', 'FANGRAPHSTEAM', 'CF', 'temperature', 'x_vect', 'y_vect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0f36-7de4-4fca-bf77-d01ebf46d99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a38159b-2394-43fd-bde5-19d0a6066d43",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ce9bd1-c9b5-480a-ae74-dad7b25ccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotogrinders(date, team_map):\n",
    "    # URL of the web page containing the table\n",
    "    url = \"https://rotogrinders.com/weather/mlb\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Get the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = soup.find_all(\"li\", class_=\"weather-blurb\")\n",
    "\n",
    "        # Create an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        for li_element in li_elements:\n",
    "            # Extract the tag colors from the <span> elements\n",
    "            tag_elements = li_element.find_all(\"span\", class_=[\"green\", \"yellow\", \"orange\", \"red\"])\n",
    "        \n",
    "            # Extract the first tag color\n",
    "            tag = tag_elements[0].text.strip() if tag_elements else None\n",
    "        \n",
    "            # Extract the second tag color if it exists\n",
    "            tag2 = tag_elements[1].text.strip() if len(tag_elements) > 1 else None\n",
    "        \n",
    "            # Extract the matchup from the <span> element with class \"bold\"\n",
    "            matchup_span = li_element.find(\"span\", class_=\"bold\")\n",
    "            matchup = matchup_span.text.strip() if matchup_span else None\n",
    "        \n",
    "            # Extract the description if it exists\n",
    "            if matchup_span:\n",
    "                description_span = matchup_span.find_next_sibling(\"span\")\n",
    "                description = description_span.text.strip() if description_span else None\n",
    "            else:\n",
    "                description = None\n",
    "        \n",
    "            # Append the data to the list\n",
    "            data.append({\"Tag\": tag, \"Tag2\": tag2, \"Matchup\": matchup, \"Description\": description})\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        df[['away', 'home']] = df['Matchup'].str.split(\" @ \", expand=True)\n",
    "\n",
    "        # Add in DK team abbreviations \n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['away'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_away\"))\n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['home'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_home\"))\n",
    "        df = df[['Tag', 'Tag2', 'Matchup', 'DKTEAM', 'DKTEAM_home', 'Description']]\n",
    "        df.rename(columns={'DKTEAM':'Away', 'DKTEAM_home': 'Home'}, inplace=True)\n",
    "        \n",
    "        # Add the date column to the DataFrame\n",
    "        df['date'] = date\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        # Return an error message if the response is not successful\n",
    "        return \"Failed to retrieve data. Response status code: {}\".format(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc553503-e074-4b83-919b-66663c3532cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b34212e4-ff7f-4a99-8504-703b7e756a10",
   "metadata": {},
   "source": [
    "##### 3. Ballpark Pal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fa60b-f1e0-43b4-a4cc-2a622b8aff57",
   "metadata": {},
   "source": [
    "This is not currently supported after Ballpark Pal switch to subscription-only in 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca7e1861-70a6-42f9-add3-06352f94c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Ballpark Pal for weather factors\n",
    "# Note: these factor in park as well and are relative to league average conditions\n",
    "def ballparkpal(date):\n",
    "    headers = {\n",
    "    'Accept': 'text/html',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    url = f'https://ballparkpal.com/ParkFactors.php?date={date_dash}'\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        tree = html.fromstring(response.text)\n",
    "        table_elements = tree.xpath('/html/body/div[1]/table')\n",
    "\n",
    "        if table_elements:\n",
    "            table_element = table_elements[0]\n",
    "\n",
    "            # Extract table rows\n",
    "            rows = table_element.xpath('.//tr')\n",
    "\n",
    "            # Extract table header (assuming it's the first row)\n",
    "            header = [th.text_content().strip() for th in rows[0].xpath('.//th')]\n",
    "\n",
    "            # Extract table data\n",
    "            data = []\n",
    "            for row in rows[1:]:\n",
    "                row_data = [td.text_content().strip() for td in row.xpath('.//td')]\n",
    "                data.append(row_data)\n",
    "\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "            \n",
    "            # Extracting the parts of the 'Game' column\n",
    "            df['Park'] = df['Game'].str.extract(r'^(.*?)\\s\\d{1,2}:\\d{2}')\n",
    "            df['Time'] = df['Game'].str.extract(r'(\\d{1,2}:\\d{2})')\n",
    "            df['Away'] = df['Game'].str.extract(r'\\d{1,2}:\\d{2}(.*?)@')\n",
    "            df['Away'] = df['Away'].str.strip()\n",
    "            df['Home'] = df['Game'].str.extract(r'@ (.*)$')\n",
    "\n",
    "            # Converting percentage columns to decimals\n",
    "            cols_to_convert = ['HR', '2B/3B', '1B', 'Runs']\n",
    "            for col in cols_to_convert:\n",
    "                df[col] = df[col].str.rstrip('%').astype(float) / 100\n",
    "\n",
    "            # Drop the 'Game' column since it's no longer needed\n",
    "            df.drop('Game', axis=1, inplace=True)\n",
    "            \n",
    "            df = df[['Park', 'Time', 'Away', 'Home', 'HR', '2B/3B', '1B', 'Runs']]\n",
    "            \n",
    "            df['GameNum'] = df.groupby('Park').cumcount() + 1\n",
    "            \n",
    "        else:\n",
    "            print(\"No table found at the specified XPath.\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    df['date'] = date\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58c49f-ff98-488b-bd59-362ee43b2c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24f72896-f1fd-4441-9b20-d14ec616c99c",
   "metadata": {},
   "source": [
    "##### 4. Park and Weather Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e5387e3-91a8-41ac-ac27-828355aa3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for create_box\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\A02. MLB API-WIP.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c8d7a6b-0680-48fa-8f52-a6267cf91ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_and_weather_factors(game_df, multiplier_dataset, period_avg_df, swish_df=None, date=todaysdate, overwrite_year=None, historic=False):\n",
    "    print(date)\n",
    "    # Extract daily games\n",
    "    daily_game_df = game_df.query(f'date == \"{date}\"').reset_index(drop=True)\n",
    "    daily_game_df.sort_values('game_datetime', inplace=True)\n",
    "    daily_game_df['game_num'] = daily_game_df.groupby('home_team').cumcount() + 1\n",
    "\n",
    "    # If we have Swish weather data, merge it in\n",
    "    if swish_df is not None:\n",
    "        # Add game num\n",
    "        swish_df['game_num'] = swish_df.groupby('home_team').cumcount() + 1\n",
    "        daily_game_df = daily_game_df.merge(swish_df, on=['home_team', 'game_num'], how='left')\n",
    "\n",
    "    \n",
    "    daily_weather_list = []\n",
    "    # Loop over games\n",
    "    for i in range(len(daily_game_df)):\n",
    "        # Extract relevant information\n",
    "        game_id = daily_game_df['game_id'][i]\n",
    "        venue_id = daily_game_df['venue_id'][i]\n",
    "        away_team = daily_game_df['away_team'][i]\n",
    "        home_team = daily_game_df['home_team'][i]\n",
    "        datetime = daily_game_df['game_datetime'][i]\n",
    "        date = daily_game_df['date'][i]\n",
    "        year = daily_game_df['year'][i]\n",
    "\n",
    "        # If we have Swish Analytics data, extract it\n",
    "        if swish_df is not None:\n",
    "            temperature = daily_game_df['temperature'][i]\n",
    "            x_vect = daily_game_df['x_vect'][i].astype(float)\n",
    "            y_vect = daily_game_df['y_vect'][i].astype(float)\n",
    "        \n",
    "        # Extract weather from box score\n",
    "        weather, wind, park, full_date, missing_weather = create_box(game_id)\n",
    "        \n",
    "        # Create game weather dataframe\n",
    "        game_weather_dictionary = {\n",
    "            'game_id': game_id,\n",
    "            'away_team': away_team,\n",
    "            'home_team': home_team,\n",
    "            'venue_id': venue_id,\n",
    "            'park': park,\n",
    "            'datetime': datetime,\n",
    "            'date': date,\n",
    "            'year': year,\n",
    "            'weather': weather,\n",
    "            'wind': wind\n",
    "        }\n",
    "        game_weather_df = pd.DataFrame(game_weather_dictionary, index=[0])\n",
    "        game_weather_df = clean_weather(game_weather_df)\n",
    "\n",
    "        # If we do not have MLB Stats API data yet, use Swish Analytics\n",
    "        if missing_weather == True:\n",
    "            print(f\"{away_team}@{home_team} uses Swish Analytics weather.\")\n",
    "            game_weather_df['temperature'] = float(temperature)\n",
    "            game_weather_df['x_vect'] = x_vect\n",
    "            game_weather_df['y_vect'] = y_vect\n",
    "            game_weather_df['windDirection'] = \"Predicted\"\n",
    "        \n",
    "        daily_weather_list.append(game_weather_df)\n",
    "\n",
    "    # Create day weather dataframe\n",
    "    daily_weather_df = pd.concat(daily_weather_list, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Overwrite year (good option if early in season, but may want to develop a rolling method)\n",
    "    if overwrite_year is not None:\n",
    "        daily_weather_df['year'] = overwrite_year\n",
    "    \n",
    "    # Columns to keep\n",
    "    keep_columns = list(daily_weather_df.columns)\n",
    "    \n",
    "    # Add park dummies\n",
    "    active_venues = list(team_map['VENUE_ID'])\n",
    "    active_venues = [str(venue) for venue in active_venues]\n",
    "    for park in active_venues:\n",
    "        daily_weather_df[f'venue_{park}'] = (daily_weather_df['venue_id'].astype(str) == park).astype(int)\n",
    "    active_venue_columns = [f\"venue_{park}\" for park in active_venues]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Loop over lefty/righty dummy\n",
    "    for lefty_dummy in [0,1]:\n",
    "        # Assign lefty dummy\n",
    "        daily_weather_df['lefty'] = lefty_dummy\n",
    "\n",
    "        if lefty_dummy == 0:\n",
    "            side = 'r'\n",
    "        else:\n",
    "            side = 'l'\n",
    "\n",
    "        # Step 2: Create interaction terms\n",
    "        weather_interactions = []\n",
    "        \n",
    "        # Weather x Park\n",
    "        for col1 in active_venue_columns:\n",
    "            for col2 in ['x_vect', 'y_vect', 'temperature']:\n",
    "                interaction_name = col1 + '_' + col2\n",
    "                daily_weather_df[interaction_name] = daily_weather_df[col1] * daily_weather_df[col2]\n",
    "                weather_interactions.append(interaction_name)\n",
    "    \n",
    "    # If it's historic, \n",
    "    if historic == True:\n",
    "        # We already have the multiplier\n",
    "        multiplier_columns = [column for column in multiplier_dataset.columns if \"mult\" in column]\n",
    "        multiplier_dataset.rename(columns={'gamePk':'game_id'}, inplace=True)\n",
    "\n",
    "        # So keep them\n",
    "        daily_weather_df = pd.merge(daily_weather_df, multiplier_dataset[['game_id'] + multiplier_columns], on=['game_id'], how='left')\n",
    "\n",
    "    \n",
    "    \n",
    "    # If it's the day of,\n",
    "    else:\n",
    "        # We need to calculate it from the league averages, park factors, and league environment (period averages)\n",
    "        league_avg_columns = [column for column in multiplier_dataset.columns if \"league\" in column]\n",
    "        factor_columns = [column for column in multiplier_dataset.columns if \"factor\" in column]\n",
    "        \n",
    "        keep_columns = ['venue_id'] + league_avg_columns + factor_columns\n",
    "\n",
    "        # Keep most recent game at each venue\n",
    "        last_game_df = multiplier_dataset.drop_duplicates('venue_id', keep='last')[keep_columns]\n",
    "\n",
    "        # Merge that onto the weather\n",
    "        daily_weather_df = pd.merge(daily_weather_df, last_game_df, on=['venue_id'], how='left')\n",
    "\n",
    "\n",
    "        ### Calculate multipliers\n",
    "        # Loop over events\n",
    "        for event in events_list:\n",
    "            # Loop over sides\n",
    "            for side in ['l', 'r']:\n",
    "                # Select model      \n",
    "                model = globals().get(f'{event}_{side}_model')\n",
    "            \n",
    "                # Assign long-term average to average of team's stats for predicting a team-agnostic rate \n",
    "                daily_weather_df[f'{event}_b_long'] = period_avg_df[event][0]\n",
    "                daily_weather_df[f'{event}_p_long'] = period_avg_df[event][0]\n",
    "\n",
    "                daily_weather_df[f'{event}_league'] = daily_weather_df[f'{event}_league_{side}']\n",
    "                daily_weather_df[f'{event}_factor'] = daily_weather_df[f'{event}_factor_{side}']\n",
    "\n",
    "\n",
    "                # Model prediction inputs\n",
    "                X = daily_weather_df[[f'{event}_b_long', f'{event}_p_long', f'{event}_league', f'{event}_factor'] + weather_interactions]\n",
    "                X = sm.add_constant(X, has_constant='add')\n",
    "\n",
    "                # Predict probability of event\n",
    "                daily_weather_df[f'predicted_{event}'] = model.predict(X)\n",
    "                # Calculate multiplier\n",
    "                daily_weather_df[f'{event}_mult_{side}'] = daily_weather_df[f'predicted_{event}'] / period_avg_df[event][0]\n",
    "            \n",
    "    \n",
    "    # Keep relevent variables\n",
    "    daily_weather_df = daily_weather_df[['game_id', 'away_team', 'home_team', 'venue_id', 'park', 'datetime', 'date', 'year', \n",
    "                                         'weather', 'wind', 'temperature', 'windSpeed', 'windDirection', 'x_vect', 'y_vect'] + \n",
    "                                         [f'{event}_mult_l' for event in events_list] + \n",
    "                                         [f'{event}_mult_r' for event in events_list]]\n",
    "\n",
    "    \n",
    "    # # Set park factors to 1 if venue_id is not among active parks    \n",
    "    # # Check if venue_id is not present in team_map['VENUE_ID']\n",
    "    # not_in_team_map = ~daily_weather_df['venue_id'].isin(team_map['VENUE_ID'].astype(int))\n",
    "    \n",
    "    # # Set values to 1 for columns in event_factors_l and event_factors_r where the condition is met\n",
    "    # for column in event_factors_l + event_factors_r:\n",
    "    #     # Set values to 1 where the condition is met\n",
    "    #     daily_weather_df.loc[not_in_team_map, column] = 1\n",
    "\n",
    "\n",
    "    return daily_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82b6573d-3d32-43ac-878c-b152bf308409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = \"20220407\"\n",
    "# end_date = todaysdate\n",
    "# game_df = read_and_save_games(team_map, generate=True)\n",
    "# game_df = game_df[(game_df['date'] >= start_date) & (game_df['date'] <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5716dec5-449f-4185-b69e-76846bcb484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplier_dataset = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))\n",
    "# period_avg_df = pd.read_csv(os.path.join(baseball_path, \"Period Averages.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "695f3612-07c0-431e-b948-fd36936833ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220407\n",
      "20220408\n",
      "20220409\n",
      "20220410\n",
      "20220411\n",
      "20220412\n",
      "20220413\n",
      "20220414\n",
      "20220415\n",
      "20220416\n",
      "20220417\n",
      "20220418\n",
      "20220419\n",
      "20220420\n",
      "20220421\n",
      "20220422\n",
      "20220423\n",
      "20220424\n",
      "20220425\n",
      "20220426\n",
      "20220427\n",
      "20220428\n",
      "20220429\n",
      "20220430\n",
      "20220501\n",
      "20220502\n",
      "20220503\n",
      "20220504\n",
      "20220505\n",
      "20220506\n",
      "20220507\n",
      "20220508\n",
      "20220509\n",
      "20220510\n",
      "20220511\n",
      "20220512\n",
      "20220513\n",
      "20220514\n",
      "20220515\n",
      "20220516\n",
      "20220517\n",
      "20220518\n",
      "20220519\n",
      "20220520\n",
      "20220521\n",
      "20220522\n",
      "20220523\n",
      "20220524\n",
      "20220525\n",
      "20220526\n",
      "20220527\n",
      "20220528\n",
      "20220529\n",
      "20220530\n",
      "20220531\n",
      "20220601\n",
      "20220602\n",
      "20220603\n",
      "20220604\n",
      "20220605\n",
      "20220606\n",
      "20220607\n",
      "20220608\n",
      "20220609\n",
      "20220610\n",
      "20220611\n",
      "20220612\n",
      "20220613\n",
      "20220614\n",
      "20220615\n",
      "20220616\n",
      "20220617\n",
      "20220618\n",
      "20220619\n",
      "20220620\n",
      "20220621\n",
      "20220622\n",
      "20220623\n",
      "20220624\n",
      "20220625\n",
      "20220626\n",
      "20220627\n",
      "20220628\n",
      "20220629\n",
      "20220630\n",
      "20220701\n",
      "20220702\n",
      "20220703\n",
      "20220704\n",
      "20220705\n",
      "20220706\n",
      "20220707\n",
      "20220708\n",
      "20220709\n",
      "20220710\n",
      "20220711\n",
      "20220712\n",
      "20220713\n",
      "20220714\n",
      "20220715\n",
      "20220716\n",
      "20220717\n",
      "20220721\n",
      "20220722\n",
      "20220723\n",
      "20220724\n",
      "20220725\n",
      "20220726\n",
      "20220727\n",
      "20220728\n",
      "20220729\n",
      "20220730\n",
      "20220731\n",
      "20220801\n",
      "20220802\n",
      "20220803\n",
      "20220804\n",
      "20220805\n",
      "20220806\n",
      "20220807\n",
      "20220808\n",
      "20220809\n",
      "20220810\n",
      "20220811\n",
      "20220812\n",
      "20220813\n",
      "20220814\n",
      "20220815\n",
      "20220816\n",
      "20220817\n",
      "20220818\n",
      "20220819\n",
      "20220820\n",
      "20220821\n",
      "20220822\n",
      "20220823\n",
      "20220824\n",
      "20220825\n",
      "20220826\n",
      "20220827\n",
      "20220828\n",
      "20220829\n",
      "20220830\n",
      "20220831\n",
      "20220901\n",
      "20220902\n",
      "20220903\n",
      "20220904\n",
      "20220905\n",
      "20220906\n",
      "20220907\n",
      "20220908\n",
      "20220909\n",
      "20220910\n",
      "20220911\n",
      "20220912\n",
      "20220913\n",
      "20220914\n",
      "20220915\n",
      "20220916\n",
      "20220917\n",
      "20220918\n",
      "20220919\n",
      "20220920\n",
      "20220921\n",
      "20220922\n",
      "20220923\n",
      "20220924\n",
      "20220925\n",
      "20220926\n",
      "20220927\n",
      "20220928\n",
      "20220929\n",
      "20220930\n",
      "20221001\n",
      "20221002\n",
      "20221003\n",
      "20221004\n",
      "20221005\n",
      "20221007\n",
      "20221008\n",
      "20221009\n",
      "20221011\n",
      "20221012\n",
      "20221013\n",
      "20221014\n",
      "20221015\n",
      "20221016\n",
      "20221018\n",
      "20221019\n",
      "20221020\n",
      "20221021\n",
      "20221022\n",
      "20221023\n",
      "20221028\n",
      "20221029\n",
      "20221101\n",
      "20221102\n",
      "20221103\n",
      "20221105\n",
      "20230330\n",
      "20230331\n",
      "20230401\n",
      "20230402\n",
      "20230403\n",
      "20230404\n",
      "20230405\n",
      "20230406\n",
      "20230407\n",
      "20230408\n",
      "20230409\n",
      "20230410\n",
      "20230411\n",
      "20230412\n",
      "20230413\n",
      "20230414\n",
      "20230415\n",
      "20230416\n",
      "20230417\n",
      "20230418\n",
      "20230419\n",
      "20230420\n",
      "20230421\n",
      "20230422\n",
      "20230423\n",
      "20230424\n",
      "20230425\n",
      "20230426\n",
      "20230427\n",
      "20230428\n",
      "20230429\n",
      "20230430\n",
      "20230501\n",
      "20230502\n",
      "20230503\n",
      "20230504\n",
      "20230505\n",
      "20230506\n",
      "20230507\n",
      "20230508\n",
      "20230509\n",
      "20230510\n",
      "20230511\n",
      "20230512\n",
      "20230513\n",
      "20230514\n",
      "20230515\n",
      "20230516\n",
      "20230517\n",
      "20230518\n",
      "20230519\n",
      "20230520\n",
      "20230521\n",
      "20230522\n",
      "20230523\n",
      "20230524\n",
      "20230525\n",
      "20230526\n",
      "20230527\n",
      "20230528\n",
      "20230529\n",
      "20230530\n",
      "20230531\n",
      "20230601\n",
      "20230602\n",
      "20230603\n",
      "20230604\n",
      "20230605\n",
      "20230606\n",
      "20230607\n",
      "20230608\n",
      "20230609\n",
      "20230610\n",
      "20230611\n",
      "20230612\n",
      "20230613\n",
      "20230614\n",
      "20230615\n",
      "20230616\n",
      "20230617\n",
      "20230618\n",
      "20230619\n",
      "20230620\n",
      "20230621\n",
      "20230622\n",
      "20230623\n",
      "20230624\n",
      "20230625\n",
      "20230626\n",
      "20230627\n",
      "20230628\n",
      "20230629\n",
      "20230630\n",
      "20230701\n",
      "20230702\n",
      "20230703\n",
      "20230704\n",
      "20230705\n",
      "20230706\n",
      "20230707\n",
      "20230708\n",
      "20230709\n",
      "20230714\n",
      "20230715\n",
      "20230716\n",
      "20230717\n",
      "20230718\n",
      "20230719\n",
      "20230720\n",
      "20230721\n",
      "20230722\n",
      "20230723\n",
      "20230724\n",
      "20230725\n",
      "20230726\n",
      "20230727\n",
      "20230728\n",
      "20230729\n",
      "20230730\n",
      "20230731\n",
      "20230801\n",
      "20230802\n",
      "20230803\n",
      "20230804\n",
      "20230805\n",
      "20230806\n",
      "20230807\n",
      "20230808\n",
      "20230809\n",
      "20230810\n",
      "20230811\n",
      "20230812\n",
      "20230813\n",
      "20230814\n",
      "20230815\n",
      "20230816\n",
      "20230817\n",
      "20230818\n",
      "20230819\n",
      "20230820\n",
      "20230821\n",
      "20230822\n",
      "20230823\n",
      "20230824\n",
      "20230825\n",
      "20230826\n",
      "20230827\n",
      "20230828\n",
      "20230829\n",
      "20230830\n",
      "20230831\n",
      "20230901\n",
      "20230902\n",
      "20230903\n",
      "20230904\n",
      "20230905\n",
      "20230906\n",
      "20230907\n",
      "20230908\n",
      "20230909\n",
      "20230910\n",
      "20230911\n",
      "20230912\n",
      "20230913\n",
      "20230914\n",
      "20230915\n",
      "20230916\n",
      "20230917\n",
      "20230918\n",
      "20230919\n",
      "20230920\n",
      "20230921\n",
      "20230922\n",
      "20230923\n",
      "20230924\n",
      "20230925\n",
      "20230926\n",
      "20230927\n",
      "20230928\n",
      "20230929\n",
      "20230930\n",
      "20231001\n",
      "20231002\n",
      "20231003\n",
      "20231004\n",
      "20231007\n",
      "20231008\n",
      "20231009\n",
      "20231010\n",
      "20231011\n",
      "20231012\n",
      "20231015\n",
      "20231016\n",
      "20231017\n",
      "20231018\n",
      "20231019\n",
      "20231020\n",
      "20231021\n",
      "20231022\n",
      "20231023\n",
      "20231024\n",
      "20231027\n",
      "20231028\n",
      "20231030\n",
      "20231031\n",
      "20231101\n",
      "20240320\n",
      "20240321\n",
      "20240328\n",
      "20240329\n",
      "20240330\n",
      "20240331\n",
      "20240401\n",
      "20240402\n",
      "20240403\n",
      "20240404\n",
      "20240405\n",
      "20240406\n",
      "20240407\n",
      "20240408\n",
      "20240409\n",
      "20240410\n",
      "20240411\n",
      "20240412\n",
      "20240413\n",
      "20240414\n",
      "20240415\n",
      "20240416\n",
      "20240417\n",
      "20240418\n",
      "20240419\n",
      "20240420\n",
      "20240421\n",
      "20240422\n",
      "20240423\n",
      "20240424\n",
      "20240425\n",
      "20240426\n",
      "20240427\n",
      "20240428\n",
      "20240429\n",
      "20240430\n",
      "20240501\n",
      "20240502\n",
      "20240503\n",
      "20240504\n",
      "20240505\n",
      "20240506\n",
      "20240507\n",
      "20240508\n",
      "20240509\n",
      "20240510\n",
      "20240511\n",
      "20240512\n",
      "20240513\n",
      "20240514\n",
      "20240515\n",
      "20240516\n",
      "20240517\n",
      "20240518\n",
      "20240519\n",
      "20240520\n",
      "20240521\n",
      "20240522\n",
      "20240523\n",
      "20240524\n",
      "20240525\n",
      "20240526\n",
      "20240527\n",
      "20240528\n",
      "20240529\n",
      "20240530\n",
      "20240531\n",
      "20240601\n",
      "20240602\n",
      "20240603\n",
      "20240604\n",
      "20240605\n",
      "20240606\n",
      "20240607\n",
      "20240608\n",
      "20240609\n",
      "20240610\n",
      "20240611\n"
     ]
    }
   ],
   "source": [
    "# for date in game_df['date'].unique():\n",
    "#     daily_weather_df = park_and_weather_factors(game_df, multiplier_dataset, period_avg_df, swish_df=None, date=date, overwrite_year=None, historic=True)\n",
    "#     daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"4. Park and Weather Factors\", f\"Park and Weather Factors {date}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68226338-ef09-4b91-8736-be8a99c737b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
