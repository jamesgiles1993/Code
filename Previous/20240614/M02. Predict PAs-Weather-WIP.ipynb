{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246dcb35-a0f2-4ce2-a46e-f6b8c1053bf2",
   "metadata": {},
   "source": [
    "# M02. Predict PAs\n",
    "- This predicts the outcome of plate appearances\n",
    "- Type: Model\n",
    "- Run Frequency: Irregular\n",
    "- Sources:\n",
    "    - MLB API\n",
    "    - Steamer\n",
    "- Dates:\n",
    "    - Created: 4/19/2024\n",
    "    - Updated: 4/21/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72339d03-2fb3-4067-97eb-994324eb3c21",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534de56-4550-4dce-99fc-73d526095998",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"U1. Imports-WIP.ipynb\"\n",
    "%run \"U2. Utilities.ipynb\"\n",
    "%run \"U3. Classes.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007683e-abe8-4fa1-bc35-8ecdd3ee63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import log_loss, classification_report, f1_score, make_scorer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from tensorflow import keras\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329e039-ddf8-46ac-ade2-bcd519279b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"A02. MLB API-WIP.ipynb\"\n",
    "%run \"A03. Steamer.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afe9b9-2ede-417d-873a-acfb5e5e7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display numbers without scientific notation\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6014a-eb27-4083-9537-1ec7e127de15",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957792e-8584-487f-866c-1a3d2754ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = create_pa_inputs(park_factors, team_map, 2015, 2024, short=50, long=300, adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078dd9c-3106-4c48-8975-ae70f76e7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need this\n",
    "multiplier_dataset = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee27de4-ce5b-47c5-a4cf-c830466c46c8",
   "metadata": {},
   "source": [
    "##### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52508861-52f1-4d58-8c14-f481006a1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "complete_dataset[batter_inputs] = batter_stats_scaler.transform(complete_dataset[batter_inputs])\n",
    "complete_dataset[pitcher_inputs] = pitcher_stats_scaler.transform(complete_dataset[pitcher_inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af98da-c1d1-43f0-a38d-7992b6b96f0a",
   "metadata": {},
   "source": [
    "### Steamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194d026-dbb6-43c7-aed1-0ba7f2dcc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Steamer hitters \n",
    "steamer_hitters_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_hitters_weekly_log.csv\"), encoding='iso-8859-1')\n",
    "# Clean\n",
    "steamer_hitters_df2 = clean_steamer_hitters(steamer_hitters_df)\n",
    "steamer_hitters_df2.dropna(subset=batter_stats_fg, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc05c0e-a762-4dd1-b624-0a3eef35f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Steamer hitters \n",
    "steamer_pitchers_df = pd.read_csv(os.path.join(baseball_path, \"A03. Steamer\", \"steamer_pitchers_weekly_log.csv\"), encoding='iso-8859-1')\n",
    "# Clean\n",
    "steamer_pitchers_df2 = clean_steamer_pitchers(steamer_pitchers_df)\n",
    "steamer_pitchers_df2.dropna(subset=pitcher_stats_fg2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96034fe3-b3ea-4798-a317-14644903ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "steamer_hitters_df2[batter_stats_fg] = batter_stats_fg_scaler.transform(steamer_hitters_df2[batter_stats_fg])\n",
    "steamer_pitchers_df2[pitcher_stats_fg] = pitcher_stats_fg_scaler.transform(steamer_pitchers_df2[pitcher_stats_fg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115569b-2eb1-4689-976b-2bec2de41de2",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74aac58-6b72-4d70-992a-234693ddd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates of Steamer projections\n",
    "# We'll take the most recent and merge in that projection for each player\n",
    "batter_steamer_dates = list(steamer_hitters_df2['date'].unique())\n",
    "pitcher_steamer_dates = list(steamer_pitchers_df2['date'].unique())\n",
    "\n",
    "# Define a function to find the largest number in \"steamer_dates\" less than or equal to a given \"date\"\n",
    "def find_steamer_date(date, steamer_dates):\n",
    "    max_steamer_date = max(filter(lambda d: d <= date, steamer_dates), default=None)\n",
    "    return max_steamer_date\n",
    "\n",
    "# Apply the function to create the \"steamer_date\" column in your DataFrame\n",
    "complete_dataset[\"batter_date\"] = complete_dataset[\"date\"].apply(lambda x: find_steamer_date(x, batter_steamer_dates))\n",
    "complete_dataset[\"pitcher_date\"] = complete_dataset[\"date\"].apply(lambda x: find_steamer_date(x, pitcher_steamer_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de79d10-e324-4580-be44-eb38ba412b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steamer stats we want to keep\n",
    "batter_stats_fg_plus = ['mlbamid', 'steamerid', 'date'] + batter_stats_fg \n",
    "pitcher_stats_fg_plus = ['mlbamid', 'steamerid', 'date'] + pitcher_stats_fg \n",
    "\n",
    "# Merge\n",
    "complete_merged_df = pd.merge(complete_dataset, steamer_hitters_df2[batter_stats_fg_plus], left_on=['batter', 'batter_date'], right_on=['mlbamid', 'date'], how='inner')\n",
    "complete_merged_df = pd.merge(complete_merged_df, steamer_pitchers_df2[pitcher_stats_fg_plus], left_on=['pitcher', 'pitcher_date'], right_on=['mlbamid', 'date'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ace11-7229-4b30-bf8f-d39d3d778dc5",
   "metadata": {},
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e123af-6012-4391-b253-5d6aadd9d5c5",
   "metadata": {},
   "source": [
    "##### Option 1: Steamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f4ca2-ffde-4601-90d1-44826dc538f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hands to use in imputation\n",
    "batter_stats_fg_imp = batter_stats_fg + ['b_L', 'p_L']\n",
    "pitcher_stats_fg_imp = pitcher_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "### Batters\n",
    "# Use Steamer stats to predict API/Statcast stats for those with limited samples\n",
    "batter_predictions = batter_imputations_model.predict(complete_merged_df.loc[complete_merged_df['pa_b'] < 40, batter_stats_fg_imp])\n",
    "\n",
    "# Impute inputs with limited sample size with predicted values\n",
    "complete_merged_df.loc[complete_merged_df['pa_b'] < 40, batter_inputs] = batter_predictions\n",
    "\n",
    "### Pitchers\n",
    "# Use Steamer stats to predict API/Statcast stats for those with limited samples\n",
    "pitcher_predictions = pitcher_imputations_model.predict(complete_merged_df.loc[complete_merged_df['pa_p'] < 40, pitcher_stats_fg_imp])\n",
    "\n",
    "# Impute inputs with limited sample size with predicted values\n",
    "complete_merged_df.loc[complete_merged_df['pa_p'] < 40, pitcher_inputs] = pitcher_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95462618-9051-4892-a8ff-89c721fcfd94",
   "metadata": {},
   "source": [
    "##### Option 2: 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6348bdb-6a17-4c4f-9e5f-96ca3143a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing instead of imputing, just weighting with 0s\n",
    "# complete_merged_df[batter_inputs].fillna(0, inplace=True)\n",
    "# complete_merged_df[pitcher_inputs].fillna(0, inplace=True)\n",
    "\n",
    "# # Calculate the weighted average for each column in pitcher_stats\n",
    "# # Could be simplified, but I wanted to show the steps\n",
    "# # Weighted average of provided value and 0. PAs and 50-PAs are weights. \n",
    "# for col in batter_inputs:\n",
    "#     complete_merged_df[col] = (complete_merged_df[col] * complete_merged_df['pa_b'] + 0 * (50-complete_merged_df['pa_b']))/50\n",
    "\n",
    "# # Calculate the weighted average for each column in pitcher_stats\n",
    "# for col in pitcher_inputs:\n",
    "#     complete_merged_df[col] = (complete_merged_df[col] * complete_merged_df['pa_p'] + 0 * (50-complete_merged_df['pa_p']))/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb0e2b-955e-41da-a0d8-4ad4522b0b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ddb37c-4952-4f64-8adf-2041ee2d1fb4",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bd072-5b6a-4fd0-a1e8-a6684523b347",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304246a9-c7ee-4c4d-8783-95b85f601287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter out events that didn't end with reaching base or an out\n",
    "# complete_merged_df = complete_merged_df.query('eventsModel != \"Cut\"').reset_index(drop=True)\n",
    "# Drop early observations (these will generally treat veterans as rookies and could bias results\n",
    "complete_merged_df = complete_merged_df.drop(index=complete_merged_df.index[:20000])\n",
    "complete_merged_df.reset_index(inplace=True, drop=True)\n",
    "# Create year variable\n",
    "complete_merged_df['year'] = complete_merged_df['date'].astype('str').str[:4]\n",
    "# Create is_out binary variable\n",
    "out_list = ['so', 'fo', 'go', 'lo', 'po']\n",
    "complete_merged_df['is_out'] = complete_merged_df['eventsModel'].str.contains('|'.join(out_list)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490495-8064-4813-863c-332e5b072e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba807729-953d-4d62-b48c-f51fed92e79e",
   "metadata": {},
   "source": [
    "### Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8924b9-4704-4da5-877a-b948a37054ac",
   "metadata": {},
   "source": [
    "##### Create Park/Weather Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a04477-10c0-453d-bc19-989e60cba10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep active parks (move this up)\n",
    "active_parks = list(team_map['VENUE_ID'].astype(int))\n",
    "complete_merged_df = complete_merged_df[complete_merged_df['venue_id'].astype(int).isin(active_parks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22d581-466c-4575-ae3f-3dfce3e15fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lefty dummy\n",
    "complete_merged_df['lefty'] = (complete_merged_df['batSide'] == \"L\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39d546-742f-4a3a-b37f-44e73c8cb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactions of weather and park variables\n",
    "weather_interactions = []\n",
    "\n",
    "for venue in team_map['VENUE_ID']:\n",
    "    for weather in ['x_vect', 'y_vect', 'temperature']:\n",
    "        complete_merged_df[f'venue_{venue}_{weather}'] = complete_merged_df[f'venue_{venue}'] * complete_merged_df[weather]\n",
    "        weather_interactions.append(f'venue_{venue}_{weather}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1617541-f916-4c83-a5b1-2fff03b7f792",
   "metadata": {},
   "source": [
    "##### Multiplier Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b011c2b-b0b7-446e-bc1f-a1893183bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    # Assign multiplier for their \n",
    "    complete_merged_df[f'{event}_mult'] = np.where(complete_merged_df['batSide'] == \"L\", complete_merged_df[f'{event}_mult_l'], complete_merged_df[f'{event}_mult_r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60285c38-348b-49d2-aa90-6b5c51fe5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_inputs = [f'{event}_mult' for event in events_list]\n",
    "# park_inputs = [f'{event}_park' for event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd433755-b3a5-4b6f-bebc-bd01174512ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59b41d-1891-4f1d-9206-e3d9469d92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add league - this is incorporated in mult and should therefore be unnecessary\n",
    "# Add park?? - should be redundant but doesn't seem to be reflected well enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6d65b-2dc5-4532-9507-0b9b1a1d3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_inputs2 = batter_inputs + pitcher_inputs + hand_inputs + game_state_inputs + imp_inputs + starter_inputs + multiplier_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782873c-ad76-4007-b6f4-b8a683b52b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_inputs2 = pa_inputs2 + ["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b9d1f-25fe-4ae2-8144-d73a99d3d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep relevant variables\n",
    "keep_list = pa_inputs2 + ['pa_b', 'pa_p', 'year', 'venue_id', 'is_out', 'eventsModel', 'batterName', 'pitcherName']\n",
    "model_dataset = complete_merged_df[keep_list]\n",
    "model_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf781d0f-ba06-4dc8-a82b-157ad1fea64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset = model_dataset[model_dataset['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()\n",
    "safe_dataset = model_dataset[~model_dataset['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fce27-c7a7-4d62-86b2-264bf6ecbab1",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c40b4-c182-4bdc-acc8-aca444b6bfba",
   "metadata": {},
   "source": [
    "##### Option 1: Early/Late Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1ffac-682b-47fd-b3cb-62f0965b47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = model_dataset.groupby(model_dataset['year']).apply(lambda x: x.head(int(len(x)*2/3)))\n",
    "# X_test = model_dataset.groupby(model_dataset['year']).apply(lambda x: x.tail(int(len(x)*1/3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac7e46-d8e6-43b6-bb73-ff52ac8123aa",
   "metadata": {},
   "source": [
    "##### Option 2: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21610309-387e-4a06-ad4b-63111f5c67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create a column 'split' with random values to achieve a 2/3 to 1/3 split\n",
    "model_dataset['split'] = np.random.choice([0, 0, 1], size=len(model_dataset))\n",
    "\n",
    "# Drop if missing information\n",
    "model_dataset.dropna(subset=pa_inputs2, inplace=True)\n",
    "model_dataset.columns = model_dataset.columns.astype(str)\n",
    "\n",
    "# Split the DataFrame into training and testing sets based on the 'split' column\n",
    "X_train = model_dataset[model_dataset['split'] == 0].drop('split', axis=1)\n",
    "X_test = model_dataset[model_dataset['split'] == 1].drop('split', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f894519-3614-4045-839f-aeb4c55ec76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset_train = X_train[X_train['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()\n",
    "safe_dataset_train = X_train[~X_train['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fbbcc-924e-4b06-a197-815283a6b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset_test = X_test[X_test['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()\n",
    "safe_dataset_test = X_test[~X_test['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb30e14-a7d8-4368-a39b-bca912c1d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del complete_merged_df, complete_dataset, model_dataset, steamer_hitters_df, steamer_hitters_df2, steamer_pitchers_df, steamer_pitchers_df2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42656873-f0cf-4fa4-8682-331e176a3a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a191091-9148-4aa5-8d4f-0ab347592f7b",
   "metadata": {},
   "source": [
    "### Outs vs. Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9fcda-ddf3-4a82-9140-92cf2fb555c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is good! layers = (250,250,250,250,250) x 10 (just takes a while to run) It's good by year too\n",
    "# layers = (10,10)\n",
    "layers = (250,250,250,250,250)\n",
    "\n",
    "binary_filename = f\"model_binary_voting_{todaysdate}.sav\"\n",
    "print(binary_filename)\n",
    "\n",
    "iters = 100\n",
    "\n",
    "state = 10\n",
    "\n",
    "# Define the individual models in the ensemble\n",
    "models = [\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=state+1, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=state+2, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=state+3, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=state+4, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=state+5, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    ]\n",
    "\n",
    "\n",
    "# Create the ensemble classifier using VotingClassifier\n",
    "model_binary = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-1).fit(X_train[pa_inputs2], X_train[['is_out']].values.ravel())\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_binary, open(os.path.join(model_path, binary_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbdf233-faa3-465d-b6e7-0c5fe7db1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "proba = model_binary.predict_proba(X_test[pa_inputs2])\n",
    "X_test['is_safe_pred'] = proba[:, 0]  # Assign the first column of probabilities\n",
    "X_test['is_out_pred']  = proba[:, 1]  # Assign the second column of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d852fc0-0084-4a8d-aad5-ffc5319aa717",
   "metadata": {},
   "outputs": [],
   "source": [
    "park = 8\n",
    "venue = active_parks[park]\n",
    "print(team_map.query(f'VENUE_ID == {venue}')['BBREFTEAM'])\n",
    "\n",
    "X_test['decile'] = pd.qcut(X_test['is_out_pred'], 10, labels=False)\n",
    "\n",
    "globals()[\"is_out_df\"] = X_test.groupby('decile')[['is_out_pred', 'is_out']].mean().reset_index()\n",
    "# globals()[\"is_out_df\"] = X_test[X_test['year'].astype(int) >= 2022].groupby('decile')[['is_out_pred', 'is_out']].mean().reset_index()\n",
    "# globals()[\"is_out_df\"] = X_test.query(f'venue_id == \"{active_parks[park]}\"').groupby('decile')[['is_out_pred', 'is_out']].mean().reset_index()\n",
    "\n",
    "# Post Processing:\n",
    "# Create dataframe to adjust predictions to better fit test data\n",
    "out_df = X_test.groupby('decile').agg({'is_out_pred': ['max', 'mean'], 'is_out': 'mean'})\n",
    "out_df.columns = ['is_out_pred_max', 'is_out_pred_mean', 'is_out_mean']\n",
    "\n",
    "out_df.to_pickle(os.path.join(model_path, 'out_df.pkl'))\n",
    "\n",
    "plt.ylim(0.6, 0.76)\n",
    "\n",
    "# Create figures\n",
    "plt.plot(is_out_df['decile'], is_out_df['is_out_pred'], color='red')\n",
    "plt.plot(is_out_df['decile'], is_out_df['is_out'], color='black')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf095a-8c81-4a10-9807-a5d236c09d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.groupby('venue_id')[['is_out_pred', 'is_out', 'b1_mult', 'b2_mult', 'hr_mult']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b997670-544d-4f1f-80dd-18d2824dfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.61-.74 still seems too close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9d924-5ce5-41bc-8df3-fc2602fbad21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bd13a-a027-41d9-9203-9e8caf1da45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Neural network layers\n",
    "layers = (25,25,25)\n",
    "# To string\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "# Activation method\n",
    "activation = 'relu'\n",
    "# Iterations\n",
    "iters = 10\n",
    "\n",
    "outs_filename = f\"model_outs_{activation}_{layers_str}_{iters}_{todaysdate}.sav\"\n",
    "print(outs_filename)\n",
    "\n",
    "# Define the individual models in the ensemble\n",
    "models = [\n",
    "    # MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.00001, early_stopping=True, validation_fraction=0.1, random_state=3, max_iter=iters),\n",
    "\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=1, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=2, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=3, early_stopping=True, learning_rate_init=0.0001, alpha=0.00001, max_iter=iters),\n",
    "\n",
    "]\n",
    "\n",
    "# Create the ensemble classifier using VotingClassifier\n",
    "model_outs = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(outs_dataset_train[pa_inputs2], outs_dataset_train[['eventsModel']].values.ravel())\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_outs, open(os.path.join(model_path, outs_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97405dd2-7c8e-4fdd-af0d-95da09f878b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict out types\n",
    "outs_outputs = list(model_outs.classes_)\n",
    "outs_outputs_pred = [x + \"_pred\" for x in outs_outputs]\n",
    "\n",
    "proba = model_outs.predict_proba(outs_dataset_test[pa_inputs2])\n",
    "for i, col in enumerate(outs_outputs_pred):\n",
    "    outs_dataset_test[f'{col}'] = proba[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540717e-82bd-478a-bd39-6d1e1e7d0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deciles\n",
    "for var in outs_outputs:\n",
    "    outs_dataset_test[f'{var}_act'] = (outs_dataset_test['eventsModel'] == var).astype('int')\n",
    "    outs_dataset_test[f'{var}_decile'] = pd.qcut(outs_dataset_test[f'{var}_pred'], 10, labels=False)\n",
    "    df_name = var + \"_df\"\n",
    "    globals()[df_name] = outs_dataset_test.groupby([f'{var}_decile'])[[f'{var}_act', f'{var}_pred']].mean().reset_index()    \n",
    "    # globals()[df_name] = outs_dataset_test[outs_dataset_test['year'].astype(int) >= 2022].groupby([f'{var}_decile'])[[f'{var}_act', f'{var}_pred']].mean().reset_index(){var}_act', f'{var}_pred']].mean().reset_index()    \n",
    "    # globals()[df_name] = outs_dataset_test[outs_dataset_test['venue_id'] == \"3\"].groupby([f'{var}_decile'])[[f'{var}_act', f'{var}_pred']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5a925-fe18-418c-b64b-cc635d59cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, var in enumerate(outs_outputs):\n",
    "    row = i // 3  # Calculate the row index based on the iteration\n",
    "    col = i % 3   # Calculate the column index based on the iteration\n",
    "    axs[row, col].plot(globals()[f\"{var}_df\"][f'{var}_decile'], globals()[f\"{var}_df\"][f'{var}_pred'], color='red')\n",
    "    axs[row, col].plot(globals()[f\"{var}_df\"][f'{var}_decile'], globals()[f\"{var}_df\"][f'{var}_act'], color='black')\n",
    "    axs[row, col].set_title(var)\n",
    "    # axs[row, col].set_ylim(0,0.35)\n",
    "\n",
    "\n",
    "# Add some space between subplots to prevent overlapping\n",
    "fig.tight_layout(pad=.0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473da68-20b4-4f06-a0ba-100a4a977269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe77ff2-1518-4c0e-907a-a3706f86cae5",
   "metadata": {},
   "source": [
    "### Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6b458-2a2d-4a61-9780-c9c4943df704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Neural network layers\n",
    "# layers = (250,250,250,250,250,250,250,250)\n",
    "layers = (25,25)\n",
    "# To string\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "# Activation method\n",
    "activation = 'relu'\n",
    "# Iterations\n",
    "iters = 10\n",
    "# Alpha\n",
    "alpha = 0.0005\n",
    "# Random state\n",
    "random_state = 1\n",
    "\n",
    "safe_filename = f\"model_safe_{activation}_{layers_str}_{iters}_{todaysdate}.sav\"\n",
    "print(safe_filename)\n",
    "\n",
    "# Define the individual models in the ensemble\n",
    "models = [\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=True, alpha=alpha, early_stopping=True, random_state=random_state+2, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=True, alpha=alpha, early_stopping=True, random_state=random_state+3, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=True, alpha=alpha, early_stopping=True, random_state=random_state+4, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=True, alpha=alpha, early_stopping=True, random_state=random_state+5, max_iter=iters),\n",
    "\n",
    "    \n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=8, early_stopping=True, learning_rate_init=0.0001, alpha=alpha, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=9, early_stopping=True, learning_rate_init=0.0001, alpha=alpha, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=10, early_stopping=True, learning_rate_init=0.0001, alpha=alpha, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=11, early_stopping=True, learning_rate_init=0.0001, alpha=alpha, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=layers, activation='relu', random_state=12, early_stopping=True, learning_rate_init=0.0001, alpha=alpha, max_iter=iters),\n",
    "]\n",
    "\n",
    "# Create the ensemble classifier using VotingClassifier\n",
    "model_safe = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(safe_dataset_train[pa_inputs2], safe_dataset_train[['eventsModel']].values.ravel())\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_safe, open(os.path.join(model_path, safe_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62faf252-62f6-418d-ac21-4f477fc4d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict safe types\n",
    "safe_outputs = list(model_safe.classes_)\n",
    "safe_outputs_pred = [x + \"_pred\" for x in safe_outputs]\n",
    "\n",
    "proba = model_safe.predict_proba(safe_dataset_test[pa_inputs2])\n",
    "for i, col in enumerate(safe_outputs_pred):\n",
    "    safe_dataset_test[f'{col}'] = proba[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb77236-8a0a-49eb-b701-007078d47ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deciles\n",
    "for var in safe_outputs:\n",
    "    safe_dataset_test[f'{var}_act'] = (safe_dataset_test['eventsModel'] == var).astype('int')\n",
    "    safe_dataset_test[f'{var}_decile'] = pd.qcut(safe_dataset_test[f'{var}_pred'], 10, labels=False)\n",
    "    df_name = var + \"_df\"\n",
    "    globals()[df_name] = safe_dataset_test.groupby(f'{var}_decile')[[f'{var}_act', f'{var}_pred']].mean().reset_index()\n",
    "    # globals()[df_name] = safe_dataset_test[safe_dataset_test['year'].astype(int) >= 2022].groupby([f'{var}_decile'])[[f'{var}_act', f'{var}_pred']].mean().reset_index()\n",
    "    # globals()[df_name] = safe_dataset_test[safe_dataset_test['venue_id'] == \"3\"].groupby([f'{var}_decile'])[[f'{var}_act', f'{var}_pred']].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b3875-8760-4396-96ef-b6897886fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, var in enumerate(safe_outputs):\n",
    "    row = i // 3  # Calculate the row index based on the iteration\n",
    "    col = i % 3   # Calculate the column index based on the iteration\n",
    "    axs[row, col].plot(globals()[f\"{var}_df\"][f'{var}_decile'], globals()[f\"{var}_df\"][f'{var}_act'], color='black')\n",
    "    axs[row, col].plot(globals()[f\"{var}_df\"][f'{var}_decile'], globals()[f\"{var}_df\"][f'{var}_pred'], color='red')\n",
    "    axs[row, col].set_title(var)\n",
    "    # axs[row, col].set_ylim(globals()[df_name][f'{var}_act'].min(),globals()[df_name][f'{var}_act'].max())\n",
    "    \n",
    "\n",
    "# Add some space between subplots to prevent overlapping\n",
    "fig.tight_layout(pad=.0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287fa31-104e-41de-a191-3cc6e5d09ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adafadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1583e46-7dcd-488d-822b-e910f9cf9f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hr_df['hr_mse'] = (hr_df['hr_act'] - hr_df['hr_pred']) ** 2\n",
    "hr_df['hr_mse'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabebe10-b4d6-4e94-bfb8-dd69b28be4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'hr'\n",
    "safe_dataset_test.groupby(f'{var}_decile')[[f'{var}_pred', f'{var}_act']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ce8a1-f6ed-4e94-ba4a-e6c33851bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2067a43-6d4f-4b44-a0ec-aa9f354c0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_dataset_test.groupby('decile')[['hr_pred', 'hr_act']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bed8ac-225d-42ad-8bcf-8a0da5c311cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_dataset_test.query('year == \"2022\" | year == \"2023\"')[['b1_pred', 'b1_act', 'b2_pred', 'b2_act', 'b3_pred', 'b3_act', 'hr_pred', 'hr_act', 'bb_pred', 'bb_act', 'hbp_pred', 'hbp_act']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464b29a-364b-43c4-a350-c638a3189137",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset_test.query('year == \"2022\" | year == \"2023\"')[['fo_pred', 'fo_act', 'go_pred', 'go_act', 'lo_pred', 'lo_act', 'po_pred', 'po_act', 'so_pred', 'so_act']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2d089-0740-4914-88d3-0f8b009eb6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
