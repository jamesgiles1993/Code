{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f14ae0-6bcd-495d-92cb-2eda9f54d663",
   "metadata": {},
   "source": [
    "# A06. Weather\n",
    "This scrapes extracts batting orders and rosters\n",
    "- Type: Data\n",
    "- Run Frequency: Pre-contest\n",
    "- Sources:\n",
    "    - Swish Analytics\n",
    "    - RotoGrinders\n",
    "    - Ballpark Pal (deprecated) \n",
    "- Dates:\n",
    "    - Created: 9/23/2023\n",
    "    - Updated: 4/21/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fe878-dd80-4ce2-8308-8798454520b2",
   "metadata": {},
   "source": [
    "##### 1. Swish Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6817ec-6b9d-46d5-8127-674f2698842f",
   "metadata": {},
   "source": [
    "##### Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b89d212-d742-473b-8a85-8fb095599d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reverses winds so that they're named for where they're going, not where they're from. This is so vectors make more sense logically.\n",
    "def wind_reverser(direction):\n",
    "    direction = direction.replace(\"N\", \"s\")\n",
    "    direction = direction.replace(\"S\", \"n\")\n",
    "    direction = direction.replace(\"E\", \"w\")\n",
    "    direction = direction.replace(\"W\", \"e\")\n",
    "    \n",
    "    return direction.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ebf5e88-0e82-4b1e-9bf6-4c28664ce2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates number of degrees for each direction\n",
    "def find_degree(direction):\n",
    "    if direction == \"N\":\n",
    "        degree = 0\n",
    "    elif direction == \"NNE\":\n",
    "        degree = 1\n",
    "    elif direction == \"NE\":\n",
    "        degree = 2\n",
    "    elif direction == \"ENE\":\n",
    "        degree = 3\n",
    "    elif direction == \"E\":\n",
    "        degree = 4\n",
    "    elif direction == \"ESE\":\n",
    "        degree = 5\n",
    "    elif direction == \"SE\":\n",
    "        degree = 6\n",
    "    elif direction == \"SSE\":\n",
    "        degree = 7\n",
    "    elif direction == \"S\":\n",
    "        degree = 8\n",
    "    elif direction == \"SSW\":\n",
    "        degree = 9\n",
    "    elif direction == \"SW\":\n",
    "        degree = 10\n",
    "    elif direction == \"WSW\":\n",
    "        degree = 11\n",
    "    elif direction == \"W\":\n",
    "        degree = 12\n",
    "    elif direction == \"WNW\":\n",
    "        degree = 13\n",
    "    elif direction == \"NW\":\n",
    "        degree = 14\n",
    "    elif direction == \"NNW\":\n",
    "        degree = 15\n",
    "        \n",
    "    degree = degree * 22.5 \n",
    "\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da62f051-0720-4938-89a0-18f3b905b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the x and y vectors given the park's orientation and the wind's direction\n",
    "def calculate_vectors(row):\n",
    "    # Determines degree of centerfield\n",
    "    park_angle = find_degree(row['CF'])\n",
    "    # Determine degree of wind\n",
    "    row['Direction'] = wind_reverser(row['Direction'])\n",
    "    wind_angle = find_degree(row['Direction']) \n",
    "    \n",
    "    # Determine angle between them\n",
    "    angle = wind_angle - park_angle \n",
    "\n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row['Speed']\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row['Speed']\n",
    "\n",
    "    return x_vect, y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff49e30-fa37-4633-b8a0-697049387004",
   "metadata": {},
   "source": [
    "##### Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26c37ef-173a-427d-8096-6a528150420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Swish Analytics for weather date\n",
    "def swishanalytics(date):\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    # Swish Analytics URL \n",
    "    url = \"https://swishanalytics.com/mlb/weather?date=\" + date_dash\n",
    "\n",
    "     # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all divs with the class 'weather-card'\n",
    "        weather_cards = soup.find_all('div', class_='weather-card')\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        dfs = []\n",
    "        \n",
    "        # Iterate over each weather card\n",
    "        for weather_card in weather_cards:\n",
    "            # Extract relevant information from the weather card\n",
    "            time_info = weather_card.find('small', class_='text-muted')\n",
    "            location_info = weather_card.find('h4', class_='lato inline vert-mid bold')\n",
    "            \n",
    "            # Extract time and location information\n",
    "            time = time_info.text.strip() if time_info else None\n",
    "            location = location_info.text.strip() if location_info else None\n",
    "            \n",
    "            # Find the table within the weather card\n",
    "            table = weather_card.find('table', class_='table-bordered')\n",
    "            \n",
    "            # If table exists, extract data from it\n",
    "            if table:\n",
    "                # Extract table data into a list of lists\n",
    "                rows = table.find_all('tr')\n",
    "                data = []\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    row_data = [cell.text.strip() for cell in cells]\n",
    "                    data.append(row_data)\n",
    "                \n",
    "                # Convert data into a pandas DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Set the first row as the column headers\n",
    "                df.columns = df.iloc[0]\n",
    "                df = df[1:]  # Remove the first row since it's the header row\n",
    "                \n",
    "                # Add time and location as additional columns\n",
    "                df['Time'] = time\n",
    "                df['Location'] = location\n",
    "\n",
    "                # Create dataframem from the second time period scraped\n",
    "                daily_weather_df = pd.DataFrame(df.iloc[:, 2]).T\n",
    "                # Extract home team name \n",
    "                daily_weather_df['Matchup'] = df['Location'][1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['Matchup'].str.split(\"@\", expand=True).iloc[:, 1]\n",
    "                daily_weather_df['FANGRAPHSTEAM'] = daily_weather_df['FANGRAPHSTEAM'].str.replace(\"\\xa0\\xa0\", \"\")\n",
    "\n",
    "                dfs.append(daily_weather_df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    # Append together dataframes\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "\n",
    "    # Identify CF\n",
    "    df = df.merge(team_map[['FANGRAPHSTEAM', 'BBREFTEAM', 'CF']], on='FANGRAPHSTEAM', how='left')\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={1:'Weather', 2:'temperature', 3:'Feels Like', 4:'Humidity', 5:'Speed', 6:'Direction', 'BBREFTEAM': 'home_team'}, inplace=True)\n",
    "\n",
    "    # Remove mph\n",
    "    df['Speed'] = df['Speed'].str.replace(\" mph\", \"\").astype(float)\n",
    "    df['temperature'] = df['temperature'].str.replace('°', '')\n",
    "    df['Feels Like'] = df['Feels Like'].str.replace('°', '')\n",
    "    \n",
    "    # Apply the calculate_vectors function row-wise and assign results to new columns\n",
    "    df[['x_vect', 'y_vect']] = df.apply(calculate_vectors, axis=1, result_type='expand')\n",
    "    \n",
    "    \n",
    "    return df[['Matchup', 'home_team', 'Weather', 'Feels Like', 'Humidity', 'Speed', 'Direction', 'FANGRAPHSTEAM', 'CF', 'temperature', 'x_vect', 'y_vect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0f36-7de4-4fca-bf77-d01ebf46d99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a38159b-2394-43fd-bde5-19d0a6066d43",
   "metadata": {},
   "source": [
    "##### 2. RotoGrinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ce9bd1-c9b5-480a-ae74-dad7b25ccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotogrinders(date, team_map):\n",
    "    # URL of the web page containing the table\n",
    "    url = \"https://rotogrinders.com/weather/mlb\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Get the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = soup.find_all(\"li\", class_=\"weather-blurb\")\n",
    "\n",
    "        # Create an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        for li_element in li_elements:\n",
    "            # Extract the tag colors from the <span> elements\n",
    "            tag_elements = li_element.find_all(\"span\", class_=[\"green\", \"yellow\", \"orange\", \"red\"])\n",
    "        \n",
    "            # Extract the first tag color\n",
    "            tag = tag_elements[0].text.strip() if tag_elements else None\n",
    "        \n",
    "            # Extract the second tag color if it exists\n",
    "            tag2 = tag_elements[1].text.strip() if len(tag_elements) > 1 else None\n",
    "        \n",
    "            # Extract the matchup from the <span> element with class \"bold\"\n",
    "            matchup_span = li_element.find(\"span\", class_=\"bold\")\n",
    "            matchup = matchup_span.text.strip() if matchup_span else None\n",
    "        \n",
    "            # Extract the description if it exists\n",
    "            if matchup_span:\n",
    "                description_span = matchup_span.find_next_sibling(\"span\")\n",
    "                description = description_span.text.strip() if description_span else None\n",
    "            else:\n",
    "                description = None\n",
    "        \n",
    "            # Append the data to the list\n",
    "            data.append({\"Tag\": tag, \"Tag2\": tag2, \"Matchup\": matchup, \"Description\": description})\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        df[['away', 'home']] = df['Matchup'].str.split(\" @ \", expand=True)\n",
    "\n",
    "        # Add in DK team abbreviations \n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['away'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_away\"))\n",
    "        df = df.merge(team_map[['ROTOGRINDERSTEAM', 'DKTEAM']], left_on=['home'], right_on=['ROTOGRINDERSTEAM'], how='left', suffixes=(\"\", \"_home\"))\n",
    "        df = df[['Tag', 'Tag2', 'Matchup', 'DKTEAM', 'DKTEAM_home', 'Description']]\n",
    "        df.rename(columns={'DKTEAM':'Away', 'DKTEAM_home': 'Home'}, inplace=True)\n",
    "        \n",
    "        # Add the date column to the DataFrame\n",
    "        df['date'] = date\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        # Return an error message if the response is not successful\n",
    "        return \"Failed to retrieve data. Response status code: {}\".format(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc553503-e074-4b83-919b-66663c3532cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b34212e4-ff7f-4a99-8504-703b7e756a10",
   "metadata": {},
   "source": [
    "##### 3. Ballpark Pal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fa60b-f1e0-43b4-a4cc-2a622b8aff57",
   "metadata": {},
   "source": [
    "This is not currently supported after Ballpark Pal switch to subscription-only in 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca7e1861-70a6-42f9-add3-06352f94c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Ballpark Pal for weather factors\n",
    "# Note: these factor in park as well and are relative to league average conditions\n",
    "def ballparkpal(date):\n",
    "    headers = {\n",
    "    'Accept': 'text/html',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Reformat date to fit URL\n",
    "    date_dash = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "    \n",
    "    url = f'https://ballparkpal.com/ParkFactors.php?date={date_dash}'\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        tree = html.fromstring(response.text)\n",
    "        table_elements = tree.xpath('/html/body/div[1]/table')\n",
    "\n",
    "        if table_elements:\n",
    "            table_element = table_elements[0]\n",
    "\n",
    "            # Extract table rows\n",
    "            rows = table_element.xpath('.//tr')\n",
    "\n",
    "            # Extract table header (assuming it's the first row)\n",
    "            header = [th.text_content().strip() for th in rows[0].xpath('.//th')]\n",
    "\n",
    "            # Extract table data\n",
    "            data = []\n",
    "            for row in rows[1:]:\n",
    "                row_data = [td.text_content().strip() for td in row.xpath('.//td')]\n",
    "                data.append(row_data)\n",
    "\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "            \n",
    "            # Extracting the parts of the 'Game' column\n",
    "            df['Park'] = df['Game'].str.extract(r'^(.*?)\\s\\d{1,2}:\\d{2}')\n",
    "            df['Time'] = df['Game'].str.extract(r'(\\d{1,2}:\\d{2})')\n",
    "            df['Away'] = df['Game'].str.extract(r'\\d{1,2}:\\d{2}(.*?)@')\n",
    "            df['Away'] = df['Away'].str.strip()\n",
    "            df['Home'] = df['Game'].str.extract(r'@ (.*)$')\n",
    "\n",
    "            # Converting percentage columns to decimals\n",
    "            cols_to_convert = ['HR', '2B/3B', '1B', 'Runs']\n",
    "            for col in cols_to_convert:\n",
    "                df[col] = df[col].str.rstrip('%').astype(float) / 100\n",
    "\n",
    "            # Drop the 'Game' column since it's no longer needed\n",
    "            df.drop('Game', axis=1, inplace=True)\n",
    "            \n",
    "            df = df[['Park', 'Time', 'Away', 'Home', 'HR', '2B/3B', '1B', 'Runs']]\n",
    "            \n",
    "            df['GameNum'] = df.groupby('Park').cumcount() + 1\n",
    "            \n",
    "        else:\n",
    "            print(\"No table found at the specified XPath.\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    df['date'] = date\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58c49f-ff98-488b-bd59-362ee43b2c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24f72896-f1fd-4441-9b20-d14ec616c99c",
   "metadata": {},
   "source": [
    "##### 4. Park and Weather Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5387e3-91a8-41ac-ac27-828355aa3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for create_box\n",
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\A02. MLB API.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8d7a6b-0680-48fa-8f52-a6267cf91ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_and_weather_factors(game_df, multiplier_dataset, period_avg_df, swish_df=None, date=None, overwrite_year=None, historic=False):\n",
    "    print(date)\n",
    "    # Extract daily games\n",
    "    daily_game_df = game_df.query(f'date == \"{date}\"').reset_index(drop=True)\n",
    "    daily_game_df.sort_values('game_datetime', inplace=True)\n",
    "    daily_game_df['game_num'] = daily_game_df.groupby('home_team').cumcount() + 1\n",
    "\n",
    "    # If we have Swish weather data, merge it in\n",
    "    if swish_df is not None:\n",
    "        # Add game num\n",
    "        swish_df['game_num'] = swish_df.groupby('home_team').cumcount() + 1\n",
    "        daily_game_df = daily_game_df.merge(swish_df, on=['home_team', 'game_num'], how='left')\n",
    "\n",
    "    \n",
    "    daily_weather_list = []\n",
    "    # Loop over games\n",
    "    for i in range(len(daily_game_df)):\n",
    "        # Extract relevant information\n",
    "        game_id = daily_game_df['game_id'][i]\n",
    "        venue_id = daily_game_df['venue_id'][i]\n",
    "        away_team = daily_game_df['away_team'][i]\n",
    "        home_team = daily_game_df['home_team'][i]\n",
    "        datetime = daily_game_df['game_datetime'][i]\n",
    "        date = daily_game_df['date'][i]\n",
    "        year = daily_game_df['year'][i]\n",
    "\n",
    "        # If we have Swish Analytics data, extract it\n",
    "        if swish_df is not None:\n",
    "            temperature = daily_game_df['temperature'][i]\n",
    "            x_vect = daily_game_df['x_vect'][i].astype(float)\n",
    "            y_vect = daily_game_df['y_vect'][i].astype(float)\n",
    "        \n",
    "        # Extract weather from box score\n",
    "        weather, wind, park, full_date, missing_weather = create_box(game_id)\n",
    "        \n",
    "        # Create game weather dataframe\n",
    "        game_weather_dictionary = {\n",
    "            'game_id': game_id,\n",
    "            'away_team': away_team,\n",
    "            'home_team': home_team,\n",
    "            'venue_id': venue_id,\n",
    "            'park': park,\n",
    "            'datetime': datetime,\n",
    "            'date': date,\n",
    "            'year': year,\n",
    "            'weather': weather,\n",
    "            'wind': wind\n",
    "        }\n",
    "        game_weather_df = pd.DataFrame(game_weather_dictionary, index=[0])\n",
    "        game_weather_df = clean_weather(game_weather_df)\n",
    "\n",
    "        # If we do not have MLB Stats API data yet, use Swish Analytics\n",
    "        if missing_weather == True:\n",
    "            print(f\"{away_team}@{home_team} uses Swish Analytics weather.\")\n",
    "            game_weather_df['temperature'] = float(temperature)\n",
    "            game_weather_df['x_vect'] = x_vect\n",
    "            game_weather_df['y_vect'] = y_vect\n",
    "            game_weather_df['windDirection'] = \"Predicted\"\n",
    "        \n",
    "        daily_weather_list.append(game_weather_df)\n",
    "\n",
    "    # Create day weather dataframe\n",
    "    daily_weather_df = pd.concat(daily_weather_list, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Overwrite year (good option if early in season, but may want to develop a rolling method)\n",
    "    if overwrite_year is not None:\n",
    "        daily_weather_df['year'] = overwrite_year\n",
    "    \n",
    "    # Columns to keep\n",
    "    keep_columns = list(daily_weather_df.columns)\n",
    "    \n",
    "    # Add park dummies\n",
    "    active_venues = list(team_map['VENUE_ID'])\n",
    "    active_venues = [str(venue) for venue in active_venues]\n",
    "    for park in active_venues:\n",
    "        daily_weather_df[f'venue_{park}'] = (daily_weather_df['venue_id'].astype(str) == park).astype(int)\n",
    "    active_venue_columns = [f\"venue_{park}\" for park in active_venues]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Loop over lefty/righty dummy\n",
    "    for lefty_dummy in [0,1]:\n",
    "        # Assign lefty dummy\n",
    "        daily_weather_df['lefty'] = lefty_dummy\n",
    "\n",
    "        if lefty_dummy == 0:\n",
    "            side = 'r'\n",
    "        else:\n",
    "            side = 'l'\n",
    "\n",
    "        # Step 2: Create interaction terms\n",
    "        weather_interactions = []\n",
    "        \n",
    "        # Weather x Park\n",
    "        for col1 in active_venue_columns:\n",
    "            for col2 in ['x_vect', 'y_vect', 'temperature']:\n",
    "                interaction_name = col1 + '_' + col2\n",
    "                daily_weather_df[interaction_name] = daily_weather_df[col1] * daily_weather_df[col2]\n",
    "                weather_interactions.append(interaction_name)\n",
    "    \n",
    "    # If it's historic, \n",
    "    if historic == True:\n",
    "        # We already have the multiplier\n",
    "        multiplier_columns = [column for column in multiplier_dataset.columns if \"mult\" in column]\n",
    "        multiplier_dataset.rename(columns={'gamePk':'game_id'}, inplace=True)\n",
    "\n",
    "        # So keep them\n",
    "        daily_weather_df = pd.merge(daily_weather_df, multiplier_dataset[['game_id'] + multiplier_columns], on=['game_id'], how='left')\n",
    "\n",
    "    \n",
    "    \n",
    "    # If it's the day of,\n",
    "    else:\n",
    "        # We need to calculate it from the league averages, park factors, and league environment (period averages)\n",
    "        league_avg_columns = [column for column in multiplier_dataset.columns if \"league\" in column]\n",
    "        factor_columns = [column for column in multiplier_dataset.columns if \"factor\" in column]\n",
    "        \n",
    "        keep_columns = ['venue_id'] + league_avg_columns + factor_columns\n",
    "\n",
    "        # Keep most recent game at each venue\n",
    "        last_game_df = multiplier_dataset.drop_duplicates('venue_id', keep='last')[keep_columns]\n",
    "\n",
    "        # Merge that onto the weather\n",
    "        daily_weather_df = pd.merge(daily_weather_df, last_game_df, on=['venue_id'], how='left')\n",
    "\n",
    "\n",
    "        ### Calculate multipliers\n",
    "        # Loop over events\n",
    "        for event in events_list:\n",
    "            # Loop over sides\n",
    "            for side in ['l', 'r']:\n",
    "                # Select model      \n",
    "                model = globals().get(f'{event}_{side}_model')\n",
    "            \n",
    "                # Assign long-term average to average of team's stats for predicting a team-agnostic rate \n",
    "                daily_weather_df[f'{event}_b_long'] = period_avg_df[event][0]\n",
    "                daily_weather_df[f'{event}_p_long'] = period_avg_df[event][0]\n",
    "\n",
    "                daily_weather_df[f'{event}_league'] = daily_weather_df[f'{event}_league_{side}']\n",
    "                daily_weather_df[f'{event}_factor'] = daily_weather_df[f'{event}_factor_{side}']\n",
    "\n",
    "\n",
    "                # Model prediction inputs\n",
    "                X = daily_weather_df[[f'{event}_b_long', f'{event}_p_long', f'{event}_league', f'{event}_factor'] + weather_interactions]\n",
    "                X = sm.add_constant(X, has_constant='add')\n",
    "\n",
    "                # Predict probability of event\n",
    "                daily_weather_df[f'predicted_{event}'] = model.predict(X)\n",
    "                # Calculate multiplier\n",
    "                daily_weather_df[f'{event}_mult_{side}'] = daily_weather_df[f'predicted_{event}'] / period_avg_df[event][0]\n",
    "            \n",
    "    \n",
    "    # Keep relevent variables\n",
    "    daily_weather_df = daily_weather_df[['game_id', 'away_team', 'home_team', 'venue_id', 'park', 'datetime', 'date', 'year', \n",
    "                                         'weather', 'wind', 'temperature', 'windSpeed', 'windDirection', 'x_vect', 'y_vect'] + \n",
    "                                         [f'{event}_mult_l' for event in events_list] + \n",
    "                                         [f'{event}_mult_r' for event in events_list]]\n",
    "\n",
    "    \n",
    "    # Fill missings with 1\n",
    "    for event in events_list:\n",
    "        daily_weather_df[f'{event}_mult_l'].fillna(1, inplace=True)\n",
    "        daily_weather_df[f'{event}_mult_r'].fillna(1, inplace=True)\n",
    "    \n",
    "    # # Set park factors to 1 if venue_id is not among active parks    \n",
    "    # # Check if venue_id is not present in team_map['VENUE_ID']\n",
    "    # not_in_team_map = ~daily_weather_df['venue_id'].isin(team_map['VENUE_ID'].astype(int))\n",
    "    \n",
    "    # # Set values to 1 for columns in event_factors_l and event_factors_r where the condition is met\n",
    "    # for column in event_factors_l + event_factors_r:\n",
    "    #     # Set values to 1 where the condition is met\n",
    "    #     daily_weather_df.loc[not_in_team_map, column] = 1\n",
    "\n",
    "\n",
    "    return daily_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86e87d8-930c-4405-a803-9555d4d64f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "# %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "# %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b6573d-3d32-43ac-878c-b152bf308409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = \"20220407\"\n",
    "# end_date = todaysdate\n",
    "# game_df = read_and_save_games(team_map, generate=True)\n",
    "# game_df = game_df[(game_df['date'] >= start_date) & (game_df['date'] <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5716dec5-449f-4185-b69e-76846bcb484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplier_dataset = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))\n",
    "# period_avg_df = pd.read_csv(os.path.join(baseball_path, \"Period Averages.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "695f3612-07c0-431e-b948-fd36936833ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date in game_df['date'].unique():\n",
    "#     daily_weather_df = park_and_weather_factors(game_df, multiplier_dataset, period_avg_df, swish_df=None, date=date, overwrite_year=None, historic=True)\n",
    "#     daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"4. Park and Weather Factors\", f\"Park and Weather Factors {date}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68226338-ef09-4b91-8736-be8a99c737b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
