{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# M01. Park and Weather Factors\n",
    "- This calculated Park x Weather Factors\n",
    "- Type: Model\n",
    "- Run Frequency: Daily\n",
    "- Sources:\n",
    "    - MLB API\n",
    "    - Steamer\n",
    "- Created: 12/10/2024\n",
    "- Updated: 12/17/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running imports...\n",
      "Imports in.\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    print(\"Running imports...\")\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U4. Datasets.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U5. Models.ipynb\"\n",
    "    print(\"Imports in.\")\n",
    "else:\n",
    "    print(\"Imports already in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce0f6-2eb6-4ecd-841b-2e99fc13c281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02405859-9235-419d-bb06-52eb83ede249",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b979a91-7a59-4d4b-b126-73771a2536ca",
   "metadata": {},
   "source": [
    "Create Latest PA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208611b0-2531-45b0-b7e7-ae2db5db9c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 51s\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "complete_dataset_unadjusted_latest = create_pa_inputs(None, start_year=2022, end_year=2024, short=50, long=300, adjust=False, generate=True, write=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4436b23-bb2d-4874-b871-4f39cec4535f",
   "metadata": {},
   "source": [
    "Keep Only Most Recent Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6172690-d36e-40a5-8f31-f597ee673563",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_unadjusted_latest = complete_dataset_unadjusted_latest[complete_dataset_unadjusted_latest['year'].astype(int) >= 2025].reset_index()\n",
    "complete_dataset_unadjusted_latest.to_csv(os.path.join(baseball_path, \"Complete Dataset - Unadjusted Latest.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd07d4-85a6-429d-8f9b-99d3e4f056de",
   "metadata": {},
   "source": [
    "Read in Earlier Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbb9297-1697-4572-b7f9-fa3730ea596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_unadjusted_earlier = pd.read_csv(os.path.join(baseball_path, \"Complete Dataset - Unadjusted through 2024.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c3481-118b-483d-a04e-a482e3336bbf",
   "metadata": {},
   "source": [
    "Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28199060-a760-4bc1-8721-0ffe09a554b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.concat([complete_dataset_unadjusted_earlier, complete_dataset_unadjusted_latest], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa893181-454e-4e95-93b4-02aa74ee77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b42d3-21c5-4cda-92ea-748580d3884b",
   "metadata": {},
   "source": [
    "### Base Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13121744-f9d8-4d98-8797-267f410948e6",
   "metadata": {},
   "source": [
    "Calculate average stats in a given base year <br>\n",
    "Note: This only has to be run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8511a-581b-47b4-8345-36b219d3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rates(df, base_year=2014):\n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Select period of interest\n",
    "    df = df[df['game_date'].dt.year == base_year]\n",
    "\n",
    "    # Calculate averages over period of interest\n",
    "    base_rate_df = pd.DataFrame(df[events_list].mean()).T\n",
    "\n",
    "    \n",
    "    return base_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a930b6-360b-42d0-99c2-8000eae19653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b183ab-f43b-4c9c-a209-b2ee24c3f190",
   "metadata": {},
   "source": [
    "### Game Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed47d7-2bce-4d17-ac4e-eb608eda8884",
   "metadata": {},
   "source": [
    "Average rates within the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf7da9-72c3-4c3c-985e-c57db1a405d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(df):    \n",
    "    # Calculate averages by game\n",
    "    game_avgs = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'])[events_list].mean().reset_index()\n",
    "\n",
    "    # Add the 'pas' column to count the number of observations in each group\n",
    "    game_avgs['pas'] = df.groupby(['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']).size().values\n",
    "\n",
    "    # Sort by date\n",
    "    game_avgs.sort_values(['game_date'], ascending=True, inplace=True)\n",
    "\n",
    "    \n",
    "    return game_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabeca8c-8017-45b2-8942-5d90250eea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_average_df = game_averages(complete_dataset)\n",
    "# game_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a92c3e-8bbf-4687-9079-68d7346bede0",
   "metadata": {},
   "source": [
    "### Player Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82edc50-a380-4caf-87ab-650d932a1e4a",
   "metadata": {},
   "source": [
    "Average stats of all the players in the game, coming into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc16b3-2c4b-4f89-b1e5-32f0356f818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_averages(df):\n",
    "    # Stats to average\n",
    "    batter_inputs_short = [f\"{event}_b_long\" for event in events_list]\n",
    "    pitcher_inputs_short = [f\"{event}_p_long\" for event in events_list]\n",
    "\n",
    "    # Apply stats from last at bat to entire game\n",
    "    df[batter_inputs_short] = df.groupby(['gamePk', 'batter'])[batter_inputs_short].transform('last')\n",
    "    df[pitcher_inputs_short] = df.groupby(['gamePk', 'pitcher'])[pitcher_inputs_short].transform('last')\n",
    "    \n",
    "    # Calculate player averages by game\n",
    "    batter_avgs = df.groupby(['gamePk'])[batter_inputs_short].mean().reset_index()\n",
    "    pitcher_avgs = df.groupby(['gamePk'])[pitcher_inputs_short].mean().reset_index()\n",
    "\n",
    "    # Concatenate together\n",
    "    player_avgs = pd.concat([batter_avgs, pitcher_avgs.drop(columns=['gamePk'])], axis=1)\n",
    "    \n",
    "    \n",
    "    return player_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f1d09-dc1a-4d72-80a1-2fc6f5c0e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_average_df = player_averages(complete_dataset)\n",
    "# player_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9a52f-86cd-4886-ac94-23bb9b111af7",
   "metadata": {},
   "source": [
    "### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617b005-f5ae-4267-bf08-551051f5cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def league_average(complete_dataset, days=30):\n",
    "    # Calculate daily sum of events\n",
    "    league_avg = complete_dataset.groupby('game_date')[events_list].sum().reset_index()\n",
    "    # Calculate total events\n",
    "    league_avg['pas'] = league_avg[events_list].sum(axis=1)\n",
    "    \n",
    "    # Use rolling sum including the current row\n",
    "    for event in events_list + ['pas']:\n",
    "        league_avg[f'{event}_sum'] = league_avg[event].rolling(window=days, min_periods=1).sum()\n",
    "\n",
    "    # Calculate average\n",
    "    for event in events_list:\n",
    "        league_avg[f'{event}_lg'] = league_avg[f'{event}_sum'] / league_avg['pas_sum']\n",
    "\n",
    "        \n",
    "    return league_avg[[\"game_date\"] + [col for col in league_avg if \"_lg\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbf862-3c02-4922-9b6f-0b22d6faf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# league_average_df = league_average(complete_dataset, 30)\n",
    "# league_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb155b-c601-4d64-a88e-c75dce2b7c7f",
   "metadata": {},
   "source": [
    "### Park Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8674102-da42-42e1-9610-d8cf2c36cb21",
   "metadata": {},
   "source": [
    "##### Rolling Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344bd9-99ba-4b5e-82bd-ff0a4f568eb1",
   "metadata": {},
   "source": [
    "Average of stats over last rolling_window games - excluding game of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31d871-22da-4911-a98b-c3ec81d8d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_averages(game_avgs, rolling_window, column):\n",
    "    # Sort by group column and date\n",
    "    rolling_avgs = game_avgs.sort_values([column, 'game_date']).copy()\n",
    "\n",
    "    # Compute rolling sum for `pas`\n",
    "    rolling_avgs['pas_rolling'] = rolling_avgs.groupby(column)['pas'].transform(\n",
    "        lambda x: x.rolling(window=rolling_window, min_periods=1, closed=\"right\").sum()\n",
    "    )\n",
    "\n",
    "    # Define function for rolling weighted average\n",
    "    def weighted_avg(group):\n",
    "        return (\n",
    "            group[events_list]\n",
    "            .rolling(window=rolling_window, min_periods=1, closed=\"right\")\n",
    "            .apply(lambda x: (x * group.loc[x.index, 'pas']).sum() / group.loc[x.index, 'pas'].sum(), raw=False)\n",
    "        )\n",
    "\n",
    "    # Apply rolling weighted average by the given column\n",
    "    rolling_avgs[events_list] = rolling_avgs.groupby(column, group_keys=False).apply(weighted_avg)\n",
    "\n",
    "    \n",
    "    return rolling_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55a7ca-d0e4-4a46-9a46-69395cd4f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to account for small sample parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3941156-6fff-405a-ae97-fdb0d6605d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_average_df = rolling_averages(game_average_df, 243, 'venue_id')\n",
    "# park_average_df = park_average_df[['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + events_list + ['pas_rolling']]\n",
    "# park_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b74003-2b91-4d48-816c-3cc638496986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_average_df = rolling_averages(game_average_df, 243, 'away_name')\n",
    "# team_average_df = team_average_df[['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + events_list + ['pas_rolling']]\n",
    "# team_average_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aa6c4-b995-464b-9450-0d7581c12282",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24c5e-2448-403b-b822-a8a282cf9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_park_factors(park_avgs, team_avgs):\n",
    "    # Sort by game_date\n",
    "    park_avgs = park_avgs.sort_values('game_date')\n",
    "    team_avgs = team_avgs.sort_values('game_date')\n",
    "\n",
    "    # Create uniform team_name variable equal to name of interest\n",
    "    park_avgs['team_name'] = park_avgs['home_name'].copy()\n",
    "    team_avgs['team_name'] = team_avgs['away_name'].copy()\n",
    "\n",
    "    # Set to datetime\n",
    "    park_avgs['game_date'] = pd.to_datetime(park_avgs['game_date'])\n",
    "    team_avgs['game_date'] = pd.to_datetime(team_avgs['game_date'])\n",
    "    \n",
    "    # Perform merge_asof\n",
    "    park_factor_df = pd.merge_asof(park_avgs, team_avgs, left_on='game_date', right_on='game_date', by='team_name', direction='backward', suffixes=('_park', '_team'))\n",
    "\n",
    "    # Calculate park factors\n",
    "    for stat in events_list:\n",
    "        park_factor_df[f'{stat}_pfx'] = park_factor_df[f'{stat}_park'] / park_factor_df[f'{stat}_team'] \n",
    "        \n",
    "    park_factor_df.rename(columns={'gamePk_park': 'gamePk'}, inplace=True)\n",
    "    keep_columns = ['gamePk'] + [col for col in park_factor_df.columns if col.endswith('pfx')]\n",
    "\n",
    "    \n",
    "    return park_factor_df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8cfd-1241-481f-8a33-7788417e22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df = create_park_factors(park_average_df, team_average_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf439c-b77e-4230-aee2-8a4397b7b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_factor_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0f210-8aa6-4479-8c1b-f6cc194edac0",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd52ba2-27f4-4260-97ad-a2e7a2892d29",
   "metadata": {},
   "source": [
    "Merge together game averages, player averages, and park factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba144343-39bf-499c-b1d9-2a6a68bf46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_df(complete_dataset, league_average_df, park_factor_df):\n",
    "    # Merge on league averages\n",
    "    analysis_df = pd.merge(complete_dataset, league_average_df, on=['game_date'], how='inner')\n",
    "    # Merge on park factors\n",
    "    analysis_df = pd.merge(analysis_df, park_factor_df, on='gamePk', how='inner')\n",
    "   \n",
    "    \n",
    "    # Extract dummies from venues\n",
    "    venue_dummy_df = pd.get_dummies(analysis_df['venue_id'], prefix='venue')\n",
    "    # Extract dummy column names\n",
    "    venue_dummies = list(venue_dummy_df.columns)\n",
    "    \n",
    "    # Add in dummies\n",
    "    analysis_df = pd.concat([analysis_df, venue_dummy_df], axis=1)\n",
    "    \n",
    "    # Select variables to keep\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    # Select relevant variables and drop missings\n",
    "    analysis_df = analysis_df[[\"eventsModel\", 'gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'batter', 'pitcher', 'batSide', 'pitchHand'] + variables + [col for col in analysis_df if col.endswith(\"_lg\")]].dropna()\n",
    "    \n",
    "    # Remove cut\n",
    "    analysis_df = analysis_df[analysis_df['eventsModel'] != \"Cut\"]\n",
    "    \n",
    "    \n",
    "    return analysis_df, venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e6f67-06ac-43c8-9436-0175a0a197ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_df, venue_dummies = create_analysis_df(complete_dataset, league_average_df, park_factor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74231-a561-4350-8dfa-44defbaf432a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0792a0-b2a1-4764-8f7b-b10b6c3244ba",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c58b5-d2a5-49c3-b158-5d4d5b88e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset['temperature'] = complete_dataset.apply(lambda row: 70 if 'Roof' in row['weather'] or 'Dome' in row['weather'] else row['temperature'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f0dfa-636c-4859-be79-2b0fb5c4450b",
   "metadata": {},
   "source": [
    "Generate or read base rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d4fe7-d269-4232-adee-a744201c2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate base rates (base year = 2014)\n",
    "# Only needs to be run once\n",
    "# Generate:\n",
    "# base_rate_df = base_rates(complete_dataset, 2014)\n",
    "# base_rate_df.to_csv(os.path.join(baseball_path, \"Base Rates.csv\"), index=False)\n",
    "\n",
    "# Read: \n",
    "base_rate_df = pd.read_csv(os.path.join(baseball_path, \"Base Rates.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871e553-13cc-4154-ac4a-3286123271b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes\n",
    "analysis_df_list = []\n",
    "# Loop over batter sides\n",
    "for batSide in ['L', 'R']:\n",
    "    print(batSide)\n",
    "    # Subset complete dataset\n",
    "    complete_dataset_side = complete_dataset[complete_dataset['batSide'] == batSide]\n",
    "    # Calculate game averages (average rates within a particular games)\n",
    "    game_average_df = game_averages(complete_dataset_side)\n",
    "    # # Calculate player averages (average rates of all players coming into the game) (deprecated? - player level is in complete_dataset, so it's unnecessary)\n",
    "    # player_average_df = player_averages(complete_dataset_side)\n",
    "    # Calculate league averages (average rates of all PAs over last n days coming into the day)\n",
    "    league_average_df = league_average(complete_dataset_side, days=30)\n",
    "    # Average rates at park over last n games (both teams)\n",
    "    park_average_df = rolling_averages(game_average_df, 243, 'venue_id')\n",
    "    # Average rates at away games over last n games (both teams)\n",
    "    team_average_df = rolling_averages(game_average_df, 243, 'away_name')\n",
    "    # Park factors\n",
    "    park_factor_df = create_park_factors(park_average_df, team_average_df)\n",
    "    # Create dataframe that can be used to train and analyze data\n",
    "    analysis_df, venue_dummies = create_analysis_df(complete_dataset, league_average_df, park_factor_df)\n",
    "    analysis_df_list.append(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc5217-c26b-4dc4-8d03-955ccc7a06ae",
   "metadata": {},
   "source": [
    "Extract Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762f4b5-a727-42f4-bcd9-7647e21bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = analysis_df_list[0].copy()\n",
    "r_analysis_df = analysis_df_list[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2af25-3985-4583-b8bc-a5b4c78f90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del analysis_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c75831-67b2-4321-8060-f23b1d6f3776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44fa17f-219c-47e7-bc8e-f2957868b864",
   "metadata": {},
   "source": [
    "### Park Latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4125b4-4db9-41d4-8315-65a140ddbecb",
   "metadata": {},
   "source": [
    "This contains the latest data available at each park, used to create WFX <br>\n",
    "Note: We can't just use multiplier dataset for this because it won't contain data at the end of the last game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42ae99-af38-43e5-9bb3-b0a7b32ee69b",
   "metadata": {},
   "source": [
    "Columns to Keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1195af-3f58-432f-95b4-36bea7147a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_latest_columns = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name'] + venue_dummies + [col for col in l_analysis_df.columns if col.endswith(\"_pfx\")] + [col for col in l_analysis_df.columns if col.endswith(\"_lg\")] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c39efb-3834-48d2-85b2-70f7de44498b",
   "metadata": {},
   "source": [
    "Write Park's Last Values to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147817-6116-4098-8e3f-b46bc39ef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[park_latest_columns].sort_values('game_date').drop_duplicates('venue_id', keep='last').to_csv(os.path.join(baseball_path, \"Park Latest - LHB.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7fdbb-d18c-47e7-a955-d7343aabe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[park_latest_columns].sort_values('game_date').drop_duplicates('venue_id', keep='last').to_csv(os.path.join(baseball_path, \"Park Latest - RHB.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f28a2-7a68-491b-bf45-9299539bf797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171df5e2-2793-4c9a-91a3-e1e011789822",
   "metadata": {},
   "source": [
    "### Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fe782-a3d4-4f2f-b87e-d593313dcb82",
   "metadata": {},
   "source": [
    "##### Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e276c0f-9ee3-48aa-814a-853d948bbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx_list = [col for col in l_analysis_df.columns if col.endswith(\"pfx\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae33f6-ae8a-4f9f-ac72-e1d48d8d2b66",
   "metadata": {},
   "source": [
    "Previous game_date at venue_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94cd0-6d31-4ca3-a0ca-43668a6f76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[pfx_list] = l_analysis_df.groupby(\"venue_id\")[pfx_list].shift(1)\n",
    "l_analysis_df[pfx_list] = l_analysis_df.groupby([\"venue_id\", \"game_date\"])[pfx_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53392d4-d415-4705-9dd4-1ecf68e0468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df[pfx_list] = r_analysis_df.groupby(\"venue_id\")[pfx_list].shift(1)\n",
    "r_analysis_df[pfx_list] = r_analysis_df.groupby([\"venue_id\", \"game_date\"])[pfx_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa86df-e891-4615-b351-caed247ded1e",
   "metadata": {},
   "source": [
    "##### League Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39378c-cff8-4f6e-af9c-4b1b65f03332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_list = [col for col in l_analysis_df.columns if col.endswith(\"lg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bab02-30b5-4a5e-96cc-e3383936f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = l_analysis_df.sort_values('game_date', ascending=True)\n",
    "l_analysis_df[lg_list] = l_analysis_df[lg_list].shift(1)\n",
    "l_analysis_df[lg_list] = l_analysis_df.groupby(\"game_date\")[lg_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4f8da-bb64-4e08-aa13-03519306a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_analysis_df = r_analysis_df.sort_values('game_date', ascending=True)\n",
    "r_analysis_df[lg_list] = r_analysis_df[lg_list].shift(1)\n",
    "r_analysis_df[lg_list] = r_analysis_df.groupby([\"game_date\"])[lg_list].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a7286-6a71-4dd5-ac2c-78943c913062",
   "metadata": {},
   "source": [
    "##### Batter Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53543a-f923-4318-a214-3809a06f0610",
   "metadata": {},
   "source": [
    "Note: You need to shift by batter and pitchHand to get the batter's last PA against that hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a3032-0731-42fe-bfcc-58d991592d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_long_list = [col for col in l_analysis_df.columns if col.endswith(\"b_long\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddcdd8-2d8b-4f28-9a5e-a56fbc5cf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[b_long_list] = l_analysis_df.groupby(['batter', 'pitchHand'])[b_long_list].shift(1)\n",
    "r_analysis_df[b_long_list] = r_analysis_df.groupby(['batter', 'pitchHand'])[b_long_list].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef67cd-558d-4049-a7cd-2ce8950580be",
   "metadata": {},
   "source": [
    "##### Pitcher Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fd134-8e9f-4b49-bac8-ab9508ffec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_long_list = [col for col in l_analysis_df.columns if col.endswith(\"p_long\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652851a9-3d52-455b-8e7e-64f318993f44",
   "metadata": {},
   "source": [
    "Note: You don't to shift by batSide to get the pitcher's last PA against that hand because all hands are the same, but why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef64c1a-5f22-40bb-810e-78f442ab8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[p_long_list] = l_analysis_df.groupby(['pitcher', 'batSide'])[p_long_list].shift(1)\n",
    "r_analysis_df[p_long_list] = r_analysis_df.groupby(['pitcher', 'batSide'])[p_long_list].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4103a0-53db-43d2-aa5d-7f8a0d52d083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c553dc3-529b-4cac-8ddf-8554977d5dc0",
   "metadata": {},
   "source": [
    "### Select Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11a93b-f63d-46ec-ac9f-2f981aab4e43",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23a588-25eb-44e0-b0c4-733efcb12b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify inputs\n",
    "training_input_list = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "# Loop over events\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    training_input_list += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7a8cb-609e-44bc-be22-caaaa6d3c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify inputs\n",
    "testing_input_list = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "# Loop over events\n",
    "for event in events_list: \n",
    "    # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "    testing_input_list += [f'{event}_lg', f'{event}_lg', f'{event}_pfx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f0967-44cb-465e-bb95-5673f1851ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779a5290-d1a6-4579-b899-d7eeb70f964b",
   "metadata": {},
   "source": [
    "### Select Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12432be5-8491-4a58-9917-c2875f5dab0e",
   "metadata": {},
   "source": [
    "Remove Infinite Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb826d-2b92-42b8-b868-7bc823c89545",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df[training_input_list] = l_analysis_df[training_input_list].replace([np.inf, -np.inf], np.nan)\n",
    "r_analysis_df[training_input_list] = r_analysis_df[training_input_list].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0608d30-a1e1-4963-a1ca-c990d07309ad",
   "metadata": {},
   "source": [
    "Drop if Missing Data (Maybe after shift?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0beb1-58cc-4bc6-9202-ff3fe6ddc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df = l_analysis_df.dropna()\n",
    "r_analysis_df = r_analysis_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f709e68-8ea6-482e-ae5f-7f1f2d5c0794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af1c0368-500b-4417-ae89-49fe7fff7bcc",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ff4ac-8215-4696-82d3-199d2b2ea431",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c428e-b2d4-406f-9541-239a1662f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "l_analysis_df['split'] = np.random.choice([0, 0, 1], size=len(l_analysis_df))\n",
    "r_analysis_df['split'] = np.random.choice([0, 0, 1], size=len(r_analysis_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5452e-daef-497a-922d-d0acb15412eb",
   "metadata": {},
   "source": [
    "Create masks to identify training and testing datasets (Might not use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86515a7e-44fe-49c4-ad80-31e00a1037d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training_mask = (l_analysis_df['split'] == 0)\n",
    "r_training_mask = (r_analysis_df['split'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7d473-660a-46a3-9bf7-a01273a37ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803d3a0-a00f-4f57-90bc-125654f9e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acec17a-7d0b-4082-a34c-a0572f6cf759",
   "metadata": {},
   "source": [
    "### WFX - L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef5450-90b8-4cbf-b6fb-e52dcc03b442",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898c9d5-0383-43c8-bb57-77daad7c5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (83,83,83,83,83)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "activation = 'relu'\n",
    "max_iter = 10\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 1\n",
    "\n",
    "quantiles = 10\n",
    "\n",
    "wfx_l_filename = f\"predict_wfx_l.pkl\"\n",
    "print(wfx_l_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf4fdd-3e76-42f3-b72f-b2aca6824da7",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee93bcb-54a1-40e8-b6eb-6bde2e9df283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Create folder\n",
    "    os.makedirs(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate), exist_ok=True)\n",
    "    \n",
    "    # Create Model\n",
    "    predict_wfx_l = MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=False, alpha=alpha, \n",
    "                                  learning_rate_init=learning_rate, early_stopping=True, random_state=random_state, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "    # Fit\n",
    "    predict_wfx_l.fit(l_analysis_df[training_input_list], l_analysis_df[['eventsModel']].values.ravel())\n",
    "\n",
    "    # Save model\n",
    "    pickle.dump(predict_wfx_l, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, wfx_l_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b033748-710c-4d48-9c18-f2d4a8965794",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5604038-b47b-4555-a7f5-954757fc6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_l_outputs = list(predict_wfx_l.classes_)\n",
    "wfx_l_outputs_pred = [x + \"_pred\" for x in list(predict_wfx_l.classes_)]\n",
    "\n",
    "l_analysis_df[wfx_l_outputs_pred] = predict_wfx_l.predict_proba(l_analysis_df[testing_input_list].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61749d-e74e-4527-a50b-c0b3d72557ed",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931c20f-82b8-4d86-b87e-4d0a292dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Get dummies\n",
    "    for event in events_list:\n",
    "        l_analysis_df[event] = (l_analysis_df['eventsModel'] == event).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b49829-2b27-47ee-9701-5798579b4da3",
   "metadata": {},
   "source": [
    "##### Calculate WFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2b1b-3ebd-4d92-9b3b-484f017dca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    l_analysis_df[f'{event}_wfx_l'] = l_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d6e3a-7f90-44ae-8225-a32b84f46722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fcec6c2-e7ee-4f7c-9a82-1a291446fd93",
   "metadata": {},
   "source": [
    "### WFX - R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c99cc-4d54-4fce-ad81-5977d6058e9f",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808fb9f-cea4-4240-a15b-a6ab9370a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (83,83,83,83,83)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "activation = 'relu'\n",
    "max_iter = 100\n",
    "alpha = 0.0001\n",
    "learning_rate = 0.00001\n",
    "batch_size='auto'\n",
    "random_state = random.randint(1,99999)\n",
    "num_models = 1\n",
    "\n",
    "quantiles = 10\n",
    "\n",
    "wfx_r_filename = f\"predict_wfx_r.pkl\"\n",
    "print(wfx_r_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce681fc-6e96-49f5-873a-89b62097f059",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e3e36-90b8-45ac-9b4d-759bb5f773de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Create Model\n",
    "    predict_wfx_r = MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=False, alpha=alpha, \n",
    "                                  learning_rate_init=learning_rate, early_stopping=True, random_state=random_state, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "    # Fit\n",
    "    predict_wfx_r.fit(r_analysis_df[training_input_list], r_analysis_df[['eventsModel']].values.ravel())\n",
    "\n",
    "    # Save model\n",
    "    pickle.dump(predict_wfx_r, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, wfx_r_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72166d7-166d-47f9-a12f-2c00111c3b67",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768ad92-30df-448d-8a86-e974de259ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfx_r_outputs = list(predict_wfx_r.classes_)\n",
    "wfx_r_outputs_pred = [x + \"_pred\" for x in wfx_r_outputs]\n",
    "\n",
    "r_analysis_df[wfx_r_outputs_pred] = predict_wfx_r.predict_proba(r_analysis_df[testing_input_list].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740374cf-17e7-414b-84c9-f0acc5f23595",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8bc6-328e-493e-98b3-4c118fc55bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(sys.modules['__main__'], '__file__'):\n",
    "    # Define the number of quantiles\n",
    "    num_quantiles = 10  # Change as needed\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    for event in events_list:\n",
    "        r_analysis_df[event] = (r_analysis_df['eventsModel'] == event).astype(int)\n",
    "\n",
    "        # Create quantile bins based on event_pred\n",
    "        r_analysis_df[f'{event}_quantile'] = pd.qcut(\n",
    "            r_analysis_df[f'{event}_pred'], q=num_quantiles, duplicates='drop'\n",
    "        )\n",
    "    \n",
    "        # Compute means for event and event_pred within each quantile\n",
    "        summary_df = r_analysis_df.groupby(f'{event}_quantile')[[f'{event}_pred', event]].mean()\n",
    "    \n",
    "        # Store the result in a dictionary\n",
    "        result_dict[event] = summary_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7f028-de4e-426e-b02b-d4d254429682",
   "metadata": {},
   "source": [
    "##### Calculate WFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af494f02-d0e0-4ea8-abb8-deaa0d9a5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events_list:\n",
    "    r_analysis_df[f'{event}_wfx_r'] = r_analysis_df[f'{event}_pred'] / base_rate_df[event][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bf838-f9d9-4ba9-80fe-5eedf4c232d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d62eceb-e366-44ca-9d34-668a3957971a",
   "metadata": {},
   "source": [
    "### Multiplier Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b17e2c-a811-44e6-9a4d-0d16d12b894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_columns = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']\n",
    "wfx_l_columns = [col for col in l_analysis_df.columns if col.endswith(\"_wfx_l\")]\n",
    "wfx_r_columns = [col for col in r_analysis_df.columns if col.endswith(\"_wfx_r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c06ae9-860d-47fb-91d5-b538501f0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset = pd.merge(l_analysis_df.drop_duplicates('gamePk', keep='last')[descriptive_columns + wfx_l_columns], \n",
    "                              r_analysis_df.drop_duplicates('gamePk', keep='last')[descriptive_columns + wfx_r_columns], on=descriptive_columns, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059d1e2-c750-48cb-9cbb-cc9b07ca68e1",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c4db5-06f3-45aa-934e-a2cf7a40c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset['date'] = multiplier_dataset['game_date'].str.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317b1c1-fd77-4155-ac82-11b70dc86d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset.to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d80922-4b5c-4da8-bd9b-cfe5e27495ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ebfe6-5963-4325-bece-99c672bea596",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAKLSDFL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ca5e2-3646-455b-835f-049da2093878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52416d-10ed-4987-964d-ed4e5e7c2677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40986f3-0648-46d6-b082-a461643fdc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c6cb5-be68-4007-95d5-0dd9ac19822b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b228b-509e-47a4-9023-bc1635d80eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564981cd-7ecf-4795-862a-f28199e2ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f113f-ad58-425d-98c8-24cab24c4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(num_models):\n",
    "    \n",
    "    # Create Model\n",
    "    predict_wfx_l = MLPClassifier(hidden_layer_sizes=layers, activation=activation, verbose=False, alpha=alpha, \n",
    "                                  learning_rate_init=learning_rate, early_stopping=True, random_state=random_state+i, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "    # Fit\n",
    "    predict_wfx_l.fit(l_analysis_df[l_training_mask][input_list], l_analysis_df[l_training_mask][['eventsModel']].values.ravel())\n",
    "\n",
    "    # Save model\n",
    "    pickle.dump(predict_wfx_l, open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, wfx_l_filename), 'wb'))\n",
    "\n",
    "    # Predict all types\n",
    "    wfx_l_outputs = list(predict_wfx_l.classes_)\n",
    "    wfx_l_outputs_pred = [x + \"_pred\" for x in wfx_l_outputs]\n",
    "    \n",
    "    l_analysis_df.loc[~l_training_mask, wfx_l_outputs_pred] = predict_wfx_l.predict_proba(l_analysis_df[~l_training_mask][input_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08315476-1997-41c8-9668-a4d2f450f311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0153ce0-a42e-42be-8509-2f0ae269e8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4946c9-f15b-4477-83d6-0712aef99623",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_analysis_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ea7ed-5f5b-4b5f-95f1-d34adc55643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.query('batterName == \"Jarren Duran\"').query('pitchHand == \"R\"')[['event', 'so_b', 'b1_b']].tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0517a-3d37-4d5c-aa6e-756068c66403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48eade1f-3053-4c35-a636-5354a1e434df",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0a386-0116-4447-ab3f-115dbc57b38c",
   "metadata": {},
   "source": [
    "$\\hat{event}$ = event_b_long + event_p_long + event_pfx + x_vect + y_vect + temperature + venue_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54645574-4eaa-408a-81dc-4411e6b1890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(analysis_df, venue_dummies, batSide, layers):\n",
    "    # Identify inputs\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    # Prepare\n",
    "    # Replace inf values with NaN (unlikely to occur in large samples)\n",
    "    analysis_df[variables] = analysis_df[variables].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    X = analysis_df.dropna(subset=variables)[variables].values  # Independent variables\n",
    "    y = analysis_df.dropna(subset=variables + ['eventsModel'])['eventsModel'].values  # Dependent variable\n",
    "\n",
    "    # Define three neural network models with slightly different configurations\n",
    "    nn_model_1 = MLPClassifier(hidden_layer_sizes=layers,activation='relu',solver='adam',max_iter=10,random_state=1)\n",
    "    nn_model_2 = MLPClassifier(hidden_layer_sizes=layers,activation='relu',solver='adam',max_iter=10,random_state=2)\n",
    "    nn_model_3 = MLPClassifier(hidden_layer_sizes=layers,activation='relu',solver='adam',max_iter=10,random_state=3)\n",
    "\n",
    "    # Create a Voting Classifier with the three models\n",
    "    voting_model = VotingClassifier([('nn1', nn_model_1),('nn2', nn_model_2),('nn3', nn_model_3)], voting='soft')\n",
    "\n",
    "    # Train the Voting Regressor\n",
    "    voting_model.fit(X, y)\n",
    "\n",
    "    # Create directory for saving the model\n",
    "    os.makedirs(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate), exist_ok=True)\n",
    "\n",
    "    # Save the Voting Classifier\n",
    "    with open(os.path.join(model_path, \"M01. Park and Weather Factors\", todaysdate, f\"predict_wfx_{batSide.lower()}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(voting_model, f)\n",
    "\n",
    "    print(f\"Voting model for {batSide}HB saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61ea85-c5aa-452f-a66b-affb10eb57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models(analysis_df, venue_dummies, \"L\", (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea2b95-d991-4585-a228-05565c31324c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb6dc96-4509-4489-af81-044f59255702",
   "metadata": {},
   "source": [
    "### Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4818e2-732d-4ba0-a0e7-5d91b220c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(df, base_rate_df, model_date, batSide):\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "    \n",
    "    \n",
    "    # Make predictions\n",
    "    # Path to the saved model for\n",
    "    saved_model_path = os.path.join(model_path, \"M01. Park and Weather Factors\", model_date, f\"predict_wfx_{batSide.lower()}.pkl\")\n",
    "\n",
    "    # Load the model\n",
    "    with open(saved_model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Create input dataframe\n",
    "    X = df.copy()\n",
    "\n",
    "    X[variables] = X[variables].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    X = X.dropna(subset=variables)\n",
    "\n",
    "    for event in events_list:\n",
    "        # Use league averages to predict (NOT BASE RATES) \n",
    "        X[f'{event}_b_long'] = X[f'{event}_lg'].astype(float).copy()\n",
    "        X[f'{event}_p_long'] = X[f'{event}_lg'].astype(float).copy()\n",
    "\n",
    "        \n",
    "    # Identify inputs\n",
    "    variables = ['x_vect', 'y_vect', 'temperature'] + venue_dummies\n",
    "    # Loop over events\n",
    "    for event in events_list: \n",
    "        # Define the dependent variable (e.g., `b1`) and independent variables\n",
    "        variables += [f'{event}_b_long', f'{event}_p_long', f'{event}_pfx']\n",
    "\n",
    "    # Extract the feature data\n",
    "    X = X[variables]\n",
    "    \n",
    "    # Predict using the loaded model\n",
    "    class_list = list(model.classes_)\n",
    "    prediction_columns = [f\"{event}_pred\" for event in class_list]\n",
    "    prediction_df = pd.DataFrame(model.predict_proba(X), columns=prediction_columns)\n",
    "    \n",
    "    # Append \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, prediction_df], axis=1)\n",
    "\n",
    "    # Calculate wfx\n",
    "    for event in events_list:\n",
    "        # Compare to base year (NOT LEAGUE AVERAGE)\n",
    "        df[f'{event}_wfx'] = df[f'{event}_pred'] / base_rate_df[event][0]\n",
    "    \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce64b02-0685-4239-a3a8-a5685d09eda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab1d5cc3-2fb9-4e6d-b4bd-bf0d59ab1335",
   "metadata": {},
   "source": [
    "### Multiplier Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0976c3-eb4d-42c0-8d1b-f959879c9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "model_date = \"20250319\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064024c-dd4c-4f9b-bd43-ec70f2db8f0c",
   "metadata": {},
   "source": [
    "##### Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6230d-e586-41a9-bb83-8518be94d322",
   "metadata": {},
   "source": [
    "Note: You only have to prepare once even if you retrain the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685842f-2890-4ded-a58e-238de1ff535a",
   "metadata": {},
   "source": [
    "Read in complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fe6d4-0455-4ff8-842b-4a017fc12ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# complete_dataset = create_pa_inputs(None, 2013, 2024, short=50, long=300, adjust=False)\n",
    "complete_dataset = create_pa_inputs(None, start_year=2020, end_year=2020, short=50, long=300, adjust=False, generate=True, write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884317c-d38c-4091-a24e-6a6e18755564",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb30ccb-56a6-4485-a9cb-4febf7e966ae",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98feac7a-d065-49fe-88f9-5f5ff295b359",
   "metadata": {},
   "source": [
    "Rerun this when you want to retrain models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d74153-bbbf-4a6c-b6f1-805a19fa3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wfx_df_list = []\n",
    "for batSide in ['L', 'R']:\n",
    "    if batSide == 'L':\n",
    "        analysis_df = analysis_df_list[0].copy()\n",
    "    else:\n",
    "        analysis_df = analysis_df_list[1].copy()\n",
    "    \n",
    "    # Drop missings\n",
    "    analysis_df = analysis_df.dropna()\n",
    "    \n",
    "    # Train models\n",
    "    if train == True:\n",
    "        train_models(analysis_df, venue_dummies, batSide, layers=(38,38,38,38,38))\n",
    "        \n",
    "    # Create dataset with wfx\n",
    "    wfx_df = run_predictions(analysis_df, base_rate_df, model_date, batSide)\n",
    "    wfx_df_list.append(wfx_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fc486-cbe9-4f7b-8555-97fb45b8b9bb",
   "metadata": {},
   "source": [
    "Separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284240f-872c-4c39-8002-e803713b69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhb_df = wfx_df_list[0].copy()\n",
    "rhb_df = wfx_df_list[1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccc099-80a9-4644-94d1-1ac52f123842",
   "metadata": {},
   "source": [
    "Scale predictions:\n",
    "- Numerator: Predicted rate\n",
    "- Denominator: Sum of all event predicted rates (should be close to one, but won't be exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb262ac4-48b3-4f4d-bd94-b36aa24fc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predictions\n",
    "pred_list = ['b1_pred', 'b2_pred', 'b3_pred', 'hr_pred', 'bb_pred', 'hbp_pred', 'so_pred', 'fo_pred', 'go_pred', 'lo_pred', 'po_pred']\n",
    "\n",
    "# Sum of prediction odds\n",
    "lhb_df['pred_sum'] = lhb_df[pred_list].sum(axis=1)\n",
    "rhb_df['pred_sum'] = rhb_df[pred_list].sum(axis=1)\n",
    "\n",
    "# Scaled\n",
    "for event in pred_list:\n",
    "    print(event)\n",
    "    lhb_df[event] = lhb_df[event] / lhb_df['pred_sum']\n",
    "    rhb_df[event] = rhb_df[event] / rhb_df['pred_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae24fa-a3e6-498f-8158-fca4a75c9e8e",
   "metadata": {},
   "source": [
    "Columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5640ce-d9c7-4297-b999-441dc32599aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = ['gamePk', 'game_date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature']\n",
    "pfx_list = [col for col in wfx_df_list[0].columns if col.endswith('pfx')]\n",
    "wfx_list = [col for col in wfx_df_list[0].columns if col.endswith('wfx')]\n",
    "pred_list = [col for col in wfx_df_list[0].columns if col.endswith('_pred')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3cc2c-4936-4fac-aa42-29b549694d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dummies = pd.get_dummies(lhb_df['eventsModel']).astype(int)\n",
    "lhb_df2 = pd.concat([lhb_df, event_dummies], axis=1)\n",
    "lhb_df2 = lhb_df2.groupby(keep_list)[events_list + pfx_list + wfx_list + pred_list].mean(numeric_only=True).reset_index()\n",
    "\n",
    "event_dummies = pd.get_dummies(rhb_df['eventsModel']).astype(int)\n",
    "rhb_df2 = pd.concat([rhb_df, event_dummies], axis=1)\n",
    "rhb_df2 = rhb_df2.groupby(keep_list)[events_list + pfx_list + wfx_list + pred_list].mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc729bf4-523a-4e4a-9c42-f2364a6b48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhb_df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a630e3e-d4f6-4c6c-bfb6-e1566135e0b6",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8fcdc-2452-4b26-894e-56a6919dc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df = pd.merge(lhb_df2, rhb_df2, on=keep_list, how='inner', suffixes=('_l', '_r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae2e9f-eb9e-4667-a15e-ed33e825daf9",
   "metadata": {},
   "source": [
    "Read in game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3f68a-3793-4078-9366-00a8efc725ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "game_df = create_games(\"20200101\", todaysdate, team_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e183e-1cd3-4712-aabb-a44ab9101784",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['date'] = game_df['date'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c712c8f-7478-4906-99d4-2f74af978ba1",
   "metadata": {},
   "source": [
    "Add date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c835b-1e3f-42e7-95b0-32257d43b791",
   "metadata": {},
   "source": [
    "Note: game_date currently in multiplier_df will have original date in cases of postponements. date in game_df will have the correct date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1590dfa-eb75-4ca6-b3bb-35a15ff0967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df = multiplier_df.merge(game_df[['game_id', 'date']], left_on='gamePk', right_on=['game_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274f582-d5cd-4a02-ba3f-5fdd53ea4169",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36869b1-172c-4dfa-94c1-64cbdb0e9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_df.sort_values(['date', 'gamePk'], ascending=[True, True]).to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a3489-5e5f-4b07-b847-4a12ee398261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7768c7ee-bde2-4a6e-ba41-20ac0cb8f23c",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f3178-069e-48c3-80d9-d3073aaf9de9",
   "metadata": {},
   "source": [
    "##### Rates by Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4377c-d362-46b5-907a-45ea1caaf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))  # 3 rows, 2 columns\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the events and their corresponding axes\n",
    "for idx, event in enumerate(events_list):\n",
    "    ax = axes[idx]  # Select the appropriate subplot\n",
    "    \n",
    "    # Step 1: Create quantile buckets for the current event\n",
    "    lhb_df2['quantile'] = pd.qcut(lhb_df2[f'{event}_pred'], q=10, labels=False)  # 10 quantiles (adjust q as needed)\n",
    "    \n",
    "    # Step 2: Group by quantiles and calculate the mean\n",
    "    quantile_means = lhb_df2.groupby('quantile').agg({f'{event}_pred': 'mean', event: 'mean'}).reset_index()\n",
    "    \n",
    "    # Step 3: Plot the predictions and actuals\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[f'{event}_pred'], label=f'Average {event}_pred', marker='o')\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[event], label=f'Average {event}', marker='x')\n",
    "    \n",
    "    # Add subplot details\n",
    "    ax.set_title(f'{event} Predictions vs Actuals')\n",
    "    ax.set_xlabel('Quantile')\n",
    "    ax.set_ylabel('Average Value')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4fc73-83bb-4565-89fc-ebb25c34da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))  # 3 rows, 2 columns\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the events and their corresponding axes\n",
    "for idx, event in enumerate(events_list):\n",
    "    ax = axes[idx]  # Select the appropriate subplot\n",
    "    \n",
    "    # Step 1: Create quantile buckets for the current event\n",
    "    rhb_df2['quantile'] = pd.qcut(rhb_df2[f'{event}_pred'], q=10, labels=False)  # 10 quantiles (adjust q as needed)\n",
    "    \n",
    "    # Step 2: Group by quantiles and calculate the mean\n",
    "    quantile_means = rhb_df2.groupby('quantile').agg({f'{event}_pred': 'mean', event: 'mean'}).reset_index()\n",
    "    \n",
    "    # Step 3: Plot the predictions and actuals\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[f'{event}_pred'], label=f'Average {event}_pred', marker='o')\n",
    "    ax.plot(quantile_means['quantile'], quantile_means[event], label=f'Average {event}', marker='x')\n",
    "    \n",
    "    # Add subplot details\n",
    "    ax.set_title(f'{event} Predictions vs Actuals')\n",
    "    ax.set_xlabel('Quantile')\n",
    "    ax.set_ylabel('Average Value')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b7a6f-68c9-4be1-8788-2e712abb4501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9677be-a9c6-4baa-8fca-5a1bb7ca3604",
   "metadata": {},
   "source": [
    "##### Yearly Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0df0e-4177-4145-ad18-549c095b421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhb_df2['year'] = lhb_df2['game_date'].str[:4]\n",
    "event = 'hr'\n",
    "lhb_df2.groupby('year')[[event, f'{event}_pred',  f'{event}_pfx', f'{event}_wfx']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480728a-c4ef-4512-912f-f591b47b81df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ba8413-fce2-4ec9-9f9e-a2cbcdbeaba7",
   "metadata": {},
   "source": [
    "##### Park Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f0974-bdb9-4fed-a17a-243c5a8a0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhb_df2['safe'] = rhb_df2[['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']].sum(axis=1)\n",
    "rhb_df2['out'] = rhb_df2[['so', 'go', 'lo', 'po', 'fo']].sum(axis=1)\n",
    "rhb_df2['safe_pred'] = rhb_df2[['b1_pred', 'b2_pred', 'b3_pred', 'hr_pred', 'bb_pred', 'hbp_pred']].sum(axis=1)\n",
    "rhb_df2['out_pred'] = rhb_df2[['so_pred', 'go_pred', 'lo_pred', 'po_pred', 'fo_pred']].sum(axis=1)\n",
    "\n",
    "lhb_df2['safe'] = lhb_df2[['b1', 'b2', 'b3', 'hr', 'bb', 'hbp']].sum(axis=1)\n",
    "lhb_df2['out'] = lhb_df2[['so', 'go', 'lo', 'po', 'fo']].sum(axis=1)\n",
    "lhb_df2['safe_pred'] = lhb_df2[['b1_pred', 'b2_pred', 'b3_pred', 'hr_pred', 'bb_pred', 'hbp_pred']].sum(axis=1)\n",
    "lhb_df2['out_pred'] = lhb_df2[['so_pred', 'go_pred', 'lo_pred', 'po_pred', 'fo_pred']].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db0ccd-5ce7-48f7-8487-b5a61dfca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_error = rhb_df2[rhb_df2['venue_id'].astype('int').isin(team_map['VENUE_ID'])].groupby('venue_id')[['safe', 'safe_pred']].mean()\n",
    "park_error['diff'] = park_error['safe'] - park_error['safe_pred']\n",
    "park_error.sort_values('diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338c5e0-a6e8-41cc-a99a-b029c8cf3475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3404f8d9-448c-4b36-b294-ac336399b0ab",
   "metadata": {},
   "source": [
    "##### Park and Park x Weather Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e2dfe-f97e-435a-8aad-b3a05216dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhb_df2.drop_duplicates('venue_id', keep='last')[['venue_id'] + [col for col in rhb_df2.columns if col.endswith(\"_pfx\")] + [col for col in rhb_df2.columns if col.endswith(\"_wfx\")]].sort_values('venue_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034e2c3-d64c-4111-b64c-0aaf1e0a1b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511398-2b77-4594-9155-2c5aaf04465d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2a6a58-f18b-48da-8b63-7da94a3ee58f",
   "metadata": {},
   "source": [
    "### Generate Park and Weather Factors files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddb0af-72f3-43f8-9ae0-093e9fd902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4175ca8-9421-4ca8-94b9-fb45bcbac3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to keep\n",
    "keep_columns = ['gamePk', 'game_date', 'date', 'venue_id', 'away_name', 'home_name', 'x_vect', 'y_vect', 'temperature'] + [col for col in multiplier_dataset.columns if \"_wfx\" in col]\n",
    "    \n",
    "multiplier_dataset.sort_values('date', inplace=True)\n",
    "for date in multiplier_dataset[pd.to_datetime(multiplier_dataset['game_date']).dt.year >= 2022]['date'].unique():\n",
    "    print(date)\n",
    "    if date > \"20220101\":\n",
    "        # Subset by date\n",
    "        daily_weather_df = multiplier_dataset[multiplier_dataset['date'] == date][keep_columns]\n",
    "\n",
    "        # Write to CSV\n",
    "        daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"3. Park and Weather Factors\", f\"{date} Park and Weather Factors.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54965ad8-c7cc-4a0c-9184-acf9132abeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46164f-30c5-4173-9e38-7829577d3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf420e-1c6a-4249-999c-32b3c3232173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0627957-32c5-40e1-a759-d9b92f28ff3b",
   "metadata": {},
   "source": [
    "### Note: Rerun B01. Matchups.ipynb if new historic Park x Weather Effects are generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
