{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac54bd7-8f81-483e-9497-9ce724e3bfaa",
   "metadata": {},
   "source": [
    "# Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd73237-db59-48b4-b1e1-2bb16a0de377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"U1. Imports.ipynb\"\n",
    "%run \"U2. Utilities.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b31b503-b5c2-41e8-bb9d-2356cd789ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import log_loss, classification_report, f1_score, make_scorer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from tensorflow import keras\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159faf23-0371-4cd0-838a-e93e91abda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads in Chadwick register with player codes.\n",
    "keep_list = ['key_mlbam', 'key_fangraphs', 'key_bbref_minors', 'key_bbref', 'name_first', 'name_last']\n",
    "chadwick = read_chadwick(keep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9426c9-cb8f-410a-8f77-7cd9082b0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads in a map of team name, codes, and the shorthand MLB uses in their URLs\n",
    "team_map = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"Team Map.csv\"))\n",
    "\n",
    "# We just need teams right now\n",
    "team_map = team_map[['FULLNAME', 'BBREFTEAM', 'MLBURL', 'FANGRAPHSTEAM', 'VENUE_ID', 'SFBBTEAM', 'DKTEAM', 'ROTOWIRETEAM', 'FANPROSTEAM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d160d-01e3-4900-9cb4-f3c06af94e82",
   "metadata": {},
   "source": [
    "# Create sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806063e4-6d01-400a-bb54-0a69889b1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"04. Dataset.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b208a161-1069-4ca8-aad9-dfcf3ce4154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sample, up until today's date\n",
    "sample = create_model_input(todaysdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5bcda-0f05-40ba-989a-746862bac8bb",
   "metadata": {},
   "source": [
    "### FanGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d7168b-b42b-4936-956b-a71044601dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all FanGraphs projections together and save it as a CSV\n",
    "batters_list = []\n",
    "# Loop over all FanGraphs files\n",
    "for filename in os.listdir(r\"C:\\Users\\james\\Documents\\MLB\\Data2\\7. Stats\\B. Clean FanGraphs\\Batters\"):\n",
    "    # Extract date\n",
    "    date = filename[12:20]\n",
    "    # Read in dataframe\n",
    "    df = pd.read_csv(os.path.join(baseball_path, \"7. Stats\", \"B. Clean FanGraphs\", \"Batters\", filename), encoding='iso-8859-1')\n",
    "    # Add date column\n",
    "    df['date'] = date\n",
    "    \n",
    "    # Append dataframe to list\n",
    "    batters_list.append(df)\n",
    "    \n",
    "# Create combined dataframe\n",
    "batters_fg_sample = pd.concat(batters_list, axis=0)\n",
    "\n",
    "# Write to CSV\n",
    "batters_fg_sample.to_csv(os.path.join(baseball_path, \"Inputs\", \"Batters FanGraphs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5588e69-381e-4726-abfe-9bfbfd8e6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all FanGraphs projections together and save it as a CSV\n",
    "pitchers_list = []\n",
    "# Loop over all FanGraphs files\n",
    "for filename in os.listdir(r\"C:\\Users\\james\\Documents\\MLB\\Data2\\7. Stats\\B. Clean FanGraphs\\Pitchers\"):\n",
    "    # Extract date\n",
    "    date = filename[13:21]\n",
    "    # Read in dataframe\n",
    "    df = pd.read_csv(os.path.join(baseball_path, \"7. Stats\", \"B. Clean FanGraphs\", \"Pitchers\", filename), encoding='iso-8859-1')\n",
    "    # Create date column\n",
    "    df['date'] = date\n",
    "    \n",
    "    try:\n",
    "        # Depending on the origin of the file (Steamer vs. FanGraphs), you may need to rename certain variables\n",
    "        df.rename(columns={'H9':'H/9', 'HR9':'HR/9', 'K9':'K/9', 'BB9':'BB/9'}, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Append dataframe to list\n",
    "    pitchers_list.append(df)\n",
    "    \n",
    "# Create combined dataframe\n",
    "pitchers_fg_sample = pd.concat(pitchers_list, axis=0)\n",
    "\n",
    "# Write to CSV\n",
    "pitchers_fg_sample.to_csv(os.path.join(baseball_path, \"Inputs\", \"Pitchers FanGraphs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc40a4a-ab18-49b5-ae33-590879f94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in FanGraphs batter projections for each day\n",
    "batters_fg_sample = pd.read_csv(os.path.join(baseball_path, \"Inputs\", \"Batters FanGraphs.csv\"))\n",
    "# Convert date to string for merge\n",
    "batters_fg_sample['date'] = batters_fg_sample['date'].astype('str')\n",
    "\n",
    "# Merge sample data (Stats API and Statcast) with projections (Fangraphs)\n",
    "sample = sample.merge(batters_fg_sample, left_on=['batter', 'date'], right_on=['mlbamid', 'date'], how='inner', suffixes=(\"\", \"_b\"))\n",
    "# Delete to clear up space\n",
    "del batters_fg_sample\n",
    "\n",
    "# Read in FanGraphs pitcher projections for each day\n",
    "pitchers_fg_sample = pd.read_csv(os.path.join(baseball_path, \"Inputs\", \"Pitchers FanGraphs.csv\"))\n",
    "# Convert date to string for merge\n",
    "pitchers_fg_sample['date'] = pitchers_fg_sample['date'].astype('str')\n",
    "\n",
    "# Merge sample data (Stats API and Statcast) with projections (Fangraphs)\n",
    "sample = sample.merge(pitchers_fg_sample, left_on=['pitcher', 'date'], right_on=['mlbamid', 'date'], how='inner', suffixes=(\"\", \"_p\"))\n",
    "# Delete to clear up space\n",
    "del pitchers_fg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f35a0-8398-4fda-a00f-36f978c64b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10902f-1b78-459d-86b6-ab4c6cc58b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00e3f24d-c10a-4d4d-b44d-22cb150d5652",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16f35d-f94d-46f9-8d0f-990192b746fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of small samples when training\n",
    "# Important: Figure out if you want this!\n",
    "# sample = sample.query('pa_b_long >= 40').query('pa_p_long >= 40')\n",
    "\n",
    "# Get rid of PA outcomes that are not valid outputs\n",
    "sample = sample.query('eventsModel != \"Cut\"').reset_index(drop=True)\n",
    "\n",
    "# Count outs\n",
    "sample['is_out'] = sample[['so', 'fo', 'go', 'lo', 'po']].sum(axis=1)\n",
    "# Rounding is necessary because SOs are adjusted for park factors, so they might be just above or just below 1.\n",
    "# This isn't an amazing solution, so I could probably do this more cleanly\n",
    "sample['is_out'] = sample['is_out'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677c531-6b77-44fa-96fa-3a8f9e4afad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to models folder \n",
    "os.chdir(r\"C:\\Users\\james\\Documents\\MLB\\Code\\Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890883f0-e1e6-42d3-8fc0-47c20adeac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove early rows because they'll treat all players like rookies\n",
    "sample = sample.drop(index=sample.index[:10000])\n",
    "sample.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247758c-610f-4367-8abe-a4486ac5c431",
   "metadata": {},
   "source": [
    "### Standardize FG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582af464-cd2a-4517-955b-e9ace4e9c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "batter_stats_fg_scaled = scaler.fit_transform(sample[batter_stats_fg])\n",
    "batter_stats_fg_scaled = pd.DataFrame(batter_stats_fg_scaled, columns=batter_stats_fg)\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "scaler_filename = \"batter_stats_fg_scaler.pkl\"\n",
    "with open(scaler_filename, \"wb\") as file:\n",
    "    pickle.dump(scaler, file)\n",
    "    \n",
    "# Standardize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "pitcher_stats_fg_scaled = scaler.fit_transform(sample[pitcher_stats_fg])\n",
    "pitcher_stats_fg_scaled = pd.DataFrame(pitcher_stats_fg_scaled, columns=pitcher_stats_fg)\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "scaler_filename = \"pitcher_stats_fg_scaler.pkl\"\n",
    "with open(scaler_filename, \"wb\") as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab0281-4f00-407c-ac01-213b2972b008",
   "metadata": {},
   "source": [
    "### Standardize Stats API and Statcast Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd2561-d138-4899-9b33-e548762c4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "batter_stats_scaled = scaler.fit_transform(sample[batter_stats])\n",
    "batter_stats_scaled = pd.DataFrame(batter_stats_scaled, columns=batter_stats)\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "scaler_filename = \"batter_stats_scaler.pkl\"\n",
    "with open(scaler_filename, \"wb\") as file:\n",
    "    pickle.dump(scaler, file)\n",
    "    \n",
    "# Standardize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "pitcher_stats_scaled = scaler.fit_transform(sample[pitcher_stats])\n",
    "pitcher_stats_scaled = pd.DataFrame(pitcher_stats_scaled, columns=pitcher_stats)\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "scaler_filename = \"pitcher_stats_scaler.pkl\"\n",
    "with open(scaler_filename, \"wb\") as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279eb90-a9f3-4961-bd10-05e188cd6c17",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbe187-dc34-4297-9552-a90dcb6a9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working dataset\n",
    "# Extra variables\n",
    "model_extra_vars = venues + years + other_list \n",
    "extra_variable_df = sample[model_extra_vars]\n",
    "\n",
    "# Event variables\n",
    "eventsModel_df = sample[['pa_b', 'pa_p', 'year', 'is_out', 'eventsModel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa09c2-d04a-4fde-b353-4a6725f86f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all together\n",
    "df = pd.concat([batter_stats_scaled, pitcher_stats_scaled, batter_stats_fg_scaled, pitcher_stats_fg_scaled, extra_variable_df, eventsModel_df], axis=1)\n",
    "# Since stats are normalized, this should just assume league average when missing\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a20280-1565-436b-9fc3-23bbe878c1e6",
   "metadata": {},
   "source": [
    "### Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdef358-0e4b-472a-9db6-a5912af4ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batter_stats_fg2 = batter_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "# # Create a copy of the DataFrame with only relevant columns\n",
    "# df_filtered = df[batter_stats_fg2 + batter_stats + ['pa_b']].copy()\n",
    "\n",
    "# # Drop rows with missing values in the features or target columns\n",
    "# df_filtered.dropna(subset=batter_stats_fg2 + batter_stats, inplace=True)\n",
    "\n",
    "# # Separate the features (batter_stats_fg2) and target (batter_stats) columns\n",
    "# features = df_filtered[batter_stats_fg2]\n",
    "# target = df_filtered[batter_stats]\n",
    "\n",
    "# # Create and fit the model\n",
    "# model = keras.Sequential([\n",
    "#     keras.layers.Dense(25, activation='relu', input_shape=(len(batter_stats_fg2),)),\n",
    "#     keras.layers.Dense(25, activation='relu'),\n",
    "#     keras.layers.Dense(25, activation='relu'),\n",
    "#     keras.layers.Dense(len(batter_stats))  # Output layer with the same number of units as the target columns\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(features, target, epochs=20, batch_size=32)\n",
    "\n",
    "# # Pickle\n",
    "# model_filename = \"batter_imputations.pkl\"\n",
    "# with open(model_filename, \"wb\") as file:\n",
    "#     pickle.dump(model, file)\n",
    "    \n",
    "# # Use the trained model to make predictions\n",
    "# prediction = model.predict(df.loc[df['pa_b'] < 40, batter_stats_fg2])\n",
    "\n",
    "# # Impute missing values in batter_stats with the predicted values\n",
    "# df.loc[df['pa_b'] < 40, batter_stats] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3103d-25da-4412-8fe4-3d488c92c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitcher_stats_fg2 = pitcher_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "# # Create a copy of the DataFrame with only relevant columns\n",
    "# df_filtered = df[pitcher_stats_fg2 + pitcher_stats + ['pa_p']].copy()\n",
    "\n",
    "# # Drop rows with missing values in the features or target columns\n",
    "# df_filtered.dropna(subset=pitcher_stats_fg2 + pitcher_stats, inplace=True)\n",
    "\n",
    "# # Separate the features (pitcher_stats_fg) and target (pitcher_stats) columns\n",
    "# features = df_filtered[pitcher_stats_fg2]\n",
    "# target = df_filtered[pitcher_stats]\n",
    "\n",
    "# # Create and fit the model\n",
    "# model = keras.Sequential([\n",
    "#     keras.layers.Dense(25, activation='relu', input_shape=(len(pitcher_stats_fg2),)),\n",
    "#     keras.layers.Dense(25, activation='relu'),\n",
    "#     keras.layers.Dense(25, activation='relu'),\n",
    "#     keras.layers.Dense(len(pitcher_stats))  # Output layer with the same number of units as the target columns\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# # Train the model\n",
    "# # model.fit(features_imputed, target, epochs=5, batch_size=32)\n",
    "# model.fit(features, target, epochs=20, batch_size=32)\n",
    "\n",
    "# # Use the trained model to make predictions\n",
    "# prediction = model.predict(df.loc[df['pa_p'] < 40, pitcher_stats_fg2])\n",
    "\n",
    "# # Pickle\n",
    "# model_filename = \"pitcher_imputations.pkl\"\n",
    "# with open(model_filename, \"wb\") as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "# # Impute missing values in pitcher_stats with the predicted values\n",
    "# df.loc[df['pa_p'] < 40, pitcher_stats] = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d08be1-a64c-4c57-8a25-30e2ff9adaf3",
   "metadata": {},
   "source": [
    "# Impute (for re-running without retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785cf58-2b57-47cc-88d8-def2f18b29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in batter imputation model\n",
    "kmeans_model_filename = \"batter_imputations.pkl\"\n",
    "with open(kmeans_model_filename, \"rb\") as file:\n",
    "    batter_kmeans = pickle.load(file)\n",
    "    \n",
    "# Add handedness to FanGraphs stats\n",
    "batter_stats_fg2 = batter_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "# Use FanGraphs stats to predict API/Statcast stats for those with limited samples\n",
    "prediction = batter_kmeans.predict(df.loc[df['pa_b'] < 40, batter_stats_fg2])\n",
    "\n",
    "# Impute missing values in batter_stats with the predicted values\n",
    "# df.loc[df['pa_b'] < 40, batter_stats] = prediction\n",
    "df.loc[df['pa_b'] < 40, batter_stats] = prediction[:sum(df['pa_b'] < 40)]\n",
    "\n",
    "\n",
    "\n",
    "# Read in pitcher imputation model\n",
    "kmeans_model_filename = \"pitcher_imputations.pkl\"\n",
    "with open(kmeans_model_filename, \"rb\") as file:\n",
    "    pitcher_kmeans = pickle.load(file)\n",
    "    \n",
    "# Add handedness to FanGraphs stats\n",
    "pitcher_stats_fg2 = pitcher_stats_fg + ['b_L', 'p_L']\n",
    "    \n",
    "# Use FanGraphs stats to predict API/Statcast stats for those with limited samples\n",
    "prediction = pitcher_kmeans.predict(df.loc[df['pa_p'] < 40, pitcher_stats_fg2])\n",
    "\n",
    "# Impute missing values in pitcher_stats with the predicted values\n",
    "# df.loc[df['pa_p'] < 40, pitcher_stats] = prediction\n",
    "df.loc[df['pa_p'] < 40, batter_stats] = prediction[:sum(df['pa_p'] < 40)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6ddfd-3fed-4fae-867d-505c1ebed166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imputation flags (could move this up, might make more sense)\n",
    "df['imp_b'] = (df['pa_b'] < 40).astype('int')\n",
    "df['imp_p'] = (df['pa_p'] < 40).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d756e1-8e2a-45c7-98df-b13bceff20eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e29871a-8523-4aac-ad8a-8fa0685b32bb",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a42a3-972a-4803-a298-f2b2128136d8",
   "metadata": {},
   "source": [
    "##### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5881b5a-7972-4ba8-86ab-014162177881",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = batter_stats + pitcher_stats + venues + years + other_list + ['pa_b', 'pa_p', 'imp_b', 'imp_p', 'year', 'is_out', 'eventsModel']\n",
    "model_dataset = df[keep_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f350ba-18e3-436a-ba40-b00690e21ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset = model_dataset[model_dataset['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()\n",
    "safe_dataset = model_dataset[~model_dataset['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c69e01-dc78-4cfc-ab35-e048259c3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing groups\n",
    "X_train = model_dataset.groupby(model_dataset['year']).apply(lambda x: x.head(int(len(x)*2/3)))\n",
    "X_test = model_dataset.groupby(model_dataset['year']).apply(lambda x: x.tail(int(len(x)*1/3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19fa37-7906-4683-a424-b1becd7eb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset_train = X_train[X_train['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()\n",
    "safe_dataset_train = X_train[~X_train['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7127336-ef02-449f-b133-c0b12cf929c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dataset_test = X_test[X_test['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()\n",
    "safe_dataset_test = X_test[~X_test['eventsModel'].isin(['so', 'lo', 'go', 'fo', 'po'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e1a7f-3ded-4038-9651-7d5a9b267986",
   "metadata": {},
   "source": [
    "### Out vs. Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c3a94-1685-44d3-89c5-028e5482dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7abdf95a-a430-45d7-b43d-cef40cb970f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs + ['imp_b', 'imp_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c5392-30b1-4531-9e62-44eb51034e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "solver = 'lbfgs'\n",
    "\n",
    "iters = 200\n",
    "\n",
    "filename = \"model_binary_\" + \"voting\" + \"_100_new.sav\"\n",
    "\n",
    "print(filename)\n",
    "\n",
    "# Define the individual models in the ensemble\n",
    "models = [\n",
    "    LogisticRegression(solver='lbfgs', max_iter=20),  \n",
    "    LogisticRegression(solver='saga', max_iter=20),   \n",
    "    MLPClassifier(hidden_layer_sizes=(100,100), activation='relu', random_state=1, max_iter=15),  \n",
    "]\n",
    "\n",
    "\n",
    "# Create the ensemble classifier using VotingClassifier\n",
    "# model_binary = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(model_dataset[inputs], model_dataset[['is_out']].values.ravel())\n",
    "model_binary = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(X_train[inputs], X_train[['is_out']].values.ravel())\n",
    "# model_binary = LogisticRegression(solver=solver, max_iter=iters).fit(X_train[inputs], X_train[['is_out']].values.ravel())\n",
    "\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_binary, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8fb18a-de6c-450c-a011-c10a70aba9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model_binary.predict_proba(X_test[inputs])\n",
    "X_test['is_safe_pred'] = proba[:, 0]  # Assign the first column of probabilities\n",
    "X_test['is_out_pred'] = proba[:, 1]  # Assign the second column of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e59feb-6d1b-40b8-aba7-613a317b1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dataset['decile'] = pd.qcut(model_dataset['is_out_pred'], 10, labels=False)\n",
    "\n",
    "# df_name = \"is_out\" + \"_df\"\n",
    "# globals()[df_name] = model_dataset.groupby('decile').mean().reset_index()\n",
    "\n",
    "X_test['decile'] = pd.qcut(X_test['is_out_pred'], 5, labels=False)\n",
    "\n",
    "df_name = \"is_out\" + \"_df\"\n",
    "globals()[df_name] = X_test.query('imp_b == 0').query('imp_p == 0').groupby('decile').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c7642-5d5b-4785-8852-d8fdcdc0ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(is_out_df['decile'], is_out_df['is_out_pred'], color='red')\n",
    "plt.plot(is_out_df['decile'], is_out_df['is_out'], color='black')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b3027-06c5-4fe5-8341-dc5480d2b503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e471c-b55c-4ec7-ad3c-f204a54b6fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00202a4-b193-4c7e-9ddf-63badbcb1fc0",
   "metadata": {},
   "source": [
    "### Outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91c61bb5-4d2d-441e-97a6-f1754203b3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_outs__relu303030_15_100.sav\n",
      "CPU times: total: 1.27 s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "layers = (30,30,30)\n",
    "# layers = (25,25,25,25,25)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "activation = 'relu'\n",
    "\n",
    "iters = 15\n",
    "\n",
    "filename = \"model_outs_\" + \"_\" + activation + layers_str + \"_\" + str(iters) + \"_100.sav\"\n",
    "print(filename)\n",
    "# Define the individual models in the ensemble\n",
    "models = [\n",
    "    MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=1, max_iter=iters),\n",
    "]\n",
    "\n",
    "# Create the ensemble classifier using VotingClassifier\n",
    "# model_outs = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(outs_dataset[inputs], outs_dataset[['eventsModel']].values.ravel())\n",
    "model_outs = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(outs_dataset_train[inputs], outs_dataset_train[['eventsModel']].values.ravel())\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_outs, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbaf7e-5929-4b66-8bdc-5e81e2231936",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_outputs = list(model_outs.classes_)\n",
    "outs_outputs_pred = [x + \"_pred\" for x in outs_outputs]\n",
    "outs_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a517b9c-53bb-43a1-8735-894f671c431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs_dataset[outs_outputs_pred] = model_outs.predict_proba(outs_dataset[inputs])\n",
    "# outs_dataset_test[outs_outputs_pred] = model_outs.predict_proba(outs_dataset_test[inputs])\n",
    "\n",
    "proba = model_outs.predict_proba(outs_dataset_test[inputs])\n",
    "for i, col in enumerate(outs_outputs_pred):\n",
    "    outs_dataset_test[f'{col}'] = proba[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca7a3e-afd3-4823-a017-75317bb6b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create deciles\n",
    "# for var in outs_outputs:\n",
    "#     outs_dataset[f'{var}_act'] = (outs_dataset['eventsModel'] == var).astype('int')\n",
    "#     outs_dataset['decile'] = pd.qcut(outs_dataset[f'{var}_pred'], 10, labels=False)\n",
    "#     df_name = var + \"_df\"\n",
    "#     globals()[df_name] = outs_dataset.groupby('decile').mean().reset_index()\n",
    "    \n",
    "# Create deciles\n",
    "for var in outs_outputs:\n",
    "    outs_dataset_test[f'{var}_act'] = (outs_dataset_test['eventsModel'] == var).astype('int')\n",
    "    outs_dataset_test['decile'] = pd.qcut(outs_dataset_test[f'{var}_pred'], 10, labels=False)\n",
    "    df_name = var + \"_df\"\n",
    "    globals()[df_name] = outs_dataset_test.query('imp_b == 0').query('imp_p == 0').groupby('decile').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f1793-f2e3-40df-86c1-e47b5f521c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, var in enumerate(outs_outputs):\n",
    "    row = i // 3  # Calculate the row index based on the iteration\n",
    "    col = i % 3   # Calculate the column index based on the iteration\n",
    "    df_name = var + \"_df\"\n",
    "    axs[row, col].plot(globals()[df_name]['decile'], globals()[df_name][f'{var}_pred'], color='red')\n",
    "    axs[row, col].plot(globals()[df_name]['decile'], globals()[df_name][f'{var}_act'], color='black')\n",
    "    axs[row, col].set_title(var)\n",
    "    # axs[row, col].set_ylim(0,0.35)\n",
    "\n",
    "\n",
    "# Add some space between subplots to prevent overlapping\n",
    "fig.tight_layout(pad=.0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92fe9f-3f15-4a09-85d1-bbd5e9ef45cb",
   "metadata": {},
   "source": [
    "### Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8352dc-4265-4309-9878-77b69d5ee00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "layers = (30,30,30,30,30)\n",
    "# layers = (25,25,25,25,25)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "model = \"safe\"\n",
    "iters = 15\n",
    "alpha = 0.0001\n",
    "activation = 'relu'\n",
    "short = 100\n",
    "\n",
    "\n",
    "# inputs = batter_stats_safe + pitcher_stats_safe + batter_stats_safe_long + pitcher_stats_safe_long + venues + years + other_list\n",
    "\n",
    "filename = \"model_\" + model + \"_\" + activation + \"_\" + layers_str + \"_\" + str(iters) + \"_\" + str(short) + \".sav\"\n",
    "print(filename)\n",
    "# Define the individual models in the ensemble\n",
    "models = [\n",
    "    MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=1, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=2, max_iter=iters),\n",
    "    MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=3, max_iter=iters),\n",
    "    # MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=15, max_iter=iters),\n",
    "]\n",
    "\n",
    "# Create the ensemble classifier using VotingClassifier\n",
    "# model_safe = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(safe_dataset[inputs], safe_dataset[['eventsModel']].values.ravel())\n",
    "model_safe = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(safe_dataset_train[inputs], safe_dataset_train[['eventsModel']].values.ravel())\n",
    "# model_safe = MLPClassifier(hidden_layer_sizes=(layers), activation=activation, verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=7, max_iter=iters).fit(safe_dataset_train[inputs], safe_dataset_train[['eventsModel']].values.ravel())\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_safe, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b28dd-9413-4438-a7f1-2cba198202b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4c180-0364-4838-be54-3e97e2c85d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_outputs = list(model_safe.classes_)\n",
    "safe_outputs_pred = [x + \"_pred\" for x in safe_outputs]\n",
    "safe_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5870f04-8377-4a70-a243-7994fbcc27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe_dataset[safe_outputs_pred] = model_safe.predict_proba(safe_dataset[inputs])\n",
    "# safe_dataset_test[safe_outputs_pred] = model_safe.predict_proba(safe_dataset_test[inputs])\n",
    "\n",
    "proba = model_safe.predict_proba(safe_dataset_test[inputs])\n",
    "for i, col in enumerate(safe_outputs_pred):\n",
    "    safe_dataset_test[f'{col}'] = proba[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb223d-6a80-4617-90a2-ffe5b597323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create deciles\n",
    "# for var in safe_outputs:\n",
    "#     safe_dataset[f'{var}_act'] = (safe_dataset['eventsModel'] == var).astype('int')\n",
    "#     safe_dataset['decile'] = pd.qcut(safe_dataset[f'{var}_pred'], 10, labels=False)\n",
    "#     df_name = var + \"_df\"\n",
    "#     globals()[df_name] = safe_dataset.groupby('decile').mean().reset_index()\n",
    "    \n",
    "# Create deciles\n",
    "for var in safe_outputs:\n",
    "    safe_dataset_test[f'{var}_act'] = (safe_dataset_test['eventsModel'] == var).astype('int')\n",
    "    safe_dataset_test['decile'] = pd.qcut(safe_dataset_test[f'{var}_pred'], 10, labels=False)\n",
    "    df_name = var + \"_df\"\n",
    "    globals()[df_name] = safe_dataset_test.query('imp_b == 0').query('imp_p == 0').groupby('decile').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f225da1-6003-408c-aafa-8c90ecd46400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, var in enumerate(safe_outputs):\n",
    "    row = i // 3  # Calculate the row index based on the iteration\n",
    "    col = i % 3   # Calculate the column index based on the iteration\n",
    "    df_name = var + \"_df\"\n",
    "    axs[row, col].plot(globals()[df_name]['decile'], globals()[df_name][f'{var}_pred'], color='red')\n",
    "    axs[row, col].plot(globals()[df_name]['decile'], globals()[df_name][f'{var}_act'], color='black')\n",
    "    axs[row, col].set_title(var)\n",
    "    # axs[row, col].set_ylim(globals()[df_name][f'{var}_act'].min(),globals()[df_name][f'{var}_act'].max())\n",
    "\n",
    "\n",
    "# Add some space between subplots to prevent overlapping\n",
    "fig.tight_layout(pad=.0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de560f-0208-4d47-9019-a1615e1ea564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c8801-717b-43ff-99a6-f8d93b98fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "breaksfadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b4a50-acaa-4ae7-8ed7-c586accf3c54",
   "metadata": {},
   "source": [
    "# Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a6bfe-47d4-4852-87c8-78a6ea90d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "layers = (25,25)\n",
    "layers_str = ''.join(str(x) for x in layers)\n",
    "model = \"full\"\n",
    "iters = 10\n",
    "alpha = 0.0001\n",
    "activation = 'relu'\n",
    "short = 100\n",
    "\n",
    "filename = \"model_\" + model + \"_\" + activation + \"_\" + layers_str + \"_\" + str(iters) + \"_\" + str(short) + \".sav\"\n",
    "print(filename)\n",
    "\n",
    "# # Define the individual models in the ensemble\n",
    "# models = [\n",
    "#     MLPClassifier(hidden_layer_sizes=(layers), activation='relu', verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=1, max_iter=iters),\n",
    "# ]\n",
    "\n",
    "# # Create the ensemble classifier using VotingClassifier\n",
    "# # model_binary = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(model_dataset[inputs], model_dataset[['is_out']].values.ravel())\n",
    "# model_full = VotingClassifier(estimators=[('model'+str(i+1), model) for i, model in enumerate(models)], voting='soft', n_jobs=-2).fit(X_train[inputs], X_train[['eventsModel']].values.ravel())\n",
    "\n",
    "model_full = MLPClassifier(hidden_layer_sizes=(layers), activation='relu', verbose=True, alpha=0.0001, early_stopping=True, validation_fraction=0.1, random_state=1, max_iter=iters).fit(X_train[inputs], X_train[['eventsModel']].values.ravel())\n",
    "\n",
    "\n",
    "# Save model\n",
    "pickle.dump(model_full, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c7b41-78be-468d-ac0b-aad8940005a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_outputs = list(model_full.classes_)\n",
    "full_outputs_pred = [x + \"_pred\" for x in full_outputs]\n",
    "full_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ec30c-33b3-41f3-bc36-384c6472028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dataset[full_outsputs_pred] = model_full.predict_proba(model_dataset[inputs])\n",
    "X_test[full_outputs_pred] = model_full.predict_proba(X_test[inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15d1bf-b6d6-4521-828a-15fd5aa1efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create deciles\n",
    "# for var in full_outputs:\n",
    "#     model_dataset[f'{var}_act'] = (model_dataset['eventsModel'] == var).astype('int')\n",
    "#     model_dataset['decile'] = pd.qcut(model_dataset[f'{var}_pred'], 10, labels=False)\n",
    "#     df_name = var + \"_df\"\n",
    "#     globals()[df_name] = model_dataset.groupby('decile').mean().reset_index()\n",
    "    \n",
    "# Create deciles\n",
    "for var in full_outputs:\n",
    "    X_test[f'{var}_act'] = (X_test['eventsModel'] == var).astype('int')\n",
    "    X_test['decile'] = pd.qcut(X_test[f'{var}_pred'], 10, labels=False)\n",
    "    df_name = var + \"_df\"\n",
    "    globals()[df_name] = X_test.groupby('decile').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497bf9b-936c-495a-97eb-cd42703e725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures\n",
    "fig, axs = plt.subplots(4, 3, figsize=(12, 16))\n",
    "\n",
    "for i, var in enumerate(full_outputs):\n",
    "    row = i // 3  # Calculate the row index based on the iteration\n",
    "    col = i % 3   # Calculate the column index based on the iteration\n",
    "    df_name = var + \"_df\"\n",
    "    axs[row, col].plot(globals()[df_name]['decile'], globals()[df_name][f'{var}_pred'], color='red')\n",
    "    axs[row, col].plot(globals()[df_name]['decile'], globals()[df_name][f'{var}_act'], color='black')\n",
    "    axs[row, col].set_title(var)\n",
    "    # axs[row, col].set_ylim(0,0.67)\n",
    "\n",
    "\n",
    "# Add some space between subplots to prevent overlapping\n",
    "fig.tight_layout(pad=.0)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ff7bd-f979-46c4-a3e6-80d471a5df89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeaae94-b8d4-4d7e-81c5-1286ae3e26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: create a way to calculate probabilities for individual matchups so you can test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
