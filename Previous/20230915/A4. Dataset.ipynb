{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5527c04d-632c-4c1a-a1bc-14b1064d47bc",
   "metadata": {},
   "source": [
    "# A4. Dataset\n",
    "Source: A3. Raw API <br>\n",
    "\n",
    "Description: This creates usable datasets from the Raw API data <br>\n",
    "Main outputs include batter and pitcher model inputs, neural network PA inputs, and a complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454c60a-b710-4a19-afd3-018f0be8cd25",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7be679-7090-48c9-b149-8a2adfff590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import re\n",
    "import datetime\n",
    "import import_ipynb\n",
    "from Utilities import *\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "baseball_path = r\"C:\\Users\\james\\Documents\\MLB\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac847237-3c15-469f-b5e8-4baf65e6a284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230407'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set today's date\n",
    "todaysdate = datetime.date.today()\n",
    "todaysdate_dash = str(todaysdate)\n",
    "todaysdate = todaysdate_dash.replace(\"-\", \"\")\n",
    "todaysdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365b12b2-a7c2-45cf-9502-5e1e3089cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads in Chadwick register with player codes.\n",
    "# Let's move this to Utilities and either make compatible with name_clean or make a separate one (mostly done in chadwick now)\n",
    "keep_list = ['key_mlbam', 'key_fangraphs', 'name_first', 'name_last']\n",
    "chadwick = read_chadwick(keep_list)\n",
    "\n",
    "# Take first two characters of first name\n",
    "chadwick['First2'] = chadwick['name_first'].str.slice(0,2)\n",
    "# And first 5 characters of last name\n",
    "chadwick['Last5'] = chadwick['name_last'].str.slice(0,5)\n",
    "\n",
    "# Make lower case\n",
    "chadwick['First2'] = chadwick['First2'].str.lower()\n",
    "chadwick['Last5'] = chadwick['Last5'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133f248-3dfa-42e4-92e4-dc77346b2a17",
   "metadata": {},
   "source": [
    "# Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb7cc7f-df83-4ffd-a8f6-6be7b8791d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wind vectors\n",
    "# Note: 2 is to centerfield, 6 is from centerfield, clockwise\n",
    "# Note: y vector is positive to centerfield, negative from centerfield\n",
    "# Note: x vector is positive from left to right, negatives from right to left\n",
    "# Assumption is wind is blowing in 8 cardinal directions, so we can use simple right isosceles triangles\n",
    "def y_vect(df):\n",
    "    wind_speed = df['windSpeed']\n",
    "    angled = df['windSpeed'] / 2 * math.sqrt(2)\n",
    "    \n",
    "    if df['windDirection'] == \"Out To CF\": \n",
    "        y_vect = wind_speed\n",
    "    elif df['windDirection'] == \"Out To RF\": \n",
    "        y_vect = angled\n",
    "    elif df['windDirection'] == \"L To R\": \n",
    "        y_vect = 0\n",
    "    elif df['windDirection'] == \"In From LF\": \n",
    "        y_vect = angled * -1\n",
    "    elif df['windDirection'] == \"In From CF\": \n",
    "        y_vect = wind_speed * - 1\n",
    "    elif df['windDirection'] == \"In From RF\": \n",
    "        y_vect = angled * -1\n",
    "    elif df['windDirection'] == \"R To L\": \n",
    "        y_vect = 0\n",
    "    elif df['windDirection'] == \"Out To LF\": \n",
    "        y_vect = angled\n",
    "    else:\n",
    "        y_vect = 0\n",
    "        \n",
    "    return y_vect\n",
    "\n",
    "def x_vect(df):\n",
    "    wind_speed = df['windSpeed']\n",
    "    angled = df['windSpeed'] / 2 * math.sqrt(2)\n",
    "    \n",
    "    if df['windDirection'] == \"L To R\": \n",
    "        x_vect = wind_speed\n",
    "    elif df['windDirection'] == \"In From LF\": \n",
    "        x_vect = angled\n",
    "    elif df['windDirection'] == \"In From CF\": \n",
    "        x_vect = 0\n",
    "    elif df['windDirection'] == \"In From RF\": \n",
    "        x_vect = angled * -1\n",
    "    elif df['windDirection'] == \"R To L\": \n",
    "        x_vect = wind_speed * - 1\n",
    "    elif df['windDirection'] == \"Out To LF\": \n",
    "        x_vect = angled * -1\n",
    "    elif df['windDirection'] == \"Out To CF\": \n",
    "        x_vect = 0\n",
    "    elif df['windDirection'] == \"Out To RF\": \n",
    "        x_vect = angled\n",
    "    else:\n",
    "        x_vect = 0\n",
    "        \n",
    "    return x_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdc969-2557-46e7-a82d-16285b3b2900",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad560891-c573-4b54-a69c-c6655df0910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather(df):\n",
    "    # Separate weather into temperature and weather type\n",
    "    df[['temperature', 'weather']] = df['weather'].str.split(\", \", expand=True)\n",
    "    df['temperature'] = df['temperature'].str.replace(\" degrees\", \"\").astype('int')\n",
    "    # Separate wind into speed and direction\n",
    "    df[['windSpeed', 'windDirection']] = df['wind'].str.split(\", \", expand=True)\n",
    "    df['windSpeed'].fillna(\"0 mph\", inplace=True)\n",
    "    df['windSpeed'] = df['windSpeed'].str.replace(\" mph\", \"\")\n",
    "    df['windSpeed'] = pd.to_numeric(df['windSpeed'], errors='coerce')\n",
    "    df['windSpeed'].fillna(0, inplace=True)\n",
    "    df['windDirection'].fillna('L to R', inplace=True)\n",
    "    df['windSpeed'].unique()\n",
    "    df['windDirection'] = df['windDirection'].str.replace(\".\", \"\")\n",
    "    # Calculate vectors\n",
    "    df['x_vect'] = df.apply(x_vect, axis=1)\n",
    "    df['y_vect'] = df.apply(y_vect, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa644436-584e-475a-b02b-c1ee57c12599",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17f3ec-b8f1-4bab-95af-8a79466dd4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ad4be9-bd9a-49e1-bc97-c8a22069919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_events(df):\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Strikeout', \"so\", \"\")\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Strikeout Double Play', \"so\", df['eventsModel'])\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Groundout', \"go\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Fielders Choice', \"go\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Double Play', \"go\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Grounded Into DP', \"go\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Triple Play', \"go\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Field Error', \"go\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Forceout', \"go\", df['eventsModel'])\n",
    "\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Lineout', \"lo\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Bunt Lineout', \"lo\", df['eventsModel'])\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Flyout', \"fo\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Sac Fly', \"fo\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Sac Fly Double Play', \"fo\", df['eventsModel'])\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Pop Out', \"po\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Bunt Pop Out', \"po\", df['eventsModel'])\n",
    "\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Hit By Pitch', \"hbp\", df['eventsModel'])\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Walk', \"bb\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Intent Walk', \"bb\", df['eventsModel'])\n",
    "\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Single', \"b1\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Double', \"b2\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Triple', \"b3\", df['eventsModel'])\n",
    "    df['eventsModel'] = np.where(df['event'] == 'Home Run', \"hr\", df['eventsModel'])\n",
    "\n",
    "    df['eventsModel'] = np.where(df['eventsModel'] == \"\", \"Cut\", df['eventsModel'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ff98f-fa1f-46c0-b411-a164afcf1383",
   "metadata": {},
   "source": [
    "# Base Running\n",
    "(Perhaps move elsewhere)\n",
    "Calculate SBA2B%, SBA3B%, SB2B%, SB3B%\n",
    "Calculate SB and CS totals\n",
    "Derive new stats: \n",
    "    SBrate = SB / (SB + CS)\n",
    "    SBArate = (SB + CS) / (BB + HBP + 1B)\n",
    "\n",
    "Use actual data for these derived stats (from API) to project the four base running stats\n",
    "    Observations should be player-seasons, but only use full seasons\n",
    "Calculate derived stats in fangraphs projections\n",
    "Use model to predict four base running stats in the fangraphs projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36401d18-9e4d-414b-9269-0ec413a0e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This turns several variables, including events, venues, hands, and bases into dummies\n",
    "def create_dummies(df):\n",
    "    event_dummies = pd.get_dummies(df['eventsModel'])\n",
    "    venue_dummies = pd.get_dummies(df['venue_id'], prefix='venue')\n",
    "    pitcher_dummies = pd.get_dummies(df['pitchHand'], prefix='p')\n",
    "    batter_dummies = pd.get_dummies(df['batSide'], prefix='b')\n",
    "    year_dummies = pd.get_dummies(df['year'], prefix='year')\n",
    "    \n",
    "    venue_list = venue_dummies.columns.tolist()\n",
    "    year_list = year_dummies.columns.tolist()\n",
    "    dummy_list = venue_list + year_list\n",
    "    \n",
    "    df = pd.concat([df, event_dummies, venue_dummies, pitcher_dummies, batter_dummies, year_dummies], axis=1)\n",
    "    \n",
    "    df['preOnFirst'] = df.groupby(['gamePk', 'inning', 'halfInning'])['postOnFirst'].shift(1)\n",
    "    df['preOnSecond'] = df.groupby(['gamePk', 'inning', 'halfInning'])['postOnSecond'].shift(1)\n",
    "    df['preOnThird'] = df.groupby(['gamePk', 'inning', 'halfInning'])['postOnThird'].shift(1)\n",
    "    \n",
    "    df['onFirst'] = df['preOnFirst'].notnull().astype('int')\n",
    "    df['onSecond'] = df['preOnSecond'].notnull().astype('int')\n",
    "    df['onThird'] = df['preOnThird'].notnull().astype('int')\n",
    "    \n",
    "    df['top'] = np.where(df['halfInning'] == \"top\", 1, 0)\n",
    "    \n",
    "    df['pa'] = np.where(df['eventsModel'] != \"Cut\", 1, 0)\n",
    "    df['ab'] = df['pa'] - df['hbp']\n",
    "    \n",
    "    return df, dummy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc99212-b89e-4178-b69d-ac3f8837bfd9",
   "metadata": {},
   "source": [
    "# Rolling Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd06dade-8189-45bd-bf84-fd2032f01cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a dataframe that can eventually be used as the model input. Has pitcher vs hitter stats, specific to hand\n",
    "def rolling_pas(df, pa_num):\n",
    "    stat_list = ['so', 'b1', 'b2', 'b3', 'hr', 'bb', 'hbp', 'lo', 'po', 'go', 'fo', 'hard_hit', 'to_left', 'to_middle', 'to_right', 'pa', 'ab']\n",
    "    df['pa_num'] = df.index\n",
    "    \n",
    "    batter_stats = []\n",
    "    pitcher_stats = []\n",
    "\n",
    "    for stat in stat_list:\n",
    "        batter_stat = stat + \"_b\"\n",
    "        pitcher_stat = stat + \"_p\"\n",
    "        batter_stats.append(batter_stat)\n",
    "        pitcher_stats.append(pitcher_stat)\n",
    "\n",
    "    df[batter_stats] = df.groupby(['batter', 'pitchHand'])[stat_list].transform(lambda x: x.shift().rolling(pa_num, min_periods=1).sum())\n",
    "    df[pitcher_stats] = df.groupby(['pitcher', 'batSide'])[stat_list].transform(lambda x: x.shift().rolling(pa_num, min_periods=1).sum())\n",
    "    df.sort_values(['pa_num'], axis=0, ascending=True, inplace=True)\n",
    "\n",
    "    # wOBA - using 2022 values throughout\n",
    "    df['woba_b'] = (0.690 * df['bb_b']) + (0.721 * df['hbp_b']) + (0.885 * df['b1_b']) + (1.262 * df['b2_b']) + (1.601 * df['b3_b']) + (2.070 * df['hr_b'])\n",
    "    df['woba_p'] = (0.690 * df['bb_p']) + (0.721 * df['hbp_p']) + (0.885 * df['b1_p']) + (1.262 * df['b2_p']) + (1.601 * df['b3_p']) + (2.070 * df['hr_p'])\n",
    "    \n",
    "    # Slugging\n",
    "    df['slg_b'] = (1 * df['b1_b']) + (2 * df['b2_b']) + (3 * df['b3_b']) + (4 * df['hr_b'])\n",
    "    df['slg_b'] = df['slg_b'] / df['ab_b']\n",
    "    df['slg_p'] = (1 * df['b1_p']) + (2 * df['b2_p']) + (3 * df['b3_p']) + (4 * df['hr_p'])\n",
    "    df['slg_p'] = df['slg_p'] / df['ab_p']\n",
    "\n",
    "    # OBP    \n",
    "    df['obp_b'] = df[['b1_b', 'b2_b', 'b3_b', 'hr_b', 'bb_b', 'hbp_b']].sum(axis=1)\n",
    "    df['obp_p'] = df[['b1_p', 'b2_p', 'b3_p', 'hr_p', 'bb_p', 'hbp_p']].sum(axis=1)\n",
    "\n",
    "    # Calculate rates\n",
    "    stat_short = ['so', 'b1', 'b2', 'b3', 'hr', 'bb', 'hbp', 'lo', 'po', 'go', 'fo', 'woba', 'obp', 'hard_hit', 'to_left', 'to_middle', 'to_right']\n",
    "    for stat in stat_short:\n",
    "        batter_stat = stat + \"_b\"\n",
    "        pitcher_stat = stat + \"_p\"  \n",
    "        df[batter_stat] = df[batter_stat] / df['pa_b']\n",
    "        df[pitcher_stat] = df[pitcher_stat] / df['pa_p']\n",
    "        \n",
    "    df.sort_values('pa_num', inplace=True)\n",
    "    \n",
    "    return df, batter_stats, pitcher_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e285989-cdf0-4eae-a27c-0ddee3d81bd1",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b135b741-f94a-4270-abd7-f3885734d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads in raw API data\n",
    "def import_data(start_year, end_year):\n",
    "    year_df_list = []\n",
    "    while start_year <= end_year:\n",
    "        # Choose file\n",
    "        filename = \"Play\" + str(start_year) + \".csv\"\n",
    "        \n",
    "        # Read in dataframe\n",
    "        year_df = pd.read_csv(os.path.join(baseball_path, \"A3. Raw API\", filename))\n",
    "        \n",
    "        # Only keep one observation per PA (don't keep each runner)\n",
    "        year_df.drop_duplicates(['gamePk', 'atBatIndex'], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "        # Create year variable \n",
    "        year_df['year'] = start_year\n",
    "        \n",
    "        # Add it to list of dataframes\n",
    "        year_df_list.append(year_df)\n",
    "                \n",
    "        start_year += 1\n",
    "        \n",
    "    df = pd.concat(year_df_list, axis=0)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    df = df[df['game_type'] == \"R\"]\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns={'level_0', 'index'}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc92f5-0d52-4b06-ba02-b24dad9378eb",
   "metadata": {},
   "source": [
    "# Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473defad-26e4-4749-9651-6d80944ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statcast(df):\n",
    "    statcast = df.copy()\n",
    "    # Hard hit dummy\n",
    "    statcast['hard_hit'] = (statcast['hardness'].str.contains('hard')).astype('int')\n",
    "    \n",
    "#     # Max pitch speed\n",
    "#     statcast['startSpeeds'] = statcast['startSpeeds'].str.replace(\"[\", \"\")\n",
    "#     statcast['startSpeeds'] = statcast['startSpeeds'].str.replace(\"]\", \"\")\n",
    "#     statcast['startSpeeds'] = statcast['startSpeeds'].str.split(\",\")\n",
    "#     for i in range(len(statcast)):\n",
    "#         try:\n",
    "#             statcast['startSpeeds'][i] = [float(x) for x in statcast['startSpeeds'][i]]\n",
    "#         except:\n",
    "#             statcast['startSpeeds'][i] = [90]\n",
    "#     statcast['maxSpeed'] = statcast['startSpeeds'].apply(max)\n",
    "    \n",
    "#     # Max spin rate\n",
    "#     statcast['spinRates'] = statcast['spinRates'].str.replace(\"[\", \"\")\n",
    "#     statcast['spinRates'] = statcast['spinRates'].str.replace(\"]\", \"\")\n",
    "#     statcast['spinRates'] = statcast['spinRates'].str.split(\",\")\n",
    "#     for i in range(len(statcast)):\n",
    "#         try:\n",
    "#             statcast['spinRates'][i] = [float(x) for x in statcast['spinRates'][i]]\n",
    "#         except:\n",
    "#             statcast['spinRates'][i] = [1800]\n",
    "#     statcast['maxSpin'] = statcast['spinRates'].apply(max)\n",
    "    \n",
    "    # Launch speeds\n",
    "    statcast['launchSpeeds'] = statcast['launchSpeeds'].str.replace(\"[\", \"\")\n",
    "    statcast['launchSpeeds'] = statcast['launchSpeeds'].str.replace(\"]\", \"\")\n",
    "    statcast['launchSpeed'] = (statcast['launchSpeeds']).astype('float', errors='ignore')\n",
    "    \n",
    "    # Launch angle\n",
    "    statcast['launchAngles'] = statcast['launchAngles'].str.replace(\"[\", \"\")\n",
    "    statcast['launchAngles'] = statcast['launchAngles'].str.replace(\"]\", \"\")\n",
    "    statcast['launchAngle'] = (statcast['launchAngles']).astype('float', errors='ignore')\n",
    "        \n",
    "    # Total distances\n",
    "    statcast['totalDistances'] = statcast['totalDistances'].str.replace(\"[\", \"\")\n",
    "    statcast['totalDistances'] = statcast['totalDistances'].str.replace(\"]\", \"\")\n",
    "    statcast['totalDistance'] = (statcast['totalDistances']).astype('float', errors='ignore')\n",
    "    \n",
    "    # Coordinates of batted ball\n",
    "    statcast['coord'] = statcast['coord'].str.replace(\"[\", \"\")\n",
    "    statcast['coord'] = statcast['coord'].str.replace(\"]\", \"\")    \n",
    "    statcast[['x', 'y']] = statcast['coord'].str.split(\",\", expand=True)\n",
    "    statcast['x'] = pd.to_numeric(statcast['x'])\n",
    "    statcast['y'] = pd.to_numeric(statcast['y'])\n",
    "    \n",
    "    statcast['spray_angle'] = np.arctan((statcast['x']-125.42)/(198.27-statcast['y'])) * 180/np.pi * 0.75\n",
    "    statcast['to_left'] = (statcast['spray_angle'] < -15).astype('int')\n",
    "    statcast['to_middle'] = ((statcast['spray_angle'] >= -15) & (statcast['spray_angle'] <= 15)).astype('int')\n",
    "    statcast['to_right'] = (statcast['spray_angle'] > 15).astype('int')\n",
    "\n",
    "    \n",
    "    return statcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf06a95-8372-4830-8ae4-1a197a31e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts raw data to clean model input\n",
    "def create_model_input(df, date):\n",
    "    keep_list = ['so_b', 'b1_b', 'b2_b', 'b3_b', 'hr_b', 'bb_b', 'hbp_b', 'lo_b', 'po_b', 'go_b', 'fo_b', 'pa_b', 'ab_b', \n",
    "                 'woba_b', 'slg_b', 'obp_b', 'hard_hit_b', 'to_left_b', 'to_middle_b', 'to_right_b', \n",
    "                 'so_p', 'b1_p', 'b2_p', 'b3_p', 'hr_p', 'bb_p', 'hbp_p', 'lo_p', 'po_p', 'go_p', 'fo_p', 'pa_p', 'ab_p', \n",
    "                 'woba_p', 'slg_p', 'obp_p', 'hard_hit_p', 'to_left_p', 'to_middle_p', 'to_right_p']\n",
    "    \n",
    "    # df = clean_columns(df)\n",
    "    # Clean weather\n",
    "    df = clean_weather(df)\n",
    "    # Create events\n",
    "    df = create_events(df)\n",
    "    # Make dummies\n",
    "    df, dummy_list = create_dummies(df)\n",
    "    # Statcast\n",
    "    df = statcast(df)\n",
    "    \n",
    "    # Turn date into datetime object\n",
    "    date = datetime.datetime.strptime(date, \"%Y%m%d\")\n",
    "    \n",
    "    # Only keep if date is before selected date\n",
    "    df = df[df['date'] < date]\n",
    "    \n",
    "    # Determine score before PA\n",
    "    df['preAwayScore'] = df.groupby(['gamePk', 'inning', 'halfInning'])['awayScore'].shift(1)\n",
    "    df['preHomeScore'] = df.groupby(['gamePk', 'inning', 'halfInning'])['homeScore'].shift(1)\n",
    "    \n",
    "    df['preAwayScore'].fillna(df['awayScore'], inplace=True)\n",
    "    df['preHomeScore'].fillna(df['homeScore'], inplace=True)\n",
    "    \n",
    "    # Calculate score differential\n",
    "    df['score_diff'] = np.where(df['top'] == 1, df['preAwayScore'] - df['preHomeScore'], df['preHomeScore'] - df['preAwayScore'])\n",
    "    \n",
    "    # Cut if event isn't one we care about (usually these are weird base running things)\n",
    "    df = df[df['Cut'] != 1]\n",
    "\n",
    "    # Calculate short time frame rolling stats (100 PAs for now)\n",
    "    dfshort, batter_stats, pitcher_stats = rolling_pas(df, 100)\n",
    "    dfmain = dfshort.copy()\n",
    "    # Calculate long time frame rolling stats (300 PAs for now)\n",
    "    dflong, batter_stats, pitcher_stats = rolling_pas(df, 300)\n",
    "    dflong = dflong[keep_list]\n",
    "    dflong = dflong.add_suffix(\"_long\")\n",
    "    # Concat them together\n",
    "    df = pd.concat([dfmain, dflong], axis=1)\n",
    "            \n",
    "    # Delete intermediate DFs\n",
    "    del dfmain, dfshort, dflong\n",
    "\n",
    "    # For the purpose of creating a sample to train the model, only keep those PAs with significant data\n",
    "    sample = df[((df['pa_p'] > 40) & (df['pa_b'] > 40)) & ((df['pa_p_long'] > 40) & (df['pa_b_long'] > 40)) ]\n",
    "\n",
    "    sample.reset_index(inplace=True)\n",
    "    \n",
    "    return sample, df, batter_stats, pitcher_stats, dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09238787-dd96-4e89-b4cb-dff3deb5e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batter inputs\n",
    "def create_batter_df(df, date):\n",
    "    # Stats of interest\n",
    "    batter_list = ['batter',  'batterName', 'batSide', 'p_L',\n",
    "     'so_b', 'b1_b', 'b2_b', 'b3_b', 'hr_b', 'bb_b', 'hbp_b', 'lo_b', 'po_b', 'go_b', 'fo_b', \n",
    "     'pa_b', 'ab_b', 'woba_b', 'slg_b', 'obp_b', 'hard_hit_b', 'to_left_b', 'to_middle_b', 'to_right_b', \n",
    "     'so_b_long', 'b1_b_long', 'b2_b_long', 'b3_b_long', 'hr_b_long', 'bb_b_long', 'hbp_b_long', 'lo_b_long', 'po_b_long', 'go_b_long', 'fo_b_long', \n",
    "     'pa_b_long', 'ab_b_long', 'woba_b_long', 'slg_b_long', 'obp_b_long', 'hard_hit_b_long', 'to_left_b_long', 'to_middle_b_long', 'to_right_b_long']\n",
    "\n",
    "    # Only keep relevant stats\n",
    "    batters = df[batter_list]\n",
    "    # Only care about most recent stats of each batter before PA\n",
    "    batters.drop_duplicates(subset=['batter', 'p_L'], keep='last', inplace=True)\n",
    "\n",
    "    # Create separate dataframes for vs RHP and LHP\n",
    "    vs_r = batters.query('p_L == 0')\n",
    "    vs_l = batters.query('p_L == 1')\n",
    "\n",
    "    # Merge them together\n",
    "    batters = vs_l.merge(vs_r, on='batter', how='outer', suffixes=('_l', '_r'))\n",
    "\n",
    "    # Drop duplicate columns\n",
    "    batters.drop(columns={'batterName_r', 'p_L_l', 'p_L_r'}, inplace=True)\n",
    "    # Only need this once\n",
    "    batters.rename(columns={'batterName_l': 'batterName'}, inplace=True)\n",
    "\n",
    "    # Merge with Chadwick\n",
    "    batters = batters.merge(chadwick, left_on='batter', right_on='key_mlbam', how='left')\n",
    "    \n",
    "    # Export\n",
    "    filename = \"Batters\" + date + \".csv\"\n",
    "\n",
    "    batters.to_csv(os.path.join(baseball_path, \"A4. Dataset\", \"Batters\", filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca2446d6-a402-4488-ab96-795baf4c72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pitcher inputs\n",
    "def create_pitcher_df(df, date):\n",
    "    # Stats of interest\n",
    "    pitcher_list = ['pitcher',  'pitcherName', 'pitchHand', 'b_L',\n",
    "     'so_p', 'b1_p', 'b2_p', 'b3_p', 'hr_p', 'bb_p', 'hbp_p', 'lo_p', 'po_p', 'go_p', 'fo_p', \n",
    "     'pa_p', 'ab_p', 'woba_p', 'slg_p', 'obp_p', 'hard_hit_p', 'to_left_p', 'to_middle_p', 'to_right_p', \n",
    "     'so_p_long', 'b1_p_long', 'b2_p_long', 'b3_p_long', 'hr_p_long', 'bb_p_long', 'hbp_p_long', 'lo_p_long', 'po_p_long', 'go_p_long', 'fo_p_long', \n",
    "     'pa_p_long', 'ab_p_long', 'woba_p_long', 'slg_p_long', 'obp_p_long', 'hard_hit_p_long', 'to_left_p_long', 'to_middle_p_long', 'to_right_p_long', \n",
    "     'inning', 'outs', 'gamePk', 'eventsModel', 'game_date']\n",
    "    \n",
    "    # Only keep relevant stats\n",
    "    pitchers = df[pitcher_list]\n",
    "    \n",
    "    # Calculate average outs\n",
    "    # Create a copy of the dataframe\n",
    "    pitchers_cut = pitchers.copy()\n",
    "    # Only look at PAs since 2019\n",
    "    pitchers_cut = pitchers_cut[pitchers_cut['game_date'] > '2019-04-01']\n",
    "    pitchers_cut.drop_duplicates(subset=['pitcher', 'gamePk', 'inning'], keep='last', inplace=True)\n",
    "    # Identify if they're a starter\n",
    "    pitchers_cut['starter'] = (pitchers_cut['inning'] == 1).astype('int')\n",
    "    # Add up starts\n",
    "    pitchers_cut = pitchers_cut.groupby(['pitcher', 'gamePk'])['outs', 'starter'].sum().reset_index()\n",
    "    # Calculate mean outs and sum of starts\n",
    "    pitchers_cut = pitchers_cut.groupby('pitcher').agg({'outs': np.mean, 'starter': np.sum}).reset_index()\n",
    "\n",
    "    # Only care about most recent stats of each pitcher before PA\n",
    "    pitchers.drop_duplicates(subset=['pitcher', 'b_L'], keep='last', inplace=True)\n",
    "    \n",
    "    # Create separate dataframes for vs RHB and LBH\n",
    "    vs_r = pitchers.query('b_L == 0')\n",
    "    vs_l = pitchers.query('b_L == 1')\n",
    "\n",
    "    # Merge them together\n",
    "    pitchers = vs_l.merge(vs_r, on='pitcher', how='outer', suffixes=('_l', '_r'))\n",
    "    # And add outs/starts\n",
    "    pitchers = pitchers.merge(pitchers_cut, on='pitcher', how='left')\n",
    "    \n",
    "    # Drop duplicate columns\n",
    "    pitchers.drop(columns={'pitcherName_r', 'b_L_l', 'b_L_r', 'inning_r', 'outs_r', 'gamePk_r', 'eventsModel_r', 'game_date_r',  'inning_l',\n",
    "                           'outs_l', 'gamePk_l', 'eventsModel_l', 'game_date_l'}, inplace=True)\n",
    "    # Only need this once\n",
    "    pitchers.rename(columns={'pitcherName_l': 'pitcherName'}, inplace=True)\n",
    "    \n",
    "    # Merge with Chadwick\n",
    "    pitchers = pitchers.merge(chadwick, left_on='pitcher', right_on='key_mlbam', how='left')\n",
    "    \n",
    "    # Take average number of batters faced (not sure this is needed anymore with the average \"outs\" variable\n",
    "    faced = df.copy()\n",
    "    faced['faced'] = 1\n",
    "    games = faced.groupby(['pitcher', 'gamePk'])['faced'].sum().reset_index()\n",
    "    games['avgFaced'] = games.groupby('pitcher')['faced'].shift().rolling(30, min_periods=5).mean()\n",
    "    games.drop_duplicates(subset=['pitcher'], keep='last', inplace=True)\n",
    "    games = games[['pitcher', 'avgFaced']]\n",
    "    pitchers = pitchers.merge(games, on='pitcher', how='inner')\n",
    "    \n",
    "    # Export\n",
    "    filename = \"Pitchers\" + date + \".csv\"\n",
    "    \n",
    "    pitchers.to_csv(os.path.join(baseball_path, \"A4. Dataset\", \"Pitchers\", filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2340bb43-c6d4-4973-ac7e-c2c2922281c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates inputs on a given date\n",
    "def create_datasets(df, date):\n",
    "    # Create data for model and data for model inputs (once model is built, sample won't really be needed)\n",
    "    sample, inputs, batter_stats, pitcher_stats, dummy_list = create_model_input(df, date)\n",
    "    # Create batter and pitcher csvfiles\n",
    "    create_batter_df(inputs, date)\n",
    "    create_pitcher_df(inputs, date)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "118c3447-d02f-4cd6-b7b6-4c57a8418d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_data(2015, 2023) # Changed to 2018 - keep full 2013- for training set below\n",
    "sample = create_datasets(df, todaysdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c56a8fb-75d0-445a-9f57-bc110792d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need average totalDistances, spinRates, startSpeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afff092-58fb-42ca-9661-27b66b943c0c",
   "metadata": {},
   "source": [
    "# Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb7764f-0b49-41fb-852c-59d2d6a51fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This reads in the complete raw dataset \n",
    "# df = import_data(2013, 2022)\n",
    "# # and creates usable datasets for each date there is a depth chartr for\n",
    "# for filename in os.listdir(r\"C:\\Users\\james\\Documents\\MLB\\Data\\Rosters\\Depth\"):\n",
    "#     date = filename[5:13]\n",
    "#     print(date)\n",
    "#     create_datasets(df, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "111eb34c-8728-41a4-8506-fc5c8a30c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir(r\"C:\\Users\\james\\Documents\\MLB\\Data\\FanGraphs\\Batters\"):\n",
    "#     date = filename[11:19]\n",
    "#     print(date)\n",
    "#     create_datasets(inputs, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857fc768-0a49-4bd9-a088-4aa987be27a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(baseball_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull Dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Sample used for training PA model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseball_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSample.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3554\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3556\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:311\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    308\u001b[0m slicer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(start_i, end_i)\n\u001b[0;32m    309\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[slicer]\n\u001b[1;32m--> 311\u001b[0m res \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mto_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[0;32m    312\u001b[0m data \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39miget_values(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mitems))]\n\u001b[0;32m    314\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:473\u001b[0m, in \u001b[0;36mBaseBlockManager.to_native_types\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_native_types\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m    Convert values to native types (strings / python objects) that are used\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    in formatting (repr / csv).\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_native_types\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:304\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:634\u001b[0m, in \u001b[0;36mBlock.to_native_types\u001b[1;34m(self, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_native_types\u001b[39m(\u001b[38;5;28mself\u001b[39m, na_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;124;03m\"\"\"convert to our native types format\"\"\"\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m     result \u001b[38;5;241m=\u001b[39m to_native_types(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, na_rep\u001b[38;5;241m=\u001b[39mna_rep, quoting\u001b[38;5;241m=\u001b[39mquoting, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:2204\u001b[0m, in \u001b[0;36mto_native_types\u001b[1;34m(values, na_rep, quoting, float_format, decimal, **kwargs)\u001b[0m\n\u001b[0;32m   2201\u001b[0m         values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2203\u001b[0m     values[mask] \u001b[38;5;241m=\u001b[39m na_rep\n\u001b[1;32m-> 2204\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatArrayFormatter\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Export files for building models\n",
    "# # Do not need to run these every day\n",
    "# # Complete dataset\n",
    "# df.to_csv(os.path.join(baseball_path, \"Inputs\", \"Full Dataset.csv\"))\n",
    "\n",
    "# # Sample used for training PA model\n",
    "# sample.to_csv(os.path.join(baseball_path, \"Inputs\", \"Sample.csv\"))\n",
    "\n",
    "# # # Park list (all park dummies)\n",
    "# # parks = df[['home_name', 'venue_id']]\n",
    "# # parks = parks.drop_duplicates().sort_values('home_name')\n",
    "# # parks.to_csv(os.path.join(baseball_path, \"Inputs\", \"All Parks.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611fac0-fe56-4607-bc5e-61372ca6d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Code was last run on: {} at {}.\".format(datetime.date.today(), datetime.datetime.now().strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29249b1e-0cd1-4d50-92d7-f0ed488a7561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
