{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b293f6-25db-4a7c-9bb0-b78be505d361",
   "metadata": {},
   "source": [
    "# 006B. Park and Weather Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66c3d1-2a3f-4bc4-a77a-728a13dbbe93",
   "metadata": {},
   "source": [
    "Note: doing rolling pas. Not sure if calendar date would be better. Has advantages with seasonality, but better hitting weather should be reflected in numerator and denominator even without that, so it might not matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6434d-8aa6-41c3-8b2c-5b9ef6075145",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b0b771-8bc9-4f75-9daf-1316bd2b5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running imports...\n",
      "Imports in.\n"
     ]
    }
   ],
   "source": [
    "if \"running_pipeline\" not in globals():\n",
    "    print(\"Running imports...\")\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U1. Imports.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U2. Utilities.ipynb\"\n",
    "    %run \"C:\\Users\\james\\Documents\\MLB\\Code\\U3. Classes.ipynb\"\n",
    "    print(\"Imports in.\")\n",
    "else:\n",
    "    print(\"Imports already in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1baa5-8ef0-4962-bba1-86e349962276",
   "metadata": {},
   "source": [
    "We want some functions from MLB API notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4cc1ff6-12e8-4723-92c4-3894115dc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_datasets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8b3626-a33a-4c54-bb97-e138778f4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\Users\\james\\Documents\\MLB\\Code\\002. MLB API.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fd04d-4066-4bfb-be9b-ea5dbc26c34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24e63f4f-be16-4cc7-89ec-1b78058bd24c",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4024ac0b-59c2-40bf-87a2-d84069a92274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating game_df for 20240817 to 20240817...\n",
      "game_df created.\n"
     ]
    }
   ],
   "source": [
    "if \"running_pipeline\" not in globals():\n",
    "    # Set date range \n",
    "    start_date = yesterdaysdate\n",
    "    end_date = yesterdaysdate\n",
    "    print(f\"Creating game_df for {start_date} to {end_date}...\")\n",
    "    game_df = read_and_save_games(team_map, generate=True)\n",
    "    game_df = game_df[(game_df['date'] >= start_date) & (game_df['date'] <= end_date)].reset_index(drop=True)\n",
    "    print(\"game_df created.\")\n",
    "else:\n",
    "    print(\"game_df already generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d211e-2db6-42e6-bfc5-76f340140c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84aa29a9-19ec-4a3a-a372-6c37548e833f",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d42db-933f-4e80-8ca0-c3380d6c6812",
   "metadata": {},
   "source": [
    "Rolling averages are NOT shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6a652-424b-4398-9e68-969bd31994b6",
   "metadata": {},
   "source": [
    "Calculate Park Factors <br>\n",
    "Park Factor = Rate at Park (both teams) / Rate in Home Team's Away Games (both teams) <br>\n",
    "For example: Fenway Park HR Factor = HR rate at Fenway / HR rate in games where Red Sox are away team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98167a0-79c0-4369-91db-1cd4dafa62b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3f374f-9e44-4605-ab28-644cd5d8abd3",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f3eef-a9f7-40ef-80dd-85ad1f9b3cbc",
   "metadata": {},
   "source": [
    "##### Period Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a0132-0c07-4a88-9c18-1a717db8af94",
   "metadata": {},
   "source": [
    "Average of stats league-wide over period of interest, used as base for calculating multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9ca187-5395-4ea1-b363-7d19e576b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_averages(df):\n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Select period of interest\n",
    "    df = df[df['game_date'] >= '01-01-2015']\n",
    "\n",
    "    # Calculate averages over period of interest\n",
    "    period_avgs = pd.DataFrame(df[events_list].mean()).T\n",
    "\n",
    "    \n",
    "    return period_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014006f-96d7-4585-9057-227ced68ddd6",
   "metadata": {},
   "source": [
    "##### Game Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e666ca-838c-4618-81aa-c72b3ce5d445",
   "metadata": {},
   "source": [
    "Averages of stats that occurred in the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea45aa75-f1b8-4306-af33-348b6a290d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_averages(df):\n",
    "    # Calculate averages by game\n",
    "    game_avgs = df.groupby(['gamePk', 'game_date', 'venue_id', 'x_vect', 'y_vect', 'temperature'])[events_list].mean().reset_index()\n",
    "\n",
    "    \n",
    "    return game_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccf61d-9004-4235-8012-4eaec25581a6",
   "metadata": {},
   "source": [
    "##### Player Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d800807-ddd1-4eba-bc31-a4d09f0dcde7",
   "metadata": {},
   "source": [
    "Averages of stats for players in game, coming into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3c2155-abe4-4ffe-abe5-511502274f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_averages(df):\n",
    "    # Note: these are already shifted if using create_pa_inputs from A02. We want the first PA for players in each game.\n",
    "    # Stats to average\n",
    "    batter_inputs_short = [f\"{event}_b_long\" for event in events_list]\n",
    "    pitcher_inputs_short = [f\"{event}_p_long\" for event in events_list]\n",
    "\n",
    "    # Apply stats from first at bat to entire game\n",
    "    # First at bat has stats through end of last game\n",
    "    # This ensures that no stats generated in-game are reflected\n",
    "    # Note: we're doing this instead of dropping duplicates to properly weight by PA\n",
    "    df[batter_inputs_short] = df.groupby(['gamePk', 'batter'])[batter_inputs_short].transform('first')\n",
    "    df[pitcher_inputs_short] = df.groupby(['gamePk', 'pitcher'])[pitcher_inputs_short].transform('first')\n",
    "    \n",
    "    # Calculate player averages by game\n",
    "    batter_avgs = df.groupby(['gamePk'])[batter_inputs_short].mean().reset_index()\n",
    "    pitcher_avgs = df.groupby(['gamePk'])[pitcher_inputs_short].mean().reset_index()\n",
    "\n",
    "    # Merge together\n",
    "    player_avgs = pd.merge(batter_avgs, pitcher_avgs, on='gamePk', how='inner')\n",
    "\n",
    "    \n",
    "    return player_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eade1fe-2ccf-4d24-abe6-8ce3fa1d2078",
   "metadata": {},
   "source": [
    "##### League Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081d159-f4d7-4339-8dc0-09eebf25d6f4",
   "metadata": {},
   "source": [
    "Averages of stats league-wide, coming into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dde60c6-9dfe-4d4b-91b6-574d0a164bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def league_averages(df, league_window, league_window_min, base_year=2014):    \n",
    "    # Calculate rolling average of stats\n",
    "    league_avgs = df[events_list].rolling(window=league_window, min_periods=league_window_min).mean()\n",
    "    league_avgs.columns = [f'{col}_league' for col in league_avgs.columns]\n",
    "\n",
    "    # Keep column names in a list\n",
    "    column_names = league_avgs.columns\n",
    "\n",
    "    # Add game date onto stats\n",
    "    league_avgs = pd.concat([df[['game_date']], league_avgs], axis=1)\n",
    "\n",
    "    # Drop duplicates, keeping last\n",
    "    league_avgs.drop_duplicates('game_date', keep='last', inplace=True)\n",
    "\n",
    "    # Shift so dates reflect stats through the end of the prior date\n",
    "    league_avgs[column_names] = league_avgs[column_names].shift(1)\n",
    "\n",
    "    # Create date variables\n",
    "    league_avgs['game_date'] = pd.to_datetime(league_avgs['game_date'])\n",
    "    league_avgs['month'] = league_avgs['game_date'].dt.month\n",
    "    league_avgs['day'] = league_avgs['game_date'].dt.day\n",
    "    \n",
    "    # Subset base year\n",
    "    base_year_df = league_avgs[league_avgs['game_date'].dt.year == base_year]\n",
    "    \n",
    "    # Merge on base year\n",
    "    league_avgs = pd.merge(league_avgs, base_year_df, on=['month', 'day'], how='left', suffixes=(\"\", \"_base\"))\n",
    "    \n",
    "    # Identify columns that contain '_base' in their names\n",
    "    base_columns = [col for col in league_avgs.columns if '_base' in col]\n",
    "\n",
    "    # Apply forward fill to those columns\n",
    "    league_avgs[base_columns] = league_avgs[base_columns].ffill()\n",
    "\n",
    "    # Create multipliers\n",
    "    for column in column_names:\n",
    "        league_avgs[column] = league_avgs[column] / league_avgs[f'{column}_base']\n",
    "        league_avgs[column].fillna(1, inplace=True)\n",
    "    \n",
    "    keep_list = ['game_date'] + list(column_names)\n",
    "    \n",
    "    league_avgs = league_avgs[keep_list]\n",
    "    \n",
    "    \n",
    "    return league_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff7bbb-7029-47bb-ab38-0b3ea6960fa5",
   "metadata": {},
   "source": [
    "##### Park Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7394bf-7c28-46b3-9f85-7fbb845eefbd",
   "metadata": {},
   "source": [
    "Averages of stats at park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ba7e1a-578b-4577-9121-b6f353c281f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_averages(df, park_window, park_window_min):\n",
    "    # Calculate rolling averages by park \n",
    "    park_avgs = df.groupby('venue_id')[events_list].rolling(window=park_window, min_periods=park_window_min).mean()\n",
    "   \n",
    "    # Reset index to align with original DataFrame\n",
    "    park_avgs = park_avgs.reset_index(level=0, drop=False)\n",
    "    \n",
    "    # Rename columns to indicate they are park averages\n",
    "    for column in park_avgs[events_list]:\n",
    "        park_avgs.rename(columns={column: f\"{column}_park\"}, inplace=True)\n",
    "\n",
    "    # Sort to return to correct ordering\n",
    "    park_avgs.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    # Add in date\n",
    "    park_avgs = pd.concat([df[['game_date', 'home_name']], park_avgs], axis=1)\n",
    "\n",
    "    # Only keep one observation per park\n",
    "    park_avgs.drop_duplicates(['game_date', 'venue_id'], keep='last', inplace=True)\n",
    "\n",
    "    \n",
    "    column_names = [column for columns in park_avgs.columns if \"_park\" in column]\n",
    "    \n",
    "    # Shift so dates reflect stats through the end of the prior date\n",
    "    park_avgs.groupby(['game_date', 'venue_id'])[column_names].shift(1)\n",
    "    \n",
    "    \n",
    "    return park_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc175a30-0a3c-408d-a79c-1bea36a0e0d8",
   "metadata": {},
   "source": [
    "##### Team Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c571f77-1d43-417a-b80f-113c2d8ec66b",
   "metadata": {},
   "source": [
    "Average of stats by team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3676c68f-6e8f-44ff-b784-3e19456954ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_averages(df, park_window, park_window_min):\n",
    "    # Calculate rolling averages by park \n",
    "    team_avgs = df.groupby('away_name')[events_list].rolling(window=park_window, min_periods=park_window_min).mean()\n",
    "   \n",
    "    # Reset index to align with original DataFrame\n",
    "    team_avgs = team_avgs.reset_index(level=0, drop=False)\n",
    "    \n",
    "    # Rename columns to indicate they are park averages\n",
    "    for column in team_avgs[events_list]:\n",
    "        team_avgs.rename(columns={column: f\"{column}_team\"}, inplace=True)\n",
    "\n",
    "    # Sort to return to correct ordering\n",
    "    team_avgs.sort_index(ascending=True, inplace=True)\n",
    "    \n",
    "    # Add in date\n",
    "    team_avgs = pd.concat([df[['game_date']], team_avgs], axis=1)\n",
    "\n",
    "    # Only keep one observation per park\n",
    "    team_avgs.drop_duplicates(['game_date', 'away_name'], keep='last', inplace=True)\n",
    "    \n",
    "    \n",
    "    return team_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f1a89-9b10-4a36-9305-8deea95894bd",
   "metadata": {},
   "source": [
    "##### Calculate Park Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52af922-04b1-4d3a-be7a-922a5eb3499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify last date in team_avgs before given dates in park_avgs\n",
    "# latest_dates = []\n",
    "\n",
    "# for index, row in park_avgs.iterrows():\n",
    "#     # Filter team_avgs based on criteria\n",
    "#     filtered_team_avgs = team_avgs[(team_avgs['away_name'] == row['home_name']) & (team_avgs['game_date'] < row['game_date'])]\n",
    "    \n",
    "#     # Find the latest date in the filtered dataframe\n",
    "#     if not filtered_team_avgs.empty:\n",
    "#         latest_date = filtered_team_avgs['game_date'].max()\n",
    "#         latest_dates.append(latest_date)\n",
    "#     else:\n",
    "#         latest_dates.append(pd.NaT)  # Append NaT if no matching date found\n",
    "\n",
    "# # Add the latest_dates to park_avgs\n",
    "# park_avgs['last_road_date'] = latest_dates\n",
    "\n",
    "# # Merge \n",
    "# factor_df = pd.merge(park_avgs, team_avgs, left_on=['home_name', 'last_road_date'], right_on=['away_name', 'game_date'], how='left', suffixes=(\"\", \"_\"))\n",
    "\n",
    "# # Loop over event rates and calculate factors\n",
    "# for event in events_list:\n",
    "#     factor_df[f'{event}_factor'] = factor_df[f'{event}_park'].astype(float) / factor_df[f'{event}_team'].astype(float)\n",
    "\n",
    "# factor_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ba2a5-5462-45e8-b124-de94768ad6e9",
   "metadata": {},
   "source": [
    "##### Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e105f88-6836-4e16-a62f-4b9ee5225efb",
   "metadata": {},
   "source": [
    "Calculates Park x Weather multipliers and all necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6be6f7b-dff7-464a-9bda-70dd98307251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, park_window, park_window_min, league_window, league_window_min, base_year, batSide=\"L\"):\n",
    "    # Only keep regular season games\n",
    "    df = df[df['game_type_x'] == \"R\"]\n",
    "    \n",
    "    # Only look at one side of the plate\n",
    "    df = df[df['batSide'] == batSide]\n",
    "    \n",
    "    # Reset index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Create uniform Cleveland name (only necessary for away team, but do both)\n",
    "    df['away_name'] = np.where(df['away_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", df['away_name'])\n",
    "    df['home_name'] = np.where(df['home_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", df['home_name'])\n",
    "\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "    # Convert outputs to numeric (some are boolean)\n",
    "    df[events_list] = df[events_list].astype('float64')\n",
    "    \n",
    "\n",
    "    ### Game Averages\n",
    "    game_avgs = game_averages(df)\n",
    "\n",
    "    ### Player Averages\n",
    "    player_avgs = player_averages(df)\n",
    "\n",
    "\n",
    "    ### League Averages\n",
    "    league_avgs = league_averages(df, league_window, league_window_min, base_year)\n",
    "\n",
    "\n",
    "    ### Park Averages\n",
    "    park_avgs = park_averages(df, park_window, park_window_min)\n",
    "    park_avgs['home_name'] = np.where(park_avgs['home_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", park_avgs['home_name'])\n",
    "\n",
    "    ### Team Averages\n",
    "    team_avgs = team_averages(df, park_window, park_window_min)\n",
    "    team_avgs['away_name'] = np.where(team_avgs['away_name'] == \"Cleveland Indians\", \"Cleveland Guardians\", team_avgs['away_name'])\n",
    "\n",
    "\n",
    "    ### Calculate Park Factors from Park Averages and Team Averages\n",
    "    # Identify last date in team_avgs before given dates in park_avgs\n",
    "    # This is so we can identify what the team was doing on the road leading up to their game at home\n",
    "    latest_dates = []\n",
    "    \n",
    "    for index, row in park_avgs.iterrows():\n",
    "        # Filter team_avgs based on criteria\n",
    "        filtered_team_avgs = team_avgs[(team_avgs['away_name'] == row['home_name']) & (team_avgs['game_date'] < row['game_date'])]\n",
    "        \n",
    "        # Find the latest date in the filtered dataframe\n",
    "        if not filtered_team_avgs.empty:\n",
    "            latest_date = filtered_team_avgs['game_date'].max()\n",
    "            latest_dates.append(latest_date)\n",
    "        else:\n",
    "            latest_dates.append(pd.NaT)  # Append NaT if no matching date found\n",
    "\n",
    "    # Add the latest_dates to park_avgs\n",
    "    park_avgs['last_road_date'] = latest_dates\n",
    "\n",
    "    # Merge \n",
    "    factor_df = pd.merge(park_avgs, team_avgs, left_on=['home_name', 'last_road_date'], right_on=['away_name', 'game_date'], how='left', suffixes=(\"\", \"_\"))\n",
    "    \n",
    "    # Calculate Park Factors\n",
    "    for event in events_list:\n",
    "        factor_df[f'{event}_factor'] = factor_df[f'{event}_park'].astype(float) / factor_df[f'{event}_team'].astype(float)\n",
    "\n",
    "    # Keep relevant columns\n",
    "    park_columns = [column for column in factor_df if \"_factor\" in column] + [column for column in factor_df if \"_park\" in column] + [column for column in factor_df if \"_team\" in column]\n",
    "    keep_columns = ['home_name', 'game_date', 'venue_id'] + park_columns\n",
    "\n",
    "    factor_df = factor_df[keep_columns]\n",
    "\n",
    "    # Cleveland has two names in the data. Need to treat as one.\n",
    "    factor_df['home_name'] = np.where(factor_df['home_name'] == \"Cleveland Indians\", \"Cleveland Guardian\", factor_df['home_name'])\n",
    "\n",
    "    factor_df['game_date'] = pd.to_datetime(factor_df['game_date'])\n",
    "\n",
    "    \n",
    "    ### Merge\n",
    "    dataset = pd.merge(game_avgs, player_avgs, on='gamePk', how='inner')\n",
    "    dataset = pd.merge(dataset, league_avgs, on='game_date', how='inner')\n",
    "    dataset = pd.merge(dataset, factor_df, on=['game_date', 'venue_id'], how='inner')\n",
    "\n",
    "    # Sort\n",
    "    dataset.sort_values('game_date', ascending=True, inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebab6f-8b6f-4967-96b5-46fa5de55c61",
   "metadata": {},
   "source": [
    "Subsets and cleans dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ccf113-6fef-4466-9239-7e561ff66ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "    # Select active ballparks\n",
    "    active_dataset = dataset[dataset['venue_id'].astype(int).isin(list(team_map['VENUE_ID']))].reset_index(drop=True)\n",
    "    \n",
    "    # Restrict to 2015-\n",
    "    active_dataset = active_dataset[active_dataset['game_date'] >= '01-01-2015']\n",
    "    \n",
    "    # Create venue_id dummies\n",
    "    active_dataset['venue_id_copy'] = active_dataset['venue_id'].copy()\n",
    "    active_dataset = pd.get_dummies(active_dataset, columns=['venue_id_copy'], drop_first=False, prefix=\"venue\", prefix_sep=\"_\")\n",
    "    \n",
    "    # Create interactions of weather and park variables\n",
    "    weather_interactions = []\n",
    "    \n",
    "    for venue in team_map['VENUE_ID']:\n",
    "        for weather in ['x_vect', 'y_vect', 'temperature']:\n",
    "            active_dataset[f'venue_{venue}_{weather}'] = active_dataset[f'venue_{venue}'] * active_dataset[weather]\n",
    "            weather_interactions.append(f'venue_{venue}_{weather}')\n",
    "\n",
    "            \n",
    "    # weather_interactions = weather_interactions + [f'venue_{venue}' for venue in list(team_map['VENUE_ID'].unique())] + ['x_vect', 'y_vect', 'temperature']\n",
    "            \n",
    "    return active_dataset, weather_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f69a71-9917-484a-b54a-358caf8c22c0",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e483b01-4632-40a4-9321-4ca3ca581e9d",
   "metadata": {},
   "source": [
    "Runs model, training it if selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0038851b-dcc6-4986-8668-412cfc86427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, event, batSide, weather_interactions, train=False):\n",
    "    # Define the dependent variable (y) and independent variables (X)\n",
    "    y = df.dropna()[f'{event}']\n",
    "\n",
    "    # Select model inputs\n",
    "    X_columns = [f'{event}_b_long', f'{event}_p_long', f'{event}_league', f'{event}_factor'] + weather_interactions\n",
    "    \n",
    "    # Drop missings\n",
    "    X = df.dropna()[X_columns]\n",
    "    # Convert columns to numeric\n",
    "    X[X_columns] = X[X_columns].astype(float)\n",
    "\n",
    "    # Add a constant to the independent variables matrix to include an intercept in the model\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Add a constant to the independent variables matrix to include an intercept in the model\n",
    "    if train == True:  \n",
    "        # Fit the linear regression model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "    else:\n",
    "        # Select model      \n",
    "        model = globals().get(f'{event}_{str.lower(batSide)}_model')\n",
    "\n",
    "        \n",
    "    return model, X\n",
    "# # WHY DIVIDE BY ACVG WHEN IT SHOULD BE BASE YEAR?\n",
    "# # Also shouold league be based on a moving target?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07cf61d-fb31-4765-b2c1-f91e98bc41ba",
   "metadata": {},
   "source": [
    "Apply predictions to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9f9a04-01d1-40c4-98df-b5ecf6e30e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(active_dataset, batSide, weather_interactions, period_avgs, train=False):\n",
    "    model_dictionary = {}\n",
    "    for event in events_list:\n",
    "        print(event)\n",
    "        # Train model\n",
    "        model, X = run_model(active_dataset, event, batSide, weather_interactions, train)\n",
    "        # If we trained a new model,\n",
    "        if train == True:\n",
    "            # Save model\n",
    "            pickle.dump(model, open(os.path.join(model_path, f\"Weather Model - {event} {batSide} {todaysdate}\"), 'wb'))\n",
    "            # Save to dictionary\n",
    "            # model_dictionary[event] = model.summary()\n",
    "    \n",
    "        # Replace with average\n",
    "        X[f'{event}_b_long'] = period_avgs[f'{event}'][0]\n",
    "        X[f'{event}_p_long'] = period_avgs[f'{event}'][0]\n",
    "    \n",
    "        # Predict\n",
    "        X[f'{event}_pred'] = model.predict(X)\n",
    "        X[f'{event}_mult'] = X[f'{event}_pred'] / period_avgs[f'{event}'][0]\n",
    "    \n",
    "        # Copy predicted rate and multiplier to active_dataset\n",
    "        active_dataset[f'{event}_pred'] = X[f'{event}_pred'].copy()    \n",
    "        active_dataset[f'{event}_mult'] = X[f'{event}_mult'].copy()\n",
    "\n",
    "    return active_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e65a7ec-6401-4d27-a0ff-da6a296b9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def park_and_weather_factors(game_df, multiplier_dataset, period_avg_df, swish_df=None, date=None, overwrite_year=None, historic=False):\n",
    "    print(date)\n",
    "    # Extract daily games\n",
    "    daily_game_df = game_df.query(f'date == \"{date}\"').reset_index(drop=True)\n",
    "    daily_game_df.sort_values('game_datetime', inplace=True)\n",
    "    daily_game_df['game_num'] = daily_game_df.groupby('home_team').cumcount() + 1\n",
    "\n",
    "    # If we have Swish weather data, merge it in\n",
    "    if swish_df is not None:\n",
    "        # Add game num\n",
    "        swish_df['game_num'] = swish_df.groupby('home_team').cumcount() + 1\n",
    "        daily_game_df = daily_game_df.merge(swish_df, on=['home_team', 'game_num'], how='left')\n",
    "\n",
    "    \n",
    "    daily_weather_list = []\n",
    "    # Loop over games\n",
    "    for i in range(len(daily_game_df)):\n",
    "        # Extract relevant information\n",
    "        game_id = daily_game_df['game_id'][i]\n",
    "        venue_id = daily_game_df['venue_id'][i]\n",
    "        away_team = daily_game_df['away_team'][i]\n",
    "        home_team = daily_game_df['home_team'][i]\n",
    "        datetime = daily_game_df['game_datetime'][i]\n",
    "        date = daily_game_df['date'][i]\n",
    "        year = daily_game_df['year'][i]\n",
    "\n",
    "        # If we have Swish Analytics data, extract it\n",
    "        if swish_df is not None:\n",
    "            temperature = daily_game_df['temperature'][i]\n",
    "            x_vect = daily_game_df['x_vect'][i].astype(float)\n",
    "            y_vect = daily_game_df['y_vect'][i].astype(float)\n",
    "        \n",
    "        # Extract weather from box score\n",
    "        weather, wind, park, full_date, missing_weather = create_box(game_id)\n",
    "        \n",
    "        # Create game weather dataframe\n",
    "        game_weather_dictionary = {\n",
    "            'game_id': game_id,\n",
    "            'away_team': away_team,\n",
    "            'home_team': home_team,\n",
    "            'venue_id': venue_id,\n",
    "            'park': park,\n",
    "            'datetime': datetime,\n",
    "            'date': date,\n",
    "            'year': year,\n",
    "            'weather': weather,\n",
    "            'wind': wind\n",
    "        }\n",
    "        game_weather_df = pd.DataFrame(game_weather_dictionary, index=[0])\n",
    "        game_weather_df = clean_weather(game_weather_df)\n",
    "\n",
    "        # If we do not have MLB Stats API data yet, use Swish Analytics\n",
    "        if missing_weather == True:\n",
    "            print(f\"{away_team}@{home_team} uses Swish Analytics weather.\")\n",
    "            game_weather_df['temperature'] = float(temperature)\n",
    "            game_weather_df['x_vect'] = x_vect\n",
    "            game_weather_df['y_vect'] = y_vect\n",
    "            game_weather_df['windDirection'] = \"Predicted\"\n",
    "        \n",
    "        daily_weather_list.append(game_weather_df)\n",
    "\n",
    "    # Create day weather dataframe\n",
    "    daily_weather_df = pd.concat(daily_weather_list, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Overwrite year (good option if early in season, but may want to develop a rolling method)\n",
    "    if overwrite_year is not None:\n",
    "        daily_weather_df['year'] = overwrite_year\n",
    "    \n",
    "    # Columns to keep\n",
    "    keep_columns = list(daily_weather_df.columns)\n",
    "    \n",
    "    # Add park dummies\n",
    "    active_venues = list(team_map['VENUE_ID'])\n",
    "    active_venues = [str(venue) for venue in active_venues]\n",
    "    for park in active_venues:\n",
    "        daily_weather_df[f'venue_{park}'] = (daily_weather_df['venue_id'].astype(str) == park).astype(int)\n",
    "    active_venue_columns = [f\"venue_{park}\" for park in active_venues]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Loop over lefty/righty dummy\n",
    "    for lefty_dummy in [0,1]:\n",
    "        # Assign lefty dummy\n",
    "        daily_weather_df['lefty'] = lefty_dummy\n",
    "\n",
    "        if lefty_dummy == 0:\n",
    "            side = 'r'\n",
    "        else:\n",
    "            side = 'l'\n",
    "\n",
    "        # Step 2: Create interaction terms\n",
    "        weather_interactions = []\n",
    "        \n",
    "        # Weather x Park\n",
    "        for col1 in active_venue_columns:\n",
    "            for col2 in ['x_vect', 'y_vect', 'temperature']:\n",
    "                interaction_name = col1 + '_' + col2\n",
    "                daily_weather_df[interaction_name] = daily_weather_df[col1] * daily_weather_df[col2]\n",
    "                weather_interactions.append(interaction_name)\n",
    "    \n",
    "    # If it's historic, \n",
    "    if historic == True:\n",
    "        # We already have the multiplier\n",
    "        multiplier_columns = [column for column in multiplier_dataset.columns if \"mult\" in column]\n",
    "        multiplier_dataset.rename(columns={'gamePk':'game_id'}, inplace=True)\n",
    "\n",
    "        # So keep them\n",
    "        keep_columns = ['game_id', 'venue_id'] + multiplier_columns\n",
    "        daily_weather_df = pd.merge(daily_weather_df, multiplier_dataset[keep_columns], on=['game_id', 'venue_id'], how='left')\n",
    "   \n",
    "    \n",
    "    # If it's the day of,\n",
    "    else:\n",
    "        # We need to calculate it from the league averages, park factors, and league environment (period averages)\n",
    "        league_avg_columns = [column for column in multiplier_dataset.columns if \"league\" in column]\n",
    "        factor_columns = [column for column in multiplier_dataset.columns if \"factor\" in column]\n",
    "        \n",
    "        keep_columns = ['venue_id'] + league_avg_columns + factor_columns\n",
    "\n",
    "        # Keep most recent game at each venue\n",
    "        last_game_df = multiplier_dataset.drop_duplicates('venue_id', keep='last')[keep_columns]\n",
    "\n",
    "        # Merge that onto the weather\n",
    "        daily_weather_df = pd.merge(daily_weather_df, last_game_df, on=['venue_id'], how='left')\n",
    "\n",
    "\n",
    "        ### Calculate multipliers\n",
    "        # Loop over events\n",
    "        for event in events_list:\n",
    "            # Loop over sides\n",
    "            for side in ['l', 'r']:\n",
    "                # Select model      \n",
    "                model = globals().get(f'{event}_{side}_model')\n",
    "            \n",
    "                # Assign long-term average to average of team's stats for predicting a team-agnostic rate \n",
    "                daily_weather_df[f'{event}_b_long'] = period_avg_df[event][0]\n",
    "                daily_weather_df[f'{event}_p_long'] = period_avg_df[event][0]\n",
    "\n",
    "                daily_weather_df[f'{event}_league'] = daily_weather_df[f'{event}_league_{side}']\n",
    "                daily_weather_df[f'{event}_factor'] = daily_weather_df[f'{event}_factor_{side}']\n",
    "\n",
    "\n",
    "                # Model prediction inputs\n",
    "                X = daily_weather_df[[f'{event}_b_long', f'{event}_p_long', f'{event}_league', f'{event}_factor'] + weather_interactions]\n",
    "                X = sm.add_constant(X, has_constant='add')\n",
    "\n",
    "                # Predict probability of event\n",
    "                daily_weather_df[f'predicted_{event}'] = model.predict(X)\n",
    "                # Calculate multiplier\n",
    "                daily_weather_df[f'{event}_mult_{side}'] = daily_weather_df[f'predicted_{event}'] / period_avg_df[event][0]\n",
    "            \n",
    "    \n",
    "    # Keep relevant variables\n",
    "    daily_weather_df = daily_weather_df[['game_id', 'away_team', 'home_team', 'venue_id', 'park', 'datetime', 'date', 'year', \n",
    "                                         'weather', 'wind', 'temperature', 'windSpeed', 'windDirection', 'x_vect', 'y_vect'] + \n",
    "                                         [f'{event}_mult_l' for event in events_list] + \n",
    "                                         [f'{event}_mult_r' for event in events_list]]\n",
    "\n",
    "    \n",
    "    # Fill missings with 1\n",
    "    for event in events_list:\n",
    "        daily_weather_df[f'{event}_mult_l'].fillna(1, inplace=True)\n",
    "        daily_weather_df[f'{event}_mult_r'].fillna(1, inplace=True)\n",
    "        \n",
    "\n",
    "    return daily_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8263dd-d0ef-48b8-903c-0ceada3250a2",
   "metadata": {},
   "source": [
    "##### Create Multiplier Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce039ad-bf21-4429-80ab-d495105be78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiplier_dataset(start_year, end_year, short, long, adjust, park_window, park_window_min, league_window, league_window_min, base_year):\n",
    "    ### Create PA dataset\n",
    "    complete_dataset = create_pa_inputs(park_factors, team_map, 2013, 2024, short=50, long=300, adjust=False)\n",
    "    \n",
    "    ### Create or read period averages\n",
    "    if train == True:\n",
    "        # Calculate averages of each stat\n",
    "        period_avgs = period_averages(complete_dataset)\n",
    "        period_avgs.to_csv(os.path.join(baseball_path, \"Period Averages.csv\"), index=False)\n",
    "    else:\n",
    "        period_avgs = pd.read_csv(os.path.join(baseball_path, \"Period Averages.csv\"))\n",
    "        \n",
    "        \n",
    "    ### Create multipliers\n",
    "    # LHB\n",
    "    batSide = \"L\"\n",
    "    dataset_l = create_dataset(complete_dataset, park_window=park_window, park_window_min=park_window_min, league_window=league_window, league_window_min=league_window_min, base_year=base_year, batSide=batSide)\n",
    "    active_dataset_l, weather_interactions = clean_dataset(dataset_l)\n",
    "    active_dataset_l = run_predictions(active_dataset_l, batSide, weather_interactions, period_avgs, train=train)\n",
    "    \n",
    "    # RHB\n",
    "    batSide = \"R\"\n",
    "    dataset_r = create_dataset(complete_dataset, park_window=park_window, park_window_min=park_window_min, league_window=league_window, league_window_min=league_window_min, base_year=base_year, batSide=batSide)\n",
    "    active_dataset_r, weather_interactions = clean_dataset(dataset_r)\n",
    "    active_dataset_r = run_predictions(active_dataset_r, batSide, weather_interactions, period_avgs, train=train)\n",
    "    \n",
    "    ### Merge\n",
    "    # Columns to merge\n",
    "    columns = ['gamePk', 'game_date', 'x_vect', 'y_vect', 'temperature', 'venue_id']\n",
    "    league_avg_columns = [column for column in active_dataset_r if \"league\" in column]\n",
    "    factor_columns = [column for column in active_dataset_r if \"factor\" in column]\n",
    "    multiplier_columns = [column for column in active_dataset_r if \"mult\" in column]\n",
    "    # venue_columns = [column for column in active_dataset_r if \"venue_\" in column]\n",
    "    \n",
    "    multiplier_dataset = pd.merge(active_dataset_l[columns + league_avg_columns + factor_columns + multiplier_columns], active_dataset_r[columns + league_avg_columns + factor_columns + multiplier_columns], on=columns, how='left', suffixes=(\"_l\", \"_r\"))\n",
    "    \n",
    "    ### Save\n",
    "    multiplier_dataset.to_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1a8bb-9fe9-4155-a497-69423b13d778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e20cfca-dfce-41a8-86b2-f8364e3d9fc0",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e37d74-c65f-4de0-b334-0fc1a5ce1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_pipeline\" not in globals():\n",
    "    train = False\n",
    "    start_year = 2013\n",
    "    end_year = 2024\n",
    "    short = 50\n",
    "    long = 300\n",
    "    adjust = False    \n",
    "    park_window = 10000\n",
    "    park_window_min = 5000\n",
    "    league_window = 60000\n",
    "    league_window_min = 60000\n",
    "    base_year = 2014\n",
    "    \n",
    "    historic = True\n",
    "    run_multiplier_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3bff7-0be2-4e44-b211-9448db212252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8a3b07-f812-4a86-87e6-c79adcbb62f1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01a079a3-20eb-4c7b-b78e-cfdbc01446e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1\n",
      "b2\n",
      "b3\n",
      "hr\n",
      "bb\n",
      "hbp\n",
      "so\n",
      "fo\n",
      "go\n",
      "lo\n",
      "po\n",
      "b1\n",
      "b2\n",
      "b3\n",
      "hr\n",
      "bb\n",
      "hbp\n",
      "so\n",
      "fo\n",
      "go\n",
      "lo\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "if run_multiplier_dataset == True:\n",
    "    create_multiplier_dataset(start_year, end_year, short, long, adjust, park_window, park_window_min, league_window, league_window_min, base_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecf6ea01-adbd-43f9-a177-4b7d7abf8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240817\n"
     ]
    }
   ],
   "source": [
    "# Read in datasets\n",
    "multiplier_dataset = pd.read_csv(os.path.join(baseball_path, \"Multiplier Dataset.csv\"))\n",
    "period_avg_df = pd.read_csv(os.path.join(baseball_path, \"Period Averages.csv\"))\n",
    "if historic == False:\n",
    "    swish_df = pd.read_csv(os.path.join(baseball_path, \"A06. Weather\", \"1. Swish Analytics\", f\"Swish Analytics {todaysdate}.csv\"), encoding='iso-8859-1')\n",
    "else:\n",
    "    swish_df = None\n",
    "        \n",
    "# Calculate park x weather factor\n",
    "for date in game_df['date'].unique():\n",
    "    daily_weather_df = park_and_weather_factors(game_df, multiplier_dataset, period_avg_df, swish_df=swish_df, date=date, overwrite_year=None, historic=historic)\n",
    "\n",
    "    # Fill missings with 1\n",
    "    daily_weather_df.fillna(1, inplace=True)\n",
    "\n",
    "    # To csv\n",
    "    daily_weather_df.to_csv(os.path.join(baseball_path, \"A06. Weather\", \"4. Park and Weather Factors\", f'Park and Weather Factors {date}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
