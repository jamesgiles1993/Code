{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Weather\n",
    "Source: <br>\n",
    "1. Swish Analytics <br>\n",
    "2. RotoGrinders <br>\n",
    "\n",
    "Description: This scrapes Swish Analytics for daily weather data <br>\n",
    "Historic data can be found using the stats API <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reverses winds so that they're named for where they're going, not where they're from. This is so vectors make more sense logically.\n",
    "def wind_reverser(direction):\n",
    "    direction = direction.replace(\"N\", \"s\")\n",
    "    direction = direction.replace(\"S\", \"n\")\n",
    "    direction = direction.replace(\"E\", \"w\")\n",
    "    direction = direction.replace(\"W\", \"e\")\n",
    "    \n",
    "    return direction.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates number of degrees for each direction\n",
    "def find_degree(direction):\n",
    "    if direction == \"N\":\n",
    "        degree = 0\n",
    "    elif direction == \"NNE\":\n",
    "        degree = 1\n",
    "    elif direction == \"NE\":\n",
    "        degree = 2\n",
    "    elif direction == \"ENE\":\n",
    "        degree = 3\n",
    "    elif direction == \"E\":\n",
    "        degree = 4\n",
    "    elif direction == \"ESE\":\n",
    "        degree = 5\n",
    "    elif direction == \"SE\":\n",
    "        degree = 6\n",
    "    elif direction == \"SSE\":\n",
    "        degree = 7\n",
    "    elif direction == \"S\":\n",
    "        degree = 8\n",
    "    elif direction == \"SSW\":\n",
    "        degree = 9\n",
    "    elif direction == \"SW\":\n",
    "        degree = 10\n",
    "    elif direction == \"WSW\":\n",
    "        degree = 11\n",
    "    elif direction == \"W\":\n",
    "        degree = 12\n",
    "    elif direction == \"WNW\":\n",
    "        degree = 13\n",
    "    elif direction == \"NW\":\n",
    "        degree = 14\n",
    "    elif direction == \"NNW\":\n",
    "        degree = 15\n",
    "        \n",
    "    degree = degree * 22.5 \n",
    "\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the x and y vectors given the park's orientation and the wind's direction\n",
    "def calculate_vectors(row):\n",
    "    # Determines degree of centerfield\n",
    "    park_angle = find_degree(row['CF'])\n",
    "    # Determine degree of wind\n",
    "    row['Direction'] = wind_reverser(row['Direction'])\n",
    "    wind_angle = find_degree(row['Direction']) \n",
    "    \n",
    "    # Determine angle between them\n",
    "    angle = wind_angle - park_angle \n",
    "\n",
    "    # Calculate vectors\n",
    "    x_vect = round(math.sin(math.radians(angle)), 5) * row['Speed']\n",
    "    y_vect = round(math.cos(math.radians(angle)), 5) * row['Speed']\n",
    "\n",
    "    return x_vect, y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swish Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Swish Analytics for weather date\n",
    "def scrape_swishanalytics(team_map):\n",
    "    team_map = team_map[['FANGRAPHSTEAM', 'BBREFTEAM', 'FULLNAME', 'VENUE_ID']]\n",
    "    \n",
    "    # Swish Analytics URL \n",
    "    url = \"https://swishanalytics.com/mlb/weather?date=\" + todaysdate_dash\n",
    "\n",
    "    # Browser header\n",
    "    hdr = {'User-Agent':'Mozilla/5.0'}\n",
    "\n",
    "    # Request data\n",
    "    req = urllib.request.Request(url, headers=hdr)\n",
    "    response = urlopen(req)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "    # Create a list of matchups in the order of the weather tables\n",
    "    matchup_list = []\n",
    "    text = soup.find_all(text=True)\n",
    "    for t in text: \n",
    "        if \"\\xa0\\xa0@\\xa0\\xa0\" in t:\n",
    "            if t.parent.name != \"small\":\n",
    "                matchup_list.append(t.parent)\n",
    "\n",
    "    # Clean list of matchups\n",
    "    matchup_list_clean = []\n",
    "    for matchup in matchup_list:\n",
    "        matchup_clean = str(matchup)\n",
    "        matchup_clean = matchup_clean.replace('<h4 class=\"lato inline vert-mid bold\">\\xa0\\xa0', \"\")\n",
    "        matchup_clean = matchup_clean.replace('\\xa0\\xa0', \"\")\n",
    "        matchup_clean = matchup_clean.replace('</h4>', \"\")\n",
    "        matchup_list_clean.append(matchup_clean)\n",
    "        \n",
    "    # Request data\n",
    "    r = requests.get(url, headers=hdr)\n",
    "\n",
    "    # Loop over every weather table (one for each item in the matchup list)   \n",
    "    i = 0\n",
    "    while i < len(matchup_list_clean):\n",
    "        # This is the table number\n",
    "        table = 3 + (i * 2)\n",
    "        # Make the table\n",
    "        df = pd.read_html(r.text)[table]\n",
    "\n",
    "        # Rename columns so they'll be consistent when they're appended together (they're usually hours)\n",
    "        num_col = len(df.columns)\n",
    "\n",
    "        df.rename(columns={df.columns[num_col-5]:\"Start\", df.columns[num_col-4]:\"Plus1\", df.columns[num_col-3]:\"Plus2\", df.columns[num_col-2]:\"Plus3\", df.columns[num_col-1]:\"Plus4\"}, inplace=True)\n",
    "\n",
    "        # Create a column with the matchup\n",
    "        df['Matchup'] = matchup_list_clean[i]\n",
    "\n",
    "        # Try to append if you can, if not, create the weather dataframe\n",
    "        try: \n",
    "            df_weather = df_weather.append(df)\n",
    "        except:\n",
    "            df_weather = df\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Clean up a bit\n",
    "    df_weather = df_weather.reset_index()\n",
    "    df_weather['Start'] = df_weather['Start'].str.replace(u\"°\", \"\")\n",
    "    df_weather['Start'] = df_weather['Start'].str.replace(\" mph\", \"\")\n",
    "    df_weather['Plus1'] = df_weather['Plus1'].str.replace(u\"°\", \"\")\n",
    "    df_weather['Plus1'] = df_weather['Plus1'].str.replace(\" mph\", \"\")\n",
    "\n",
    "    # Create temperature, wind speed, and wind direction dataframes\n",
    "    df_temp = df_weather[df_weather['Unnamed: 0'] == \"Temp\"]\n",
    "    df_speed = df_weather[df_weather['Unnamed: 0'] == \"Wind Speed\"]\n",
    "    df_dir = df_weather[df_weather['Unnamed: 0'] == \"Wind Dir\"]\n",
    "\n",
    "    # Only keep relevant variables and label them appropriately\n",
    "    df_temp = df_temp[['Matchup', 'Plus1']]\n",
    "    df_temp.rename(columns={'Plus1': 'TEMP_PARK_CT'}, inplace=True)\n",
    "\n",
    "    df_speed = df_speed[['Matchup', 'Plus1']]\n",
    "    df_speed.rename(columns={'Plus1': 'Speed'}, inplace=True)\n",
    "\n",
    "    df_dir = df_dir[['Matchup', 'Plus1']]\n",
    "    df_dir.rename(columns={'Plus1': 'Direction'}, inplace=True)\n",
    "\n",
    "    # Merge them all together to get the weather data\n",
    "    df_weather_inputs = df_temp.merge(df_speed, on='Matchup', how='inner')\n",
    "    df_weather_inputs = df_weather_inputs.merge(df_dir, on='Matchup', how='inner')\n",
    "\n",
    "    # Choose the home team \n",
    "    df_weather_inputs['FANGRAPHSTEAM'] = df_weather_inputs['Matchup'].str.split(\"@\").str[1]\n",
    "    df_weather_inputs = pd.merge(df_weather_inputs, team_map, on='FANGRAPHSTEAM', how='left')\n",
    "    df_weather_inputs.rename(columns={'BBREFTEAM':'Team'}, inplace=True)\n",
    "\n",
    "    # Convert to numeric\n",
    "    df_weather_inputs['Speed'] = df_weather_inputs['Speed'].astype('float')\n",
    "    df_weather_inputs['TEMP_PARK_CT'] = df_weather_inputs['TEMP_PARK_CT'].astype('int')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Read in ballpark orientations (which direction center field is in relation to home plate)\n",
    "    orientations = pd.read_excel(os.path.join(baseball_path, \"Utilities\", \"Park Orientations.xlsx\"))\n",
    "\n",
    "    # Merge with weather data\n",
    "    weather = orientations.merge(df_weather_inputs, on='Team', how='inner')\n",
    "\n",
    "    # Calculate wind vectors\n",
    "    weather[['x_vect', 'y_vect']] = weather.apply(calculate_vectors, axis=1, result_type='expand')\n",
    "\n",
    "    # Only keep second game of a double header\n",
    "    weather = weather.drop_duplicates(subset='Matchup', keep='last')\n",
    "    \n",
    "    \n",
    "    # Read in list of parks\n",
    "    all_parks = pd.read_csv(os.path.join(baseball_path, \"Utilities\", \"All Parks.csv\"))\n",
    "    # Create dummies for each park (set = 0)\n",
    "    venue_dummies = pd.get_dummies(all_parks['venue_id'], prefix='venue')\n",
    "    for col in venue_dummies.columns:\n",
    "        weather[col] = 0\n",
    "    # Look over VENUE_ID, set venue_ dummy = 1 if it corresponds to VENUE_ID\n",
    "    for venue in weather['VENUE_ID'].unique().tolist():\n",
    "        venue_dummy = \"venue_\" + str(venue)\n",
    "        weather[venue_dummy] = np.where(weather['VENUE_ID'] == venue, 1, 0)\n",
    "\n",
    "    # Create variables\n",
    "    # More variables exist in backtest data (from API) that aren't needed when scraping day-of\n",
    "    weather['game_date'] = todaysdate\n",
    "    weather['venue_id'] = weather['VENUE_ID']\n",
    "    weather['BBREFTEAM'] = weather['Team']\n",
    "    weather['weather'] = \"Missing\"\n",
    "    weather['Speed'] = \"Missing\"\n",
    "    weather['CF_angle'] = \"Missing\"\n",
    "    weather['wind_angle'] = \"Missing\"\n",
    "    weather['angle'] = \"Missing\"\n",
    "\n",
    "    # Keep relevant variables\n",
    "    weather = weather[['venue_id', 'game_date', 'weather', 'Speed', 'BBREFTEAM', 'CF', \n",
    "             'TEMP_PARK_CT', 'Speed', 'CF_angle', 'wind_angle', 'angle', 'x_vect', \n",
    "             'y_vect',\n",
    "             'venue_1',    'venue_2',    'venue_3',    'venue_4',    'venue_5', 'venue_7', \n",
    "             'venue_10',   'venue_12',   'venue_13',   'venue_14',   'venue_15',   \n",
    "             'venue_16',   'venue_17',   'venue_19',   'venue_22',   'venue_31',\n",
    "             'venue_32',   'venue_680',  'venue_2392', 'venue_2394', 'venue_2395',\n",
    "             'venue_2535', 'venue_2536', 'venue_2602', 'venue_2680', 'venue_2681',\n",
    "             'venue_2701', 'venue_2735', 'venue_2756', 'venue_2889', 'venue_3289',\n",
    "             'venue_3309', 'venue_3312', 'venue_3313', 'venue_4169', 'venue_4705',\n",
    "             'venue_5010', 'venue_5325', 'venue_5365', 'venue_5381', 'venue_5445']]\n",
    "\n",
    "\n",
    "    # Create daily weather file\n",
    "    weather.to_excel(os.path.join(baseball_path, \"8. Weather\", \"A. Swish Analytics\", f\"Daily_Weather_{todaysdate}.xlsx\"), index=False)\n",
    "\n",
    "\n",
    "    weather[['BBREFTEAM', 'x_vect', 'y_vect', 'TEMP_PARK_CT']].sort_values('y_vect', ascending=False) \n",
    "    \n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RotoGrinders Weather Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_rotogrinders():\n",
    "    # URL of the web page containing the table\n",
    "    url = \"https://rotogrinders.com/weather/mlb\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the HTML content from the response\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    tags = []\n",
    "    matchups = []\n",
    "    descriptions = []\n",
    "\n",
    "    ul_element = soup.find(\"ul\", class_=\"lst data\")\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=[\"Tag\", \"Tag2\", \"Matchup\", \"Description\"])\n",
    "\n",
    "    # Iterate over the li elements\n",
    "    li_elements = ul_element.find_all(\"li\")\n",
    "    for li_element in li_elements:\n",
    "        # Find all span elements with tag class\n",
    "        tag_elements = li_element.find_all(\"span\", class_=[\"green tag\", \"yellow tag\", \"orange tag\", \"red tag\"])\n",
    "\n",
    "        # Extract the first tag color and assign it to the 'Tag' column\n",
    "        tag = tag_elements[0].text.strip()\n",
    "\n",
    "        # Extract the second tag color if it exists, otherwise set it to None\n",
    "        tag2 = tag_elements[1].text.strip() if len(tag_elements) > 1 else None\n",
    "\n",
    "        # Extract the matchup and description\n",
    "        matchup = li_element.find(\"span\", class_=\"game\").text.strip().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        description = li_element.find(\"span\", class_=\"description\").text.strip().replace(\" - \", \"\")\n",
    "\n",
    "        # Append the data to the DataFrame\n",
    "        df = df.append({\"Tag\": tag, \"Tag2\": tag2, \"Matchup\": matchup, \"Description\": description}, ignore_index=True)\n",
    "\n",
    "    df.to_csv(os.path.join(baseball_path, \"8. Weather\", \"B. RotoGrinders\", \"RotoGrinders {}.csv\").format(todaysdate))\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ballpark Pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Ballpark Pal for weather factors\n",
    "# Note: these factor in park as well and are relative to league average conditions\n",
    "def scrape_ballparkpal(date_dash):\n",
    "    headers = {\n",
    "    'Accept': 'text/html',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    url = f'https://ballparkpal.com/ParkFactors.php?date={date_dash}'\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        tree = html.fromstring(response.text)\n",
    "        table_elements = tree.xpath('/html/body/div[1]/table')\n",
    "\n",
    "        if table_elements:\n",
    "            table_element = table_elements[0]\n",
    "\n",
    "            # Extract table rows\n",
    "            rows = table_element.xpath('.//tr')\n",
    "\n",
    "            # Extract table header (assuming it's the first row)\n",
    "            header = [th.text_content().strip() for th in rows[0].xpath('.//th')]\n",
    "\n",
    "            # Extract table data\n",
    "            data = []\n",
    "            for row in rows[1:]:\n",
    "                row_data = [td.text_content().strip() for td in row.xpath('.//td')]\n",
    "                data.append(row_data)\n",
    "\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "            \n",
    "            # Extracting the parts of the 'Game' column\n",
    "            df['Park'] = df['Game'].str.extract(r'^(.*?)\\s\\d{1,2}:\\d{2}')\n",
    "            df['Time'] = df['Game'].str.extract(r'(\\d{1,2}:\\d{2})')\n",
    "            df['Away'] = df['Game'].str.extract(r'\\d{1,2}:\\d{2}(.*?)@')\n",
    "            df['Away'] = df['Away'].str.strip()\n",
    "            df['Home'] = df['Game'].str.extract(r'@ (.*)$')\n",
    "\n",
    "            # Converting percentage columns to decimals\n",
    "            cols_to_convert = ['HR', '2B/3B', '1B', 'Runs']\n",
    "            for col in cols_to_convert:\n",
    "                df[col] = df[col].str.rstrip('%').astype(float) / 100\n",
    "\n",
    "            # Drop the 'Game' column since it's no longer needed\n",
    "            df.drop('Game', axis=1, inplace=True)\n",
    "            \n",
    "            df = df[['Park', 'Time', 'Away', 'Home', 'HR', '2B/3B', '1B', 'Runs']]\n",
    "            \n",
    "            df['GameNum'] = df.groupby('Park').cumcount() + 1\n",
    "            \n",
    "        else:\n",
    "            print(\"No table found at the specified XPath.\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        \n",
    "    date = date_dash.replace('-', '')\n",
    "    \n",
    "    df.to_csv(os.path.join(baseball_path, \"8. Weather\", \"C. Ballpark Pal\", f\"Ballpark Pal {date}.csv\"), index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-07\n",
      "Already done\n",
      "2022-04-08\n",
      "Already done\n",
      "2022-04-09\n",
      "Already done\n",
      "2022-04-10\n",
      "Already done\n",
      "2022-04-11\n",
      "Already done\n",
      "2022-04-12\n",
      "Already done\n",
      "2022-04-13\n",
      "Already done\n",
      "2022-04-14\n",
      "Already done\n",
      "2022-04-15\n",
      "Already done\n",
      "2022-04-16\n",
      "Already done\n",
      "2022-04-17\n",
      "Already done\n",
      "2022-04-18\n",
      "Already done\n",
      "2022-04-19\n",
      "Already done\n",
      "2022-04-20\n",
      "Already done\n",
      "2022-04-21\n",
      "Already done\n",
      "2022-04-22\n",
      "Already done\n",
      "2022-04-23\n",
      "Already done\n",
      "2022-04-24\n",
      "Already done\n",
      "2022-04-25\n",
      "Already done\n",
      "2022-04-26\n",
      "Already done\n",
      "2022-04-27\n",
      "Already done\n",
      "2022-04-28\n",
      "Already done\n",
      "2022-04-29\n",
      "Already done\n",
      "2022-04-30\n",
      "Already done\n",
      "2022-05-01\n",
      "Already done\n",
      "2022-05-02\n",
      "Already done\n",
      "2022-05-03\n",
      "Already done\n",
      "2022-05-04\n",
      "Already done\n",
      "2022-05-05\n",
      "Already done\n",
      "2022-05-06\n",
      "Already done\n",
      "2022-05-07\n",
      "Already done\n",
      "2022-05-08\n",
      "Already done\n",
      "2022-05-09\n",
      "Already done\n",
      "2022-05-10\n",
      "Already done\n",
      "2022-05-11\n",
      "Already done\n",
      "2022-05-12\n",
      "Already done\n",
      "2022-05-13\n",
      "Already done\n",
      "2022-05-14\n",
      "Already done\n",
      "2022-05-15\n",
      "Already done\n",
      "2022-05-16\n",
      "Already done\n",
      "2022-05-17\n",
      "Already done\n",
      "2022-05-18\n",
      "Already done\n",
      "2022-05-19\n",
      "Already done\n",
      "2022-05-20\n",
      "Already done\n",
      "2022-05-21\n",
      "Already done\n",
      "2022-05-22\n",
      "Already done\n",
      "2022-05-23\n",
      "Already done\n",
      "2022-05-24\n",
      "Already done\n",
      "2022-05-25\n",
      "Already done\n",
      "2022-05-26\n",
      "Already done\n",
      "2022-05-29\n",
      "Already done\n",
      "2022-05-30\n",
      "Already done\n",
      "2022-05-31\n",
      "Already done\n",
      "2022-06-01\n",
      "Already done\n",
      "2022-06-02\n",
      "Already done\n",
      "2022-06-03\n",
      "Already done\n",
      "2022-06-04\n",
      "Already done\n",
      "2022-06-05\n",
      "Already done\n",
      "2022-06-06\n",
      "Already done\n",
      "2022-06-07\n",
      "Already done\n",
      "2022-06-08\n",
      "Already done\n",
      "2022-06-09\n",
      "Already done\n",
      "2022-06-10\n",
      "Already done\n",
      "2022-06-11\n",
      "Already done\n",
      "2022-06-12\n",
      "Already done\n",
      "2022-06-13\n",
      "Already done\n",
      "2022-06-14\n",
      "Already done\n",
      "2022-06-15\n",
      "Already done\n",
      "2022-06-16\n",
      "Already done\n",
      "2022-06-17\n",
      "Already done\n",
      "2022-06-18\n",
      "Already done\n",
      "2022-06-19\n",
      "Already done\n",
      "2022-06-20\n",
      "Already done\n",
      "2022-06-21\n",
      "Already done\n",
      "2022-06-22\n",
      "Already done\n",
      "2022-06-23\n",
      "Already done\n",
      "2022-06-25\n",
      "Already done\n",
      "2022-06-26\n",
      "Already done\n",
      "2022-06-27\n",
      "Already done\n",
      "2022-06-28\n",
      "Already done\n",
      "2022-06-29\n",
      "Already done\n",
      "2022-06-30\n",
      "Already done\n",
      "2022-07-01\n",
      "Already done\n",
      "2022-07-02\n",
      "Already done\n",
      "2022-07-03\n",
      "Already done\n",
      "2022-07-04\n",
      "Already done\n",
      "2022-07-05\n",
      "Already done\n",
      "2022-07-06\n",
      "Already done\n",
      "2022-07-07\n",
      "Already done\n",
      "2022-07-08\n",
      "Already done\n",
      "2022-07-09\n",
      "Already done\n",
      "2022-07-10\n",
      "Already done\n",
      "2022-07-11\n",
      "Already done\n",
      "2022-07-12\n",
      "Already done\n",
      "2022-07-13\n",
      "Already done\n",
      "2022-07-14\n",
      "Already done\n",
      "2022-07-15\n",
      "Already done\n",
      "2022-07-16\n",
      "Already done\n",
      "2022-07-17\n",
      "Already done\n",
      "2022-07-21\n",
      "Already done\n",
      "2022-07-22\n",
      "2022-07-23\n",
      "2022-07-24\n",
      "2022-07-25\n",
      "2022-07-26\n",
      "2022-07-27\n",
      "2022-07-28\n",
      "2022-07-29\n",
      "2022-07-30\n",
      "2022-07-31\n",
      "2022-08-01\n",
      "2022-08-02\n",
      "2022-08-03\n",
      "2022-08-04\n",
      "2022-08-05\n",
      "2022-08-06\n",
      "2022-08-07\n",
      "2022-08-08\n",
      "2022-08-09\n",
      "2022-08-10\n",
      "2022-08-11\n",
      "2022-08-12\n",
      "2022-08-13\n",
      "2022-08-14\n",
      "2022-08-15\n",
      "2022-08-16\n",
      "2022-08-17\n",
      "2022-08-18\n",
      "2022-08-19\n",
      "2022-08-20\n",
      "2022-08-21\n",
      "2022-08-22\n",
      "2022-08-23\n",
      "2022-08-24\n",
      "2022-08-25\n",
      "2022-08-26\n",
      "2022-08-27\n",
      "2022-08-28\n",
      "2022-08-29\n",
      "2022-08-30\n",
      "2022-08-31\n",
      "2022-09-01\n",
      "2022-09-02\n",
      "2022-09-03\n",
      "2022-09-04\n",
      "2022-09-05\n",
      "2022-09-06\n",
      "2022-09-07\n",
      "2022-09-08\n",
      "2022-09-10\n",
      "2022-09-12\n",
      "2022-09-13\n",
      "2022-09-14\n",
      "2022-09-15\n",
      "2022-09-16\n",
      "2022-09-17\n",
      "2022-09-18\n",
      "2022-09-19\n",
      "2022-09-20\n",
      "2022-09-21\n",
      "2022-09-22\n",
      "2022-09-23\n",
      "2022-09-24\n",
      "2022-09-25\n",
      "2022-09-26\n",
      "2022-09-27\n",
      "2022-09-28\n",
      "2022-09-29\n",
      "2022-09-30\n",
      "2022-10-01\n",
      "2022-10-02\n",
      "2022-10-03\n",
      "2022-10-04\n",
      "2022-10-05\n",
      "2023-03-30\n",
      "2023-03-31\n",
      "2023-04-01\n",
      "2023-04-02\n",
      "2023-04-03\n",
      "2023-04-04\n",
      "2023-04-05\n",
      "2023-04-06\n",
      "2023-04-07\n",
      "2023-04-08\n",
      "2023-04-09\n",
      "2023-04-10\n",
      "2023-04-11\n",
      "2023-04-12\n",
      "2023-04-13\n",
      "2023-04-14\n",
      "2023-04-15\n",
      "2023-04-16\n",
      "2023-04-17\n",
      "2023-04-18\n",
      "2023-04-19\n",
      "2023-04-20\n",
      "2023-04-21\n",
      "2023-04-22\n",
      "2023-04-23\n",
      "2023-04-24\n",
      "2023-04-25\n",
      "2023-04-26\n",
      "2023-04-27\n",
      "2023-04-28\n",
      "2023-04-29\n",
      "2023-04-30\n",
      "2023-05-01\n",
      "2023-05-02\n",
      "2023-05-03\n",
      "2023-05-04\n",
      "2023-05-05\n",
      "2023-05-06\n",
      "2023-05-07\n",
      "2023-05-08\n",
      "2023-05-09\n",
      "2023-05-10\n",
      "2023-05-11\n",
      "2023-05-12\n",
      "2023-05-13\n",
      "2023-05-14\n",
      "2023-05-15\n",
      "2023-05-16\n",
      "2023-05-17\n",
      "2023-05-18\n",
      "2023-05-19\n",
      "2023-05-20\n",
      "2023-05-21\n",
      "2023-05-22\n",
      "2023-05-23\n",
      "2023-05-24\n",
      "2023-05-25\n",
      "2023-05-26\n",
      "2023-05-27\n",
      "2023-05-28\n",
      "2023-05-29\n",
      "2023-05-30\n",
      "2023-05-31\n",
      "2023-06-01\n",
      "2023-06-02\n",
      "2023-06-03\n",
      "2023-06-04\n",
      "2023-06-05\n",
      "2023-06-06\n",
      "2023-06-07\n",
      "2023-06-08\n",
      "2023-06-09\n",
      "2023-06-10\n",
      "2023-06-11\n",
      "2023-06-12\n",
      "2023-06-13\n",
      "2023-06-14\n",
      "2023-06-15\n",
      "2023-06-16\n",
      "2023-06-17\n",
      "2023-06-18\n",
      "2023-06-19\n",
      "2023-06-20\n",
      "2023-06-21\n",
      "2023-06-22\n",
      "2023-06-23\n",
      "2023-06-24\n",
      "2023-06-25\n",
      "2023-06-26\n",
      "2023-06-27\n",
      "2023-06-28\n",
      "2023-06-29\n",
      "2023-06-30\n",
      "2023-07-01\n",
      "2023-07-02\n",
      "2023-07-03\n",
      "2023-07-04\n",
      "2023-07-05\n",
      "2023-07-06\n",
      "2023-07-07\n",
      "2023-07-08\n",
      "2023-07-09\n",
      "2023-07-14\n",
      "2023-07-15\n",
      "2023-07-16\n",
      "2023-07-17\n",
      "2023-07-18\n",
      "2023-07-19\n",
      "2023-07-20\n",
      "2023-07-21\n",
      "2023-07-22\n",
      "2023-07-23\n",
      "2023-07-24\n",
      "2023-07-25\n",
      "2023-07-26\n",
      "2023-07-27\n",
      "2023-07-28\n",
      "2023-07-29\n",
      "2023-07-30\n",
      "2023-07-31\n",
      "2023-08-01\n",
      "2023-08-02\n",
      "2023-08-03\n",
      "2023-08-04\n",
      "2023-08-05\n",
      "2023-08-06\n",
      "2023-08-07\n",
      "2023-08-08\n",
      "2023-08-09\n",
      "2023-08-10\n",
      "2023-08-11\n",
      "2023-08-12\n",
      "2023-08-13\n",
      "2023-08-14\n",
      "2023-08-15\n",
      "2023-08-16\n",
      "2023-08-17\n",
      "2023-08-18\n",
      "Already done\n"
     ]
    }
   ],
   "source": [
    "# source_dir = r'C:\\Users\\james\\Documents\\MLB\\Data2\\8. Weather\\A. Swish Analytics'\n",
    "# target_dir = r'C:\\Users\\james\\Documents\\MLB\\Data2\\8. Weather\\C. Ballpark Pal'\n",
    "\n",
    "# # Get a list of file names in the source directory\n",
    "# file_list = os.listdir(source_dir)\n",
    "\n",
    "# # Regular expression pattern to match the date in the file name\n",
    "# date_pattern = r'Daily_Weather_(\\d{4})(\\d{2})(\\d{2}).xlsx'\n",
    "\n",
    "# for file_name in file_list:\n",
    "#     # Extract the date from the file name using regex\n",
    "#     match = re.match(date_pattern, file_name)\n",
    "#     if match:\n",
    "#         year, month, day = match.groups()\n",
    "#         date_dash = f\"{year}-{month}-{day}\"\n",
    "#         date_plain = date_dash.replace(\"-\", \"\")\n",
    "#         print(date_dash)\n",
    "        \n",
    "#         # Check if a file with the same date exists in the target directory\n",
    "#         target_file_path = os.path.join(target_dir, f\"Ballpark Pal {date_plain}.csv\")\n",
    "#         if not os.path.exists(target_file_path):\n",
    "#             scrape_ballparkpal(date_dash)\n",
    "#             time.sleep(15)\n",
    "#         else:\n",
    "#             print(\"Already done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "# import datetime\n",
    "# import dateutil.parser\n",
    "# import distutils.dir_util\n",
    "# import glob\n",
    "# import IPython.display\n",
    "# import json\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import pathlib\n",
    "# import pickle\n",
    "# import pyautogui\n",
    "# import pytz\n",
    "# import re\n",
    "# import requests\n",
    "# import selenium\n",
    "# import shutil\n",
    "# import statsapi\n",
    "# import statsmodels.formula.api as smf\n",
    "# import time\n",
    "# import unidecode\n",
    "# import warnings\n",
    "# import webbrowser\n",
    "# import xlrd\n",
    "# import random\n",
    "# import urllib\n",
    "# from urllib.request import urlopen, Request\n",
    "# import zipfile\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import date\n",
    "# from IPython.display import display, Javascript\n",
    "# from joblib import Parallel, delayed\n",
    "# from lxml import html\n",
    "# from pathlib import Path\n",
    "# from scipy import stats\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium import webdriver\n",
    "# from openpyxl import load_workbook\n",
    "# from functools import partial\n",
    "\n",
    "# from statsapi import get\n",
    "# from pydfs_lineup_optimizer import get_optimizer, Site, Sport, Player, TeamStack, PlayerFilter, RandomFantasyPointsStrategy\n",
    "\n",
    "# os.chdir(r\"C:\\Users\\james\\Documents\\MLB\\Code\")\n",
    "\n",
    "# # from Utilities import *\n",
    "# # from Classes import *\n",
    "# # from simulation_functions_three import *\n",
    "\n",
    "# import smtplib\n",
    "# import ssl\n",
    "# from email.mime.text import MIMEText\n",
    "# from email.mime.multipart import MIMEMultipart\n",
    "# from email.mime.base import MIMEBase\n",
    "# from email import encoders\n",
    "\n",
    "# # Ensure the warning is ignored only once\n",
    "# warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "# # Display the DataFrame\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.width\", None)\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# # Set paths\n",
    "# model_path = r\"C:\\Users\\james\\Documents\\MLB\\Code\\Models\"\n",
    "# baseball_path = r\"C:\\Users\\james\\Documents\\MLB\\Data2\"\n",
    "# download_path = r\"C:\\Users\\james\\Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
