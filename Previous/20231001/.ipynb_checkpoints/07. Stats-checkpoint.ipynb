{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc63541-ac5e-4646-87ca-1134c4c1180d",
   "metadata": {},
   "source": [
    "# 07. Stats\n",
    "Source: <br> \n",
    "1. FanGraphs API <br>\n",
    "\n",
    "This imports stats from FanGraphs <br>\n",
    "This calculates stats that aren't used in the models but help us get there <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44283ed-806a-457a-837a-28a10ea9999e",
   "metadata": {},
   "source": [
    "### Scrape FanGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b86dfa1-82b2-4738-80d9-4e1f4bad0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FanGraphs API - Batters\n",
    "def scrape_batters():\n",
    "    # Read in API json\n",
    "    batters_lb = pd.read_json('https://www.fangraphs.com/api/projections?type=steamer&stats=bat&pos=all&team=0&players=0&lg=all')\n",
    "    \n",
    "    # Name is currently some weird thing with a bunch of data. We don't need all that\n",
    "    batters_lb['Name'] = batters_lb['PlayerName']\n",
    "    # Rename to match steam. Note that steamerid = key_fangraphs\n",
    "    batters_lb.rename(columns={'playerids':'steamerid'}, inplace=True)\n",
    "    # Convert to string\n",
    "    chadwick['key_fangraphs'] = (chadwick['key_fangraphs']).astype('str')\n",
    "    # Remove trailing .0\n",
    "    chadwick['key_fangraphs'] = chadwick['key_fangraphs'].str.replace(r'\\.\\d', \"\", regex=True)\n",
    "    \n",
    "    # Merge with chadwick for mlbamid\n",
    "    batters_lb = batters_lb.merge(chadwick, left_on='steamerid', right_on='key_fangraphs', how='left')\n",
    "    crosswalk['steamerid'] = crosswalk['steamerid'].astype('str')\n",
    "    batters_lb = batters_lb.merge(crosswalk, on='steamerid', how='left')\n",
    "    \n",
    "    \n",
    "    # Create missing columns to match what's provided by steamer \n",
    "    batters_lb['proj_date'] = todaysdate\n",
    "    batters_lb['mlbamid'] = batters_lb['key_mlbam']\n",
    "    batters_lb['bats'] = \"MI\" # Not included in FanGraphs data\n",
    "    batters_lb['playerid'] = batters_lb['steamerid']\n",
    "    batters_lb['NIBB'] = batters_lb['BB'] - batters_lb['IBB']\n",
    "    batters_lb.rename(columns={'name_first':'firstname', 'name_last':'lastname', 'minpos':'position'}, inplace=True)\n",
    "\n",
    "    batters_lb['mlbamid'].fillna(batters_lb['mlbamid_fill'], inplace=True)\n",
    "    \n",
    "    # Keep relevant variables and in order\n",
    "    batters_lb = batters_lb[['proj_date', 'mlbamid', 'steamerid', 'firstname', 'lastname', 'Team', 'position', 'bats', \n",
    "                             'PA', 'IBB', 'NIBB', 'BB', 'SO', 'HBP', 'H', '2B', '3B', 'HR', 'OBP', 'SLG', 'wOBA', 'SB', 'CS', 'playerid', 'Name']]\n",
    "\n",
    "    \n",
    "    # Export to CSV\n",
    "    batters_lb.to_csv(os.path.join(baseball_path, \"7. Stats\", \"A. Raw FanGraphs\", \"Batters\", \"Batters_FG_\" + todaysdate + \".csv\"), encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5b19f24-4170-43ae-9b0c-ac7830c6c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FanGraphs API - Pitchers\n",
    "def scrape_pitchers():\n",
    "    # Read in API json\n",
    "    pitchers_lb = pd.read_json('https://www.fangraphs.com/api/projections?type=steamer&stats=pit&pos=all&team=0&players=0&lg=all')\n",
    "\n",
    "    # Name is currently some weird thing with a bunch of data. We don't need all that\n",
    "    pitchers_lb['Name'] = pitchers_lb['PlayerName']\n",
    "    # Rename to match steam. Note that steamerid = key_fangraphs\n",
    "    pitchers_lb.rename(columns={'playerids':'steamerid'}, inplace=True)\n",
    "    # Convert to string\n",
    "    chadwick['key_fangraphs'] = (chadwick['key_fangraphs']).astype('str')\n",
    "    # Remove trailing .0\n",
    "    chadwick['key_fangraphs'] = chadwick['key_fangraphs'].str.replace(r'\\.\\d', \"\", regex=True)\n",
    "    \n",
    "    # Merge with chadwick for mlbamid\n",
    "    pitchers_lb = pitchers_lb.merge(chadwick, left_on='steamerid', right_on='key_fangraphs', how='left')\n",
    "    crosswalk['steamerid'] = crosswalk['steamerid'].astype('str')\n",
    "    pitchers_lb = pitchers_lb.merge(crosswalk, on='steamerid', how='left')\n",
    "\n",
    "\n",
    "    # Create missing columns to match what's provided by steamer \n",
    "    pitchers_lb['proj_date'] = todaysdate\n",
    "    pitchers_lb['mlbamid'] = pitchers_lb['key_mlbam']\n",
    "    pitchers_lb['Throws'] = \"MI\" # Not included in FanGraphs data\n",
    "    pitchers_lb['playerid'] = pitchers_lb['steamerid']\n",
    "    pitchers_lb.rename(columns={'name_first':'firstname', 'name_last':'lastname', 'minpos':'position'}, inplace=True)\n",
    "\n",
    "    pitchers_lb['mlbamid'].fillna(pitchers_lb['mlbamid_fill'], inplace=True)\n",
    "\n",
    "    \n",
    "    # Keep relevant variables and in order\n",
    "    pitchers_lb = pitchers_lb[['proj_date', 'mlbamid', 'steamerid', 'firstname', 'lastname', 'Throws', \n",
    "                               'IP', 'G', 'GS', 'K/9', 'BB/9', 'H', 'HR', 'playerid', 'Name']]\n",
    "    \n",
    "    # Export to CSV\n",
    "    pitchers_lb.to_csv(os.path.join(baseball_path, \"7. Stats\", \"A. Raw FanGraphs\", \"Pitchers\", \"Pitchers_FG_\" + todaysdate + \".csv\"), encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90668205-2c3b-426c-b936-cd4c93a0a2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c888a735-c96c-4f51-ad3d-a5f3c6914aaf",
   "metadata": {},
   "source": [
    "### Create Useful Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "609b2398-43ad-45a5-b9d8-3084d9ff0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intermediate_batters(date):\n",
    "    # Read in file\n",
    "    filename = \"Batters_FG_\" + date + \".csv\"\n",
    "    df = pd.read_csv(os.path.join(baseball_path, \"7. Stats\", \"A. Raw FanGraphs\", \"Batters\", filename), encoding='iso-8859-1')\n",
    "    # Create singles\n",
    "    df['1B'] = df['H'] - df['2B'] - df['3B'] - df['HR']\n",
    "    \n",
    "    # Basic stats\n",
    "    hit_list = ['1B', '2B', '3B', 'HR', 'BB', 'HBP', 'SO']\n",
    "\n",
    "    # Advance stats\n",
    "    rate_list = ['OBP', 'SLG', 'wOBA']\n",
    "    for stat in hit_list:\n",
    "        rate = stat + \"_rate\"\n",
    "        rate_list.append(rate)\n",
    "        df[rate] = df[stat] / df['PA']\n",
    "\n",
    "    df['SBA'] = df['SB'] + df['CS']\n",
    "    df['SBO'] = df['1B'] + df['BB'] + df['HBP']\n",
    "    df['sba_imp'] = df['SBA'] / df['SBO']\n",
    "\n",
    "    # Cap imputed SBA \n",
    "    df['sba_imp'] = np.where(df['sba_imp'] > 0.15, 0.15, df['sba_imp'])\n",
    "\n",
    "    # Determine stolen base success rate\n",
    "    df['sbr'] = df['SB'] / df['SBA']\n",
    "    \n",
    "    keep_list = ['Name', 'mlbamid', 'playerid', 'sba_imp', 'sbr'] + rate_list\n",
    "    df = df[keep_list]\n",
    "    \n",
    "    df['sbr'].fillna(0.6, inplace=True) # assume 25th percentile \n",
    "    df['sba_imp'].fillna(0.05, inplace=True) # assume low prob\n",
    "    \n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.rename(columns={'1b_rate': 'b1_rate', '2b_rate': 'b2_rate', '3b_rate': 'b3_rate'}, inplace=True)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    sba_2b_reg = pickle.load(open(os.path.join(model_path, 'sba_2b_20220901.sav'), 'rb'))\n",
    "    df['sba_2b'] = sba_2b_reg.predict(df[['sba_imp']])\n",
    "\n",
    "    sba_3b_reg = pickle.load(open(os.path.join(model_path, 'sba_3b_20220901.sav'), 'rb'))\n",
    "    df['sba_3b'] = sba_3b_reg.predict(df[['sba_imp']])\n",
    "\n",
    "    sb_2b_reg = pickle.load(open(os.path.join(model_path, 'sb_2b_20220901.sav'), 'rb'))\n",
    "    df['sb_2b'] = sb_2b_reg.predict(df[['sbr']])\n",
    "\n",
    "    sb_3b_reg = pickle.load(open(os.path.join(model_path, 'sb_3b_20220901.sav'), 'rb'))\n",
    "    df['sb_3b'] = sb_3b_reg.predict(df[['sbr']])\n",
    "       \n",
    "    # Merge with chadwick to fix certain fangraphs ids \n",
    "    df = df.merge(chadwick, left_on='mlbamid', right_on='key_mlbam', how='left')\n",
    "    # df['key_fangraphs'] = df['key_fangraphs'].astype('int', errors='ignore')\n",
    "    df['playerid'] = np.where(df['playerid'].str.startswith(\"sa\") & ~df['key_fangraphs'].isna(), df['key_fangraphs'], df['playerid'])\n",
    "    df['playerid'] = df['playerid'].astype('string')\n",
    "    df['playerid'] = df['playerid'].str.replace(r'\\.0', '', regex=True)\n",
    "        \n",
    "    df.drop(columns={'index', 'key_fangraphs', 'key_mlbam', 'name_first', 'name_last'}, inplace=True)\n",
    "    \n",
    "    # Write intermediate FanGraphs data to csv\n",
    "    df.to_csv(os.path.join(baseball_path, \"7. Stats\", \"B. Clean FanGraphs\", \"Batters\", \"Batters_FG2_\" + date + \".csv\"), encoding='iso-8859-1')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caaace8c-a39a-48b3-9247-4a6926396a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intermediate_pitchers(date):\n",
    "    # Read in file\n",
    "    filename = \"Pitchers_FG_\" + date + \".csv\"\n",
    "    df = pd.read_csv(os.path.join(baseball_path, \"7. Stats\", \"A. Raw FanGraphs\", \"Pitchers\", filename), encoding='iso-8859-1')\n",
    "    \n",
    "    df['H9'] = df['H'] / df['IP'] * 9\n",
    "    df['HR9'] = df['HR'] / df['IP'] * 9\n",
    "    \n",
    "    df.rename(columns={'K/9':'K9', 'BB/9':'BB9'}, inplace=True)\n",
    "    \n",
    "    keep_list = ['playerid', 'mlbamid', 'H9', 'HR9', 'K9', 'BB9'] \n",
    "    df = df[keep_list]\n",
    "    \n",
    "    # Merge with chadwick to fix certain fangraphs ids \n",
    "    df = df.merge(chadwick, left_on='mlbamid', right_on='key_mlbam', how='left')\n",
    "    # df['key_fangraphs'] = df['key_fangraphs'].astype('int', errors='ignore')\n",
    "    df['playerid'] = np.where(df['playerid'].str.startswith(\"sa\") & ~df['key_fangraphs'].isna(), df['key_fangraphs'], df['playerid'])\n",
    "    df['playerid'] = df['playerid'].astype('string')\n",
    "    df['playerid'] = df['playerid'].str.replace(r'\\.0', '', regex=True)\n",
    "    \n",
    "    df.drop(columns={'index', 'key_fangraphs', 'key_mlbam', 'name_first', 'name_last'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Write intermediate FanGraphs data to csv\n",
    "    df.to_csv(os.path.join(baseball_path, \"7. Stats\", \"B. Clean FanGraphs\", \"Pitchers\", \"Pitchers_FG2_\" + date + \".csv\"), encoding='iso-8859-1')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb13783e-2be3-4035-ba94-26666f21802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batter_merge(date):\n",
    "    # Read in batter stats from API\n",
    "    batter_filename = \"Batters\" + date + \".csv\"\n",
    "    batters_api = pd.read_csv(os.path.join(baseball_path, \"4. Dataset\", \"Batters\", batter_filename), encoding='iso-8859-1')\n",
    "    \n",
    "    # Make string, remove trailing .0\n",
    "    batters_api = fix_fangraphs(batters_api)\n",
    "    \n",
    "    # Read in batter projections from FanGraphs\n",
    "    batters_fg = create_intermediate_batters(date)\n",
    "    batters_fg['key_fangraphs'] = batters_fg['playerid']\n",
    "    \n",
    "    \n",
    "    # Merge API data with FG data\n",
    "    batters_df = batters_api.merge(batters_fg, left_on='key_mlbam', right_on='mlbamid', how='outer')\n",
    "    \n",
    "    return batters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed28c3e2-bfd0-483a-96a6-a392cf99e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitcher_merge(date):\n",
    "    # Read in pitcher stats from API\n",
    "    pitcher_filename = \"Pitchers\" + date + \".csv\"\n",
    "    pitchers_api = pd.read_csv(os.path.join(baseball_path, \"4. Dataset\", \"Pitchers\", pitcher_filename), encoding='iso-8859-1')\n",
    "    \n",
    "    # Make string, remove trailing .0\n",
    "    pitchers_api = fix_fangraphs(pitchers_api)\n",
    "    \n",
    "    # Read in pitcher projections from FanGraphs\n",
    "    pitchers_fg = create_intermediate_pitchers(date)\n",
    "    pitchers_fg['key_fangraphs'] = pitchers_fg['playerid']\n",
    "\n",
    "    # Merge API data with FG data\n",
    "    pitchers_df = pitchers_api.merge(pitchers_fg, left_on='key_mlbam', right_on='mlbamid', how='outer')\n",
    "\n",
    "    \n",
    "    return pitchers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58c57b-9dcc-417a-895f-f66f82b2080f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb3d7750-52bc-4287-bda1-a7e1e42b2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\james\\Documents\\MLB\\Code\\Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaca567f-4655-4a9d-bffb-31f062e1a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normal stats  \n",
    "# Standardize the data using StandardScaler\n",
    "scaler_filename = \"batter_stats_scaler.pkl\"\n",
    "with open(scaler_filename, \"rb\") as file:\n",
    "    batter_scaler = pickle.load(file)\n",
    "    \n",
    "scaler_filename = \"pitcher_stats_scaler.pkl\"\n",
    "with open(scaler_filename, \"rb\") as file:\n",
    "    pitcher_scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edbd9836-c754-424e-83b4-975458638724",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FG Stats\n",
    "# Standardize the data using StandardScaler\n",
    "scaler_filename = \"batter_stats_fg_scaler.pkl\"\n",
    "with open(scaler_filename, \"rb\") as file:\n",
    "    batter_fg_scaler = pickle.load(file)\n",
    "    \n",
    "scaler_filename = \"pitcher_stats_fg_scaler.pkl\"\n",
    "with open(scaler_filename, \"rb\") as file:\n",
    "    pitcher_fg_scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46532e76-6b7f-4166-ba1c-e33f94ab941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputations\n",
    "imp_filename = \"batter_imputations.pkl\"\n",
    "with open(imp_filename, \"rb\") as file:\n",
    "    batter_stats_model = pickle.load(file)\n",
    "\n",
    "imp_filename = \"pitcher_imputations.pkl\"\n",
    "with open(imp_filename, \"rb\") as file:\n",
    "    pitcher_stats_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6d26cf-bd31-4ebd-aae8-27ea04cfdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory back so I can call it from another sheet and it won't change the directory for that sheet\n",
    "os.chdir(r\"C:\\Users\\james\\Documents\\MLB\\Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d3a4cd-0385-4487-b77c-394ad0d0d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"Imports.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7d7d4-6308-49f2-be6f-a19efde30aff",
   "metadata": {},
   "source": [
    "### Batter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415660d0-0987-4d8a-b703-9d0ef92bf990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create position/length-specific inputs from stats\n",
    "stats = ['b1','b2','b3','hr','bb','hbp',\n",
    "            'so','fo','go','lo','po',\n",
    "            'iso','slg','obp','woba','estimated_woba_using_speedangle',\n",
    "            'to_left','to_middle','to_right',\n",
    "            'hard_hit','barrel','totalDistance', 'maxSpeed', 'maxSpin', 'launchSpeed', 'ab', 'pa']\n",
    "\n",
    "batter_stats_short = [f\"{stat}_b\" for stat in stats]\n",
    "batter_stats_long  = [f\"{stat}_b_long\" for stat in stats]\n",
    "\n",
    "# FanGraphs stats\n",
    "batter_stats_fg =    ['b1_rate','b2_rate','b3_rate','hr_rate','bb_rate','hbp_rate','so_rate', 'woba', 'slg', 'obp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13cd2da-9ed5-417b-bf2b-a2163260c532",
   "metadata": {},
   "source": [
    "### Pitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecafdc29-287a-4007-afde-c609c58a2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create position/length-specific inputs from stats\n",
    "pitcher_stats_short = [f\"{stat}_p\" for stat in stats]\n",
    "pitcher_stats_long  = [f\"{stat}_p_long\" for stat in stats]\n",
    "\n",
    "# FanGraphs stats\n",
    "pitcher_stats_fg =    ['H/9','HR/9','K/9','BB/9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f19d5c-8c51-400d-ba81-218a53105323",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b84bd2c-9410-4ff0-8e95-f671d41f17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venues\n",
    "venue_nums = ['1', '2', '3', '4', '5', '7', '10', '12', '13', '14', '15', '16', '17', '19', '22', '31', '32', \n",
    "              '680', '2392', '2394', '2395', '2535', '2536', '2602', '2680', '2681', '2701', '2735', '2756', \n",
    "              '2889', '3289', '3309', '3312', '3313', '4169', '4705', '5010', '5325', '5365', '5381', '5445']\n",
    "\n",
    "venues = [f\"venue_{num}\" for num in venue_nums]\n",
    "\n",
    "# Years\n",
    "years = [f\"year_{year}\" for year in range(2015,2024)]\n",
    "\n",
    "# Matchup, weather, and game stat\n",
    "other_list = ['p_L','b_L','x_vect','y_vect','temperature','onFirst','onSecond','onThird','inning','top','score_diff']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336cf18-184b-44f8-a2d9-4a2eb6665083",
   "metadata": {},
   "source": [
    "### Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ddf716-1e5f-4588-9720-23e7449abb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats that do not apply to the position or we just don't want\n",
    "exclude = [\"maxSpeed_b\", \"maxSpin_b\", \"maxSpeed_b_long\", \"maxSpin_b_long\", \n",
    "           \"totalDistance_p\", \"totalDistance_p_long\", \"launchSpeed_p\", \"launchSpeed_p_long\",\n",
    "           \"ab_b\", \"pa_b\", \"ab_b_long\", \"pa_b_long\", \n",
    "           \"ab_p\", \"pa_p\", \"ab_p_long\", \"pa_p_long\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374c889-e29e-45f5-a81e-6f175f5d120c",
   "metadata": {},
   "source": [
    "### Input Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592d70dc-52fe-482b-93fd-1c419a9df784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batter inputs (into final mode)\n",
    "batter_stats = batter_stats_short + batter_stats_long\n",
    "batter_stats = [item for item in batter_stats if item not in exclude]\n",
    "\n",
    "# Pitcher inputs (into final mode)\n",
    "pitcher_stats = pitcher_stats_short + pitcher_stats_long\n",
    "pitcher_stats = [item for item in pitcher_stats if item not in exclude]\n",
    "\n",
    "# All inputs into final model\n",
    "inputs = batter_stats + pitcher_stats + venues + years + other_list\n",
    "\n",
    "\n",
    "# Add additional variables for ease of use\n",
    "inputs_plus = inputs + ['batterName', 'pitcherName', 'batter', 'pitcher', 'batSide', 'pitchHand', 'eventsModel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a8143-8d14-44db-a4fc-ee5357038449",
   "metadata": {},
   "source": [
    "### Create Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66f4df9b-8aef-44e5-9cf0-f18b24fc4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(rosters_path, team_folder, filename, date, batters_df, pitchers_df):\n",
    "    print(filename)\n",
    "    # Read in roster\n",
    "    df = pd.read_csv(os.path.join(rosters_path, filename), encoding='iso-8859-1')\n",
    "\n",
    "    # Destination     \n",
    "    excel_file = filename.replace(\".csv\", \"\")\n",
    "    excel_file = excel_file + \".xlsx\"\n",
    "    file_name = os.path.join(baseball_path, \"7. Stats\", \"C. Teams\", team_folder, excel_file)\n",
    "\n",
    "    ### Batters\n",
    "    batters_merged = df.merge(batters_df, left_on='id', right_on='batter', how='left', suffixes=(\"\", \"_api\"))\n",
    "\n",
    "    # Only keep batters\n",
    "    batters_merged = batters_merged.query('position != \"P\"')\n",
    "\n",
    "    # Create dummy variable for if they're a lefty. This is necessary to project. (maybe move this)\n",
    "    batters_merged['b_L'] = np.where(batters_merged['batSide'] == \"L\", 1, 0)\n",
    "\n",
    "\n",
    "    ## Standardize FG\n",
    "    batters_merged[batter_stats_fg] = batter_fg_scaler.transform(batters_merged[batter_stats_fg])        \n",
    "\n",
    "    ## Standardize normal stats\n",
    "    # Left\n",
    "    # Rename stats to make compatible with scaler\n",
    "    for stat in batter_stats:\n",
    "        # Get the column name with the '_l' suffix\n",
    "        column_l = stat + '_l'\n",
    "\n",
    "        # Rename the column by removing the '_l' suffix\n",
    "        batters_merged.rename(columns={column_l: stat}, inplace=True)\n",
    "\n",
    "    # Apply the scaler to the renamed column\n",
    "    batters_merged[batter_stats] = batter_scaler.transform(batters_merged[batter_stats])\n",
    "\n",
    "    for stat in batter_stats:\n",
    "        # Get the column name with the '_l' suffix\n",
    "        column_l = stat + '_l'\n",
    "\n",
    "        batters_merged.rename(columns={stat: column_l}, inplace=True)\n",
    "\n",
    "    # Right\n",
    "    for stat in batter_stats:\n",
    "        # Get the column name with the '_r' suffix\n",
    "        column_r = stat + '_r'\n",
    "\n",
    "        # Rename the column by removing the '_r' suffix\n",
    "        batters_merged.rename(columns={column_r: stat}, inplace=True)\n",
    "\n",
    "    # Apply the scaler to the renamed column\n",
    "    batters_merged[batter_stats] = batter_scaler.transform(batters_merged[batter_stats])\n",
    "\n",
    "    for stat in batter_stats:\n",
    "        # Get the column name with the '_r' suffix\n",
    "        column_r = stat + '_r'\n",
    "\n",
    "        batters_merged.rename(columns={stat: column_r}, inplace=True)\n",
    "\n",
    "    ## Impute for small sample\n",
    "    # Use the trained model to make predictions\n",
    "\n",
    "    batter_stats_fg2 = batter_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "    # Vs left        \n",
    "    for stat in batter_stats:\n",
    "        batters_merged[stat] = batters_merged[f'{stat}_l']\n",
    "\n",
    "    batters_merged['p_L'] = 1 \n",
    "\n",
    "    try:\n",
    "        prediction = batter_stats_model.predict(batters_merged.loc[batters_merged['pa_b_l'] < 40, batter_stats_fg2])\n",
    "    except:\n",
    "        prediction = None\n",
    "        print(\"No batter L imputations\")\n",
    "\n",
    "    # Impute missing values in pitcher_stats with the predicted values\n",
    "    batter_stats_l = [f'{stat}_l' for stat in batter_stats]\n",
    "    batters_merged.loc[batters_merged['pa_b_l'] < 40, batter_stats_l] = prediction\n",
    "\n",
    "    # Vs right\n",
    "    for stat in batter_stats:\n",
    "        batters_merged[stat] = batters_merged[f'{stat}_r']\n",
    "\n",
    "    batters_merged['p_L'] = 0 \n",
    "\n",
    "    try:\n",
    "        prediction = batter_stats_model.predict(batters_merged.loc[batters_merged['pa_b_r'] < 40,  batter_stats_fg2])\n",
    "    except:\n",
    "        prediction = None\n",
    "        print(\"No batter R imputations\")\n",
    "\n",
    "    # Impute missing values in pitcher_stats with the predicted values\n",
    "    batter_stats_r = [f'{stat}_r' for stat in batter_stats]\n",
    "    batters_merged.loc[batters_merged['pa_b_r'] < 40, batter_stats_r] = prediction\n",
    "\n",
    "\n",
    "    # Save as Excel\n",
    "    batters_merged.to_excel(file_name, sheet_name=\"Batters\", engine='openpyxl')\n",
    "\n",
    "\n",
    "    ### Pitcher\n",
    "    pitchers_merged = df.merge(pitchers_df, left_on='id', right_on='pitcher', how='left', suffixes=(\"\", \"_api\"))\n",
    "\n",
    "    # Only keep pitchers\n",
    "    desired_positions = ['P', 'TWP', 'Pitcher', 'Two-Way Player']\n",
    "    pitchers_merged = pitchers_merged[pitchers_merged['position'].isin(desired_positions)]\n",
    "\n",
    "    # Create dummy variable for if they're a lefty. This is necessary to project. (maybe move this)\n",
    "    pitchers_merged['p_L'] = np.where(pitchers_merged['pitchHand'] == \"L\", 1, 0)\n",
    "\n",
    "    pitchers_merged.rename(columns={'H9':'H/9', 'HR9':'HR/9','K9':'K/9','BB9':'BB/9'}, inplace=True)\n",
    "    pitcher_stats_fg = ['H/9', 'HR/9','K/9', 'BB/9']\n",
    "\n",
    "    ## Standardize FG\n",
    "    pitchers_merged[pitcher_stats_fg] = pitcher_fg_scaler.transform(pitchers_merged[pitcher_stats_fg])        \n",
    "\n",
    "    ## Standardize normal stats\n",
    "    # Left\n",
    "    # Rename stats to make compatible with scaler\n",
    "    for stat in pitcher_stats:\n",
    "        # Get the column name with the '_l' suffix\n",
    "        column_l = stat + '_l'\n",
    "\n",
    "        # Rename the column by removing the '_l' suffix\n",
    "        pitchers_merged.rename(columns={column_l: stat}, inplace=True)\n",
    "\n",
    "    # Apply the scaler to the renamed column\n",
    "    pitchers_merged[pitcher_stats] = pitcher_scaler.transform(pitchers_merged[pitcher_stats])\n",
    "\n",
    "    for stat in pitcher_stats:\n",
    "        # Get the column name with the '_l' suffix\n",
    "        column_l = stat + '_l'\n",
    "\n",
    "        pitchers_merged.rename(columns={stat: column_l}, inplace=True)\n",
    "\n",
    "    # Right\n",
    "    for stat in pitcher_stats:\n",
    "        # Get the column name with the '_r' suffix\n",
    "        column_r = stat + '_r'\n",
    "\n",
    "        # Rename the column by removing the '_r' suffix\n",
    "        pitchers_merged.rename(columns={column_r: stat}, inplace=True)\n",
    "\n",
    "    # Apply the scaler to the renamed column\n",
    "    pitchers_merged[pitcher_stats] = pitcher_scaler.transform(pitchers_merged[pitcher_stats])\n",
    "\n",
    "    for stat in pitcher_stats:\n",
    "        # Get the column name with the '_r' suffix\n",
    "        column_r = stat + '_r'\n",
    "\n",
    "        pitchers_merged.rename(columns={stat: column_r}, inplace=True)\n",
    "\n",
    "    ## Impute for small sample\n",
    "    # Use the trained model to make predictions\n",
    "\n",
    "    pitcher_stats_fg2 = pitcher_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "    # Vs left        \n",
    "    for stat in pitcher_stats:\n",
    "        pitchers_merged[stat] = pitchers_merged[f'{stat}_l']\n",
    "\n",
    "    pitchers_merged['b_L'] = 1 \n",
    "\n",
    "    try:\n",
    "        prediction = pitcher_stats_model.predict(pitchers_merged.loc[pitchers_merged['pa_p_l'] < 40, pitcher_stats_fg2])\n",
    "    except:\n",
    "        prediction = None\n",
    "        print(\"No pitcher L imputations\")\n",
    "\n",
    "    # Impute missing values in pitcher_stats with the predicted values\n",
    "    pitcher_stats_l = [f'{stat}_l' for stat in pitcher_stats]\n",
    "    pitchers_merged.loc[pitchers_merged['pa_p_l'] < 40, pitcher_stats_l] = prediction\n",
    "\n",
    "    # Vs right\n",
    "    for stat in pitcher_stats:\n",
    "        pitchers_merged[stat] = pitchers_merged[f'{stat}_r']\n",
    "\n",
    "    pitchers_merged['b_L'] = 0 \n",
    "\n",
    "    try:\n",
    "        prediction = pitcher_stats_model.predict(pitchers_merged.loc[pitchers_merged['pa_p_r'] < 40,  pitcher_stats_fg2])\n",
    "    except:\n",
    "        prediction = None\n",
    "        print(\"No pitcher R imputations\")\n",
    "\n",
    "    # Impute missing values in pitcher_stats with the predicted values\n",
    "    pitcher_stats_r = [f'{stat}_r' for stat in pitcher_stats]\n",
    "    pitchers_merged.loc[pitchers_merged['pa_p_r'] < 40, pitcher_stats_r] = prediction\n",
    "\n",
    "\n",
    "    pitchers_merged.rename(columns={'H/9':'H9', 'HR/9':'HR9','K/9':'K9','BB/9':'BB9'}, inplace=True)\n",
    "\n",
    "    # Save as Excel\n",
    "    with pd.ExcelWriter(file_name, mode='a', engine='openpyxl') as writer:  \n",
    "        pitchers_merged.to_excel(writer, sheet_name='Pitchers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77fc9b82-1b1c-45b7-9f60-fb371e28a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_team_rosters(date=None):\n",
    "    # Create new folder with daily rosters\n",
    "    team_folder = \"Daily\" + date\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(os.path.join(baseball_path, \"7. Stats\", \"C. Teams\", team_folder))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Locate daily rosters\n",
    "    rosters_folder = \"Rosters\" + date\n",
    "    rosters_path = os.path.join(baseball_path, \"6. Rosters\", rosters_folder)\n",
    "    \n",
    "    \n",
    "    # Merge API and FG data\n",
    "    batters_df = batter_merge(date)\n",
    "    pitchers_df = pitcher_merge(date)\n",
    "    \n",
    "    # Get the list of files in the rosters_path directory\n",
    "    file_list = os.listdir(rosters_path)\n",
    "                \n",
    "    # Use Parallel to run the process_file function in parallel for each file\n",
    "    Parallel(n_jobs=-2)(delayed(process_file)(rosters_path, team_folder, filename, date, batters_df, pitchers_df) for filename in file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7ccc2-d52b-43d8-ae15-6b542bfb3568",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8dcc0d-3c8d-4f22-b714-6d01ee261b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stats(date=None, historic=False):\n",
    "    if historic == False:\n",
    "        create_team_rosters(date)\n",
    "        \n",
    "   \n",
    "    else:\n",
    "        def create_team_rosters2(date):\n",
    "            try:\n",
    "                create_team_rosters(date)\n",
    "            except:\n",
    "                print(\"Missing for {}.\".format(date))\n",
    "\n",
    "        # Maybe add parallel processing support to this\n",
    "        for i in range(len(history)):  \n",
    "            create_team_rosters2(history['date'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
